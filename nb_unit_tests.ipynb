{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing NITS.\n",
      "Testing arch = [1, 10, 1], 'neg_exp' A_constraint, 'softmax' final_layer_constraint against discretized mixture of logistics.\n",
      "All tests passed!\n",
      "Testing Conditional NITS.\n",
      "All tests passed!\n",
      "Testing autoregressive conditional NITS.\n",
      "All tests passed!\n",
      "Passed all unit tests!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from nits.model import *\n",
    "from nits.autograd_model import *\n",
    "\n",
    "device = 'cpu'\n",
    "# device = 'cuda:2'\n",
    "\n",
    "base_arch = [4, 4, 1]\n",
    "\n",
    "n = 32\n",
    "start, end = -2., 2.\n",
    "monotonic_const = 1e-2\n",
    "\n",
    "print(\"Testing NITS.\")\n",
    "\n",
    "for A_constraint in ['neg_exp', 'exp']:\n",
    "    for final_layer_constraint in ['softmax', 'exp']:\n",
    "#             print(\"\"\"\n",
    "#             Testing configuration:\n",
    "#                 d: {}\n",
    "#                 A_constraint: {}\n",
    "#                 final_layer_constraint: {}\n",
    "#                   \"\"\".format(d, A_constraint, final_layer_constraint))\n",
    "        ############################\n",
    "        # DEFINE MODELS            #\n",
    "        ############################\n",
    "        d = 1\n",
    "        arch = [d] + base_arch\n",
    "        model = NITS(d=d, start=start, end=end, arch=arch,\n",
    "                             monotonic_const=monotonic_const, A_constraint=A_constraint,\n",
    "                             final_layer_constraint=final_layer_constraint).to(device)\n",
    "        params = torch.randn((n, d * model.n_params)).to(device)\n",
    "\n",
    "        ############################\n",
    "        # SANITY CHECKS            #\n",
    "        ############################\n",
    "\n",
    "        # check that the function integrates to 1\n",
    "        assert torch.allclose(torch.ones((n, d)).to(device),\n",
    "                              model.cdf(model.end, params) - model.cdf(model.start, params), atol=1e-5)\n",
    "\n",
    "        # check that the pdf is all positive\n",
    "        z = torch.linspace(start, end, steps=n, device=device)[:,None].tile((1, d))\n",
    "        assert (model.pdf(z, params) >= 0).all()\n",
    "\n",
    "        # check that the cdf is the inverted\n",
    "        cdf = model.cdf(z, params[0:1])\n",
    "        icdf = model.icdf(cdf, params[0:1])\n",
    "        assert (z - icdf <= 1e-3).all()\n",
    "\n",
    "        ############################\n",
    "        # COMPARE TO AUTOGRAD NITS #\n",
    "        ############################\n",
    "        autograd_model = ModelInverse(arch=arch, start=start, end=end, store_weights=False,\n",
    "                                      A_constraint=A_constraint, monotonic_const=monotonic_const,\n",
    "                                      final_layer_constraint=final_layer_constraint).to(device)\n",
    "\n",
    "        def zs_params_to_forwards(zs, params):\n",
    "            out = []\n",
    "            for z, param in zip(zs, params):\n",
    "                for d_ in range(d):\n",
    "                    start_idx, end_idx = d_ * autograd_model.n_params, (d_ + 1) * autograd_model.n_params\n",
    "                    autograd_model.set_params(param[start_idx:end_idx])\n",
    "                    out.append(autograd_model.apply_layers(z[d_:d_+1][None,:]))\n",
    "\n",
    "            out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "            return out\n",
    "\n",
    "        autograd_outs = zs_params_to_forwards(z, params)\n",
    "        outs = model.forward_(z, params)\n",
    "        assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "        def zs_params_to_cdfs(zs, params):\n",
    "            out = []\n",
    "            for z, param in zip(zs, params):\n",
    "                for d_ in range(d):\n",
    "                    start_idx, end_idx = d_ * autograd_model.n_params, (d_ + 1) * autograd_model.n_params\n",
    "                    autograd_model.set_params(param[start_idx:end_idx])\n",
    "                    out.append(autograd_model.cdf(z[d_:d_+1][None,:]))\n",
    "\n",
    "            out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "            return out\n",
    "\n",
    "        autograd_outs = zs_params_to_cdfs(z, params)\n",
    "        outs = model.cdf(z, params)\n",
    "        assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "        def zs_params_to_backwards(zs, params):\n",
    "            out = []\n",
    "            for z, param in zip(zs, params):\n",
    "                for d_ in range(d):\n",
    "                    start_idx, end_idx = d_ * autograd_model.n_params, (d_ + 1) * autograd_model.n_params\n",
    "                    autograd_model.set_params(param[start_idx:end_idx])\n",
    "                    out.append(autograd_model.f_(z[d_:d_+1][None,:]))\n",
    "\n",
    "            out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "            return out\n",
    "\n",
    "        autograd_outs = zs_params_to_backwards(z, params)\n",
    "        outs = model.backward_(z, params)\n",
    "        assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "        def zs_params_to_pdfs(zs, params):\n",
    "            out = []\n",
    "            for z, param in zip(zs, params):\n",
    "                for d_ in range(d):\n",
    "                    start_idx, end_idx = d_ * autograd_model.n_params, (d_ + 1) * autograd_model.n_params\n",
    "                    autograd_model.set_params(param[start_idx:end_idx])\n",
    "                    out.append(autograd_model.pdf(z[d_:d_+1][None,:]))\n",
    "\n",
    "            out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "            return out\n",
    "\n",
    "        autograd_outs = zs_params_to_pdfs(z, params)\n",
    "        outs = model.pdf(z, params)\n",
    "        assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "        def zs_params_to_icdfs(zs, params):\n",
    "            out = []\n",
    "            for z, param in zip(zs, params):\n",
    "                for d_ in range(d):\n",
    "                    start_idx, end_idx = d_ * autograd_model.n_params, (d_ + 1) * autograd_model.n_params\n",
    "                    autograd_model.set_params(param[start_idx:end_idx])\n",
    "                    out.append(autograd_model.F_inv(z[d_:d_+1][None,:]))\n",
    "\n",
    "            out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "            return out\n",
    "\n",
    "        y = torch.rand((n, d)).to(device)\n",
    "        autograd_outs = zs_params_to_icdfs(y, params)\n",
    "        outs = model.icdf(y, params)\n",
    "        assert torch.allclose(autograd_outs, outs, atol=1e-1)\n",
    "\n",
    "        # try with single parameter, many zs\n",
    "\n",
    "        def zs_params_to_pdfs(zs, param):\n",
    "            out = []\n",
    "            for z in zs:\n",
    "                for d_ in range(d):\n",
    "                    start_idx, end_idx = d_ * autograd_model.n_params, (d_ + 1) * autograd_model.n_params\n",
    "                    autograd_model.set_params(param[start_idx:end_idx])\n",
    "                    out.append(autograd_model.pdf(z[d_:d_+1][None,:]))\n",
    "\n",
    "            out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "            return out\n",
    "\n",
    "        autograd_outs = zs_params_to_pdfs(z, params[0])\n",
    "        outs = model.pdf(z, params[0:1])\n",
    "        assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "        # try with single z, many parameters\n",
    "\n",
    "        def zs_params_to_pdfs(z, params):\n",
    "            out = []\n",
    "            for param in params:\n",
    "                for d_ in range(d):\n",
    "                    start_idx, end_idx = d_ * autograd_model.n_params, (d_ + 1) * autograd_model.n_params\n",
    "                    autograd_model.set_params(param[start_idx:end_idx])\n",
    "                    out.append(autograd_model.pdf(z[d_:d_+1][None,:]))\n",
    "\n",
    "            out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "            return out\n",
    "\n",
    "        autograd_outs = zs_params_to_pdfs(z[0], params)\n",
    "        outs = model.pdf(z[0:1], params)\n",
    "        assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "from nits.discretized_mol import *\n",
    "print(\"Testing arch = [1, 10, 1], 'neg_exp' A_constraint, 'softmax' final_layer_constraint \" \\\n",
    "      \"against discretized mixture of logistics.\")\n",
    "\n",
    "model = NITS(d=1, start=-1e5, end=1e5, arch=[1, 10, 1],\n",
    "                     monotonic_const=0., A_constraint='neg_exp',\n",
    "                     final_layer_constraint='softmax').to(device)\n",
    "params = torch.randn((n, model.n_params, 1, 1))\n",
    "z = torch.randn((n, 1, 1, 1))\n",
    "\n",
    "loss1 = discretized_mix_logistic_loss_1d(z, params)\n",
    "loss2 = discretized_nits_loss(z, params, nits_model=model)\n",
    "\n",
    "assert (loss1 - loss2).norm() < 1e-2, (loss1 - loss2).norm()\n",
    "\n",
    "model = NITS(d=1, start=-1e7, end=1e7, arch=[1, 10, 1],\n",
    "                     monotonic_const=0., A_constraint='neg_exp',\n",
    "                     final_layer_constraint='softmax').to(device)\n",
    "\n",
    "loss1 = discretized_mix_logistic_loss_1d(z, params)\n",
    "loss2 = discretized_nits_loss(z, params, nits_model=model)\n",
    "\n",
    "assert (loss1 - loss2).norm() < 1e-3, (loss1 - loss2).norm()\n",
    "\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "print(\"Testing Conditional NITS.\")\n",
    "start, end = -2., 2.\n",
    "monotonic_const = 1e-2\n",
    "d = 2\n",
    "c_arch = [d] + base_arch\n",
    "A_constraint = 'exp'\n",
    "final_layer_constraint = 'softmax'\n",
    "device = 'cpu'\n",
    "\n",
    "c_model = ConditionalNITS(d=d, start=start, end=end, arch=c_arch,\n",
    "                          monotonic_const=monotonic_const, A_constraint=A_constraint,\n",
    "                          final_layer_constraint=final_layer_constraint,\n",
    "                          autoregressive=False).to(device)\n",
    "\n",
    "c_params = torch.randn((n, c_model.tot_params)).to(device)\n",
    "z = torch.linspace(start, end, steps=n, device=device)[:,None].tile((1, d)).to(device)\n",
    "\n",
    "def cond_zs_params_to_cdfs(zs, params):\n",
    "    out = []\n",
    "    for z, param in zip(zs, params):\n",
    "        for d_ in range(d):\n",
    "            c_autograd_model = ModelInverse(arch=c_arch, start=start, end=end, store_weights=False,\n",
    "                                           A_constraint=A_constraint, monotonic_const=monotonic_const,\n",
    "                                           final_layer_constraint=final_layer_constraint,\n",
    "                                           non_conditional_dim=d_).to(device)\n",
    "            start_idx, end_idx = d_ * c_autograd_model.n_params, (d_ + 1) * c_autograd_model.n_params\n",
    "            c_autograd_model.set_params(param[start_idx:end_idx])\n",
    "            out.append(c_autograd_model.cdf(z[None,:]))\n",
    "\n",
    "    out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "    return out\n",
    "\n",
    "autograd_outs = cond_zs_params_to_cdfs(z, c_params)\n",
    "outs = c_model.cdf(z, c_params)\n",
    "assert torch.allclose(autograd_outs, outs, atol=1e-3), (autograd_outs - outs).norm()\n",
    "\n",
    "def cond_zs_params_to_pdfs(zs, params):\n",
    "    out = []\n",
    "    for z, param in zip(zs, params):\n",
    "        for d_ in range(d):\n",
    "            c_autograd_model = ModelInverse(arch=c_arch, start=start, end=end, store_weights=False,\n",
    "                                           A_constraint=A_constraint, monotonic_const=monotonic_const,\n",
    "                                           final_layer_constraint=final_layer_constraint,\n",
    "                                           non_conditional_dim=d_).to(device)\n",
    "            start_idx, end_idx = d_ * c_autograd_model.n_params, (d_ + 1) * c_autograd_model.n_params\n",
    "            c_autograd_model.set_params(param[start_idx:end_idx])\n",
    "            out.append(c_autograd_model.pdf(z[None,:]))\n",
    "\n",
    "    out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "    return out\n",
    "\n",
    "autograd_outs = cond_zs_params_to_pdfs(z, c_params)\n",
    "outs = c_model.pdf(z, c_params)\n",
    "assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "# testing the inverse_cdf function\n",
    "\n",
    "def cond_zs_params_to_icdfs(ys, zs, params):\n",
    "    out = []\n",
    "    for y, z, param in zip(ys, zs, params):\n",
    "        for d_ in range(d):\n",
    "            c_autograd_model = ModelInverse(arch=c_arch, start=start, end=end, store_weights=False,\n",
    "                                           A_constraint=A_constraint, monotonic_const=monotonic_const,\n",
    "                                           final_layer_constraint=final_layer_constraint,\n",
    "                                           non_conditional_dim=d_).to(device)\n",
    "            start_idx, end_idx = d_ * c_autograd_model.n_params, (d_ + 1) * c_autograd_model.n_params\n",
    "            c_autograd_model.set_params(param[start_idx:end_idx])\n",
    "            out.append(c_autograd_model.F_inv(y[d_:d_+1][None,:], given_x=z[None,:]))\n",
    "\n",
    "    out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "    return out\n",
    "\n",
    "y = torch.rand((n, d)).to(device)\n",
    "autograd_outs = cond_zs_params_to_icdfs(y, z, c_params)\n",
    "outs = c_model.icdf(y, c_params, given_x=z)\n",
    "assert torch.allclose(autograd_outs, outs, atol=1e-1)\n",
    "\n",
    "for i in range(d):\n",
    "    tmp = torch.cat([z[:,:i], outs[:,i:i+1], z[:,i+1:]], axis=1)\n",
    "    res = c_model.cdf(tmp, c_params)\n",
    "    assert torch.allclose(res[:,i], y[:,i], atol=1e-2)\n",
    "\n",
    "for i in range(d):\n",
    "    tmp = torch.cat([z[:,:i], outs[:,i:i+1], z[:,i+1:]], axis=1)\n",
    "    res = cond_zs_params_to_cdfs(tmp, c_params)\n",
    "    assert torch.allclose(res[:,i], y[:,i], atol=1e-2)\n",
    "\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "print('Testing autoregressive conditional NITS.')\n",
    "start, end = -2., 2.\n",
    "monotonic_const = 1e-2\n",
    "A_constraint = 'neg_exp'\n",
    "final_layer_constraint = 'softmax'\n",
    "device = 'cpu'\n",
    "\n",
    "c_model = ConditionalNITS(d=d, start=start, end=end, arch=c_arch,\n",
    "                          monotonic_const=monotonic_const, A_constraint=A_constraint,\n",
    "                          final_layer_constraint=final_layer_constraint,\n",
    "                          autoregressive=True).to(device)\n",
    "\n",
    "c_params = torch.randn((n, c_model.tot_params)).to(device)\n",
    "z = torch.linspace(start, end, steps=n, device=device)[:,None].tile((1, d)).to(device)\n",
    "\n",
    "def causal_mask(x, i):\n",
    "    x = x.clone()[None,:]\n",
    "    x[:,i+1:] = 0.\n",
    "    return x\n",
    "\n",
    "def cond_zs_params_to_cdfs(zs, params):\n",
    "    out = []\n",
    "    for z, param in zip(zs, params):\n",
    "        for d_ in range(d):\n",
    "            c_autograd_model = ModelInverse(arch=c_arch, start=start, end=end, store_weights=False,\n",
    "                                           A_constraint=A_constraint, monotonic_const=monotonic_const,\n",
    "                                           final_layer_constraint=final_layer_constraint,\n",
    "                                           non_conditional_dim=d_, b_constraint='').to(device)\n",
    "            start_idx, end_idx = d_ * c_autograd_model.n_params, (d_ + 1) * c_autograd_model.n_params\n",
    "            c_autograd_model.set_params(param[start_idx:end_idx])\n",
    "\n",
    "            # set mask and apply function\n",
    "            z_masked = causal_mask(z, d_)\n",
    "            out.append(c_autograd_model.cdf(z_masked))\n",
    "\n",
    "    out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "    return out\n",
    "\n",
    "autograd_outs = cond_zs_params_to_cdfs(z, c_params)\n",
    "outs = c_model.cdf(z, c_params)\n",
    "assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "def cond_zs_params_to_pdfs(zs, params):\n",
    "    out = []\n",
    "    for z, param in zip(zs, params):\n",
    "        for d_ in range(d):\n",
    "            c_autograd_model = ModelInverse(arch=c_arch, start=start, end=end, store_weights=False,\n",
    "                                           A_constraint=A_constraint, monotonic_const=monotonic_const,\n",
    "                                           final_layer_constraint=final_layer_constraint,\n",
    "                                           non_conditional_dim=d_, b_constraint='').to(device)\n",
    "            start_idx, end_idx = d_ * c_autograd_model.n_params, (d_ + 1) * c_autograd_model.n_params\n",
    "            c_autograd_model.set_params(param[start_idx:end_idx])\n",
    "\n",
    "            # set mask and apply function\n",
    "            z_masked = causal_mask(z, d_)\n",
    "            out.append(c_autograd_model.pdf(z_masked))\n",
    "\n",
    "    out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "    return out\n",
    "\n",
    "autograd_outs = cond_zs_params_to_pdfs(z, c_params)\n",
    "outs = c_model.pdf(z, c_params)\n",
    "assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "# testing the inverse_cdf function\n",
    "\n",
    "def cond_zs_params_to_icdfs(ys, zs, params):\n",
    "    out = []\n",
    "    for y, z, param in zip(ys, zs, params):\n",
    "        for d_ in range(d):\n",
    "            c_autograd_model = ModelInverse(arch=c_arch, start=start, end=end, store_weights=False,\n",
    "                                           A_constraint=A_constraint, monotonic_const=monotonic_const,\n",
    "                                           final_layer_constraint=final_layer_constraint,\n",
    "                                           non_conditional_dim=d_, b_constraint='').to(device)\n",
    "            start_idx, end_idx = d_ * c_autograd_model.n_params, (d_ + 1) * c_autograd_model.n_params\n",
    "            c_autograd_model.set_params(param[start_idx:end_idx])\n",
    "\n",
    "            # set mask and apply function\n",
    "            z_masked = torch.cat(out[len(out)-d_:] + [torch.zeros((1, d - d_))], axis=1)\n",
    "            out.append(c_autograd_model.F_inv(y[d_:d_+1][None,:], given_x=z_masked))\n",
    "\n",
    "    out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "    return out\n",
    "\n",
    "y = torch.rand((n, d)).to(device)\n",
    "autograd_outs = cond_zs_params_to_icdfs(y, z, c_params)\n",
    "outs = c_model.icdf(y, c_params)\n",
    "assert torch.allclose(autograd_outs, outs, atol=1e-1)\n",
    "\n",
    "assert torch.allclose(c_model.cdf(outs, c_params), y, atol=1e-3)\n",
    "assert torch.allclose(cond_zs_params_to_cdfs(autograd_outs, c_params), y, atol=1e-3)\n",
    "\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "print(\"Passed all unit tests!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretized_mix_logistic_loss(x, l):\n",
    "    \"\"\" log-likelihood for mixture of discretized logistics, assumes the data has been rescaled to [-1,1] interval \"\"\"\n",
    "    # Pytorch ordering\n",
    "    x = x.permute(0, 2, 3, 1)\n",
    "    l = l.permute(0, 2, 3, 1)\n",
    "    xs = [int(y) for y in x.size()]\n",
    "    ls = [int(y) for y in l.size()]\n",
    "\n",
    "    # here and below: unpacking the params of the mixture of logistics\n",
    "    nr_mix = 10\n",
    "    logit_probs = l[:, :, :, :nr_mix]\n",
    "    l = l[:, :, :, nr_mix:].contiguous().view(xs + [nr_mix * 3]) # 3 for mean, scale, coef\n",
    "    print(l.shape)\n",
    "    means = l[:, :, :, :, :nr_mix]\n",
    "    log_scales = torch.clamp(l[:, :, :, :, nr_mix:2 * nr_mix], min=-7.)\n",
    "\n",
    "    coeffs = F.tanh(l[:, :, :, :, 2 * nr_mix:3 * nr_mix])\n",
    "    print(means.shape, log_scales.shape, coeffs.shape)\n",
    "    # here and below: getting the means and adjusting them based on preceding\n",
    "    # sub-pixels\n",
    "    x = x.contiguous()\n",
    "    x = x.unsqueeze(-1) + Variable(torch.zeros(xs + [nr_mix]).to(x.device), requires_grad=False)\n",
    "    m2 = (means[:, :, :, 1, :] + coeffs[:, :, :, 0, :]\n",
    "                * x[:, :, :, 0, :]).view(xs[0], xs[1], xs[2], 1, nr_mix)\n",
    "\n",
    "    m3 = (means[:, :, :, 2, :] + coeffs[:, :, :, 1, :] * x[:, :, :, 0, :] +\n",
    "                coeffs[:, :, :, 2, :] * x[:, :, :, 1, :]).view(xs[0], xs[1], xs[2], 1, nr_mix)\n",
    "\n",
    "    means = torch.cat((means[:, :, :, 0, :].unsqueeze(3), m2, m3), dim=3)\n",
    "    centered_x = x - means\n",
    "    inv_stdv = torch.exp(-log_scales)\n",
    "    plus_in = inv_stdv * (centered_x + 1. / 255.)\n",
    "    cdf_plus = F.sigmoid(plus_in)\n",
    "    min_in = inv_stdv * (centered_x - 1. / 255.)\n",
    "    cdf_min = F.sigmoid(min_in)\n",
    "    # log probability for edge case of 0 (before scaling)\n",
    "    log_cdf_plus = plus_in - F.softplus(plus_in)\n",
    "    # log probability for edge case of 255 (before scaling)\n",
    "    log_one_minus_cdf_min = -F.softplus(min_in)\n",
    "    cdf_delta = cdf_plus - cdf_min  # probability for all other cases\n",
    "    mid_in = inv_stdv * centered_x\n",
    "    log_pdf_mid = mid_in - log_scales - 2. * F.softplus(mid_in)\n",
    "    \n",
    "    inner_inner_cond = (cdf_delta > 1e-5).float()\n",
    "    inner_inner_out  = inner_inner_cond * torch.log(torch.clamp(cdf_delta, min=1e-12)) + (1. - inner_inner_cond) * (log_pdf_mid - np.log(127.5))\n",
    "    inner_cond       = (x > 0.999).float()\n",
    "    inner_out        = inner_cond * log_one_minus_cdf_min + (1. - inner_cond) * inner_inner_out\n",
    "    cond             = (x < -0.999).float()\n",
    "    log_probs        = cond * log_cdf_plus + (1. - cond) * inner_out\n",
    "    log_probs        = torch.sum(log_probs, dim=3) + log_prob_from_logits(logit_probs)\n",
    "\n",
    "    return -torch.sum(log_sum_exp(log_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 1, 3, 30])\n",
      "torch.Size([32, 1, 1, 3, 10]) torch.Size([32, 1, 1, 3, 10]) torch.Size([32, 1, 1, 3, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/henry/.local/lib/python3.6/site-packages/torch/nn/functional.py:1898: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/home/henry/.local/lib/python3.6/site-packages/torch/nn/functional.py:1909: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(456.7060)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_params = torch.randn((n, 100, 1, 1))\n",
    "z = torch.randn((n, 3, 1, 1))\n",
    "\n",
    "discretized_mix_logistic_loss(z, c_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c78640552c1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m                           \u001b[0mmonotonic_const\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmonotonic_const\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mA_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                           \u001b[0mfinal_layer_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfinal_layer_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                           autoregressive=True).to(device)\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mc_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/NITS/nits/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, d, arch, start, end, A_constraint, monotonic_const, final_layer_constraint, autoregressive)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_constraint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0march\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnits_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start, end = -2., 2.\n",
    "monotonic_const = 1e-2\n",
    "A_constraint = 'neg_exp'\n",
    "final_layer_constraint = 'softmax'\n",
    "device = 'cpu'\n",
    "\n",
    "c_model = ConditionalNITS(d=d, start=start, end=end, arch=c_arch,\n",
    "                          monotonic_const=monotonic_const, A_constraint=A_constraint,\n",
    "                          final_layer_constraint=final_layer_constraint,\n",
    "                          autoregressive=True).to(device)\n",
    "\n",
    "c_params = torch.randn((n, c_model.n_params, 1, 1))\n",
    "z = torch.randn((n, 1, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not discretized_nits_loss(z, c_params, c_model).isnan().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = nits_sample(c_params, arch, 0, 0, c_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discretized_nits_loss(sample, c_params, arch, c_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nits_model.cdf(imgs, params[:,i,j,:].reshape(-1, nits_model.tot_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_reshape(x, l, nits_model):\n",
    "    \"\"\" log-likelihood for mixture of discretized logistics, assumes the data has been rescaled to [-1,1] interval \"\"\"\n",
    "    # Pytorch ordering\n",
    "    x = x.permute(0, 2, 3, 1)\n",
    "    l = l.permute(0, 2, 3, 1)\n",
    "    xs = [int(y) for y in x.size()]\n",
    "    ls = [int(y) for y in l.size()]\n",
    "\n",
    "    # here and below: getting the means and adjusting them based on preceding\n",
    "    # sub-pixels\n",
    "    x = x.contiguous()\n",
    "\n",
    "    nits_model = nits_model.to(x.device)\n",
    "    x = x.reshape(-1, nits_model.d)\n",
    "    params = l.reshape(-1, nits_model.tot_params)\n",
    "\n",
    "    return x, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that the reshaping operation is correct\n",
    "z = torch.arange(n).reshape(n, 1, 1, 1).tile(1, d, 1, 1)\n",
    "c_params = torch.arange(n).reshape(n, 1, 1, 1).tile(1, c_model.tot_params, 1, 1)\n",
    "new_z, new_params = loss_reshape(z, c_params, c_model)\n",
    "assert (new_z == torch.arange(n).reshape(n, 1).tile(1, d)).all()\n",
    "assert (new_params == torch.arange(n).reshape(n, 1).tile(1, c_model.tot_params)).all()\n",
    "\n",
    "z = torch.arange(d).reshape(1, d, 1, 1).tile(n, 1, 1, 1)\n",
    "c_params = torch.arange(c_model.tot_params).reshape(c_model.tot_params, 1, 1, 1).tile(n, 1, 1, 1)\n",
    "new_z, new_params = loss_reshape(z, c_params, c_model)\n",
    "assert (new_z == torch.arange(d).reshape(1, d).tile(n, 1)).all()\n",
    "assert (new_params == torch.arange(c_model.tot_params).reshape(1, c_model.tot_params).tile(n, 1)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_reshape(params, nits_model):\n",
    "    params = params.permute(0, 2, 3, 1)\n",
    "    batch_size, height, width, params_per_pixel = params.shape\n",
    "\n",
    "    nits_model = nits_model.to(params.device)\n",
    "\n",
    "    n_params = nits_model.n_params\n",
    "    n_channels = int(params_per_pixel / n_params)\n",
    "\n",
    "    data = torch.zeros((batch_size, n_channels, height, width), device=params.device)\n",
    "    \n",
    "    imgs = nits_model.sample(1, params[:,i,j,:].reshape(-1, nits_model.tot_params)).clamp(min=-1., max=1.)\n",
    "    data[:,:,i,j] = imgs.reshape((batch_size, n_channels))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
