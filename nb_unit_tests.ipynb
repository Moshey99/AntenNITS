{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing NITS.\n",
      "Testing arch = [1, 10, 1], 'neg_exp' constraint_type, 'softmax' final_layer_constraint against discretized mixture of logistics.\n",
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from nits.model import *\n",
    "from nits.autograd_model import *\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "n = 8\n",
    "start, end = -2., 2.\n",
    "arch = [1, 8, 1]\n",
    "monotonic_const = 1e-2\n",
    "\n",
    "print(\"Testing NITS.\")\n",
    "\n",
    "for d in [1, 2, 10]:\n",
    "    for constraint_type in ['neg_exp', 'exp']:\n",
    "        for final_layer_constraint in ['softmax', 'exp']:\n",
    "#             print(\"\"\"\n",
    "#             Testing configuration:\n",
    "#                 d: {}\n",
    "#                 constraint_type: {}\n",
    "#                 final_layer_constraint: {}\n",
    "#                   \"\"\".format(d, constraint_type, final_layer_constraint))\n",
    "            ############################\n",
    "            # DEFINE MODELS            #\n",
    "            ############################\n",
    "\n",
    "            model = NITS(d=d, start=start, end=end, arch=arch,\n",
    "                                 monotonic_const=monotonic_const, constraint_type=constraint_type,\n",
    "                                 final_layer_constraint=final_layer_constraint).to(device)\n",
    "            params = torch.randn((n, d * model.n_params)).to(device)\n",
    "\n",
    "            ############################\n",
    "            # SANITY CHECKS            #\n",
    "            ############################\n",
    "\n",
    "            # check that the function integrates to 1\n",
    "            assert torch.allclose(torch.ones((n, d)).to(device),\n",
    "                                  model.cdf(model.end, params) - model.cdf(model.start, params), atol=1e-5)\n",
    "\n",
    "            # check that the pdf is all positive\n",
    "            z = torch.linspace(start, end, steps=n, device=device)[:,None].tile((1, d))\n",
    "            assert (model.pdf(z, params) >= 0).all()\n",
    "\n",
    "            # check that the cdf is the inverted\n",
    "            cdf = model.cdf(z, params[0:1])\n",
    "            icdf = model.icdf(cdf, params[0:1])\n",
    "            assert (z - icdf <= 1e-3).all()\n",
    "\n",
    "            ############################\n",
    "            # COMPARE TO AUTOGRAD NITS #\n",
    "            ############################\n",
    "            autograd_model = ModelInverse(arch=arch, start=start, end=end, store_weights=False,\n",
    "                                          constraint_type=constraint_type, monotonic_const=monotonic_const,\n",
    "                                          final_layer_constraint=final_layer_constraint)\n",
    "\n",
    "            def zs_params_to_forwards(zs, params):\n",
    "                out = []\n",
    "                for z, param in zip(zs, params):\n",
    "                    for d_ in range(d):\n",
    "                        start_idx, end_idx = d_ * autograd_model.n_params, (d_ + 1) * autograd_model.n_params\n",
    "                        autograd_model.set_params(param[start_idx:end_idx])\n",
    "                        out.append(autograd_model.apply_layers(z[d_:d_+1][None,:]))\n",
    "\n",
    "                out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "                return out\n",
    "\n",
    "            autograd_outs = zs_params_to_forwards(z, params)\n",
    "            outs = model.forward_(z, params)\n",
    "            assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "            def zs_params_to_cdfs(zs, params):\n",
    "                out = []\n",
    "                for z, param in zip(zs, params):\n",
    "                    for d_ in range(d):\n",
    "                        start_idx, end_idx = d_ * autograd_model.n_params, (d_ + 1) * autograd_model.n_params\n",
    "                        autograd_model.set_params(param[start_idx:end_idx])\n",
    "                        out.append(autograd_model.cdf(z[d_:d_+1][None,:]))\n",
    "\n",
    "                out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "                return out\n",
    "\n",
    "            autograd_outs = zs_params_to_cdfs(z, params)\n",
    "            outs = model.cdf(z, params)\n",
    "            assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "            def zs_params_to_pdfs(zs, params):\n",
    "                out = []\n",
    "                for z, param in zip(zs, params):\n",
    "                    for d_ in range(d):\n",
    "                        start_idx, end_idx = d_ * autograd_model.n_params, (d_ + 1) * autograd_model.n_params\n",
    "                        autograd_model.set_params(param[start_idx:end_idx])\n",
    "                        out.append(autograd_model.pdf(z[d_:d_+1][None,:]))\n",
    "\n",
    "                out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "                return out\n",
    "\n",
    "            autograd_outs = zs_params_to_pdfs(z, params)\n",
    "            outs = model.pdf(z, params)\n",
    "            assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "            def zs_params_to_icdfs(zs, params):\n",
    "                out = []\n",
    "                for z, param in zip(zs, params):\n",
    "                    for d_ in range(d):\n",
    "                        start_idx, end_idx = d_ * autograd_model.n_params, (d_ + 1) * autograd_model.n_params\n",
    "                        autograd_model.set_params(param[start_idx:end_idx])\n",
    "                        out.append(autograd_model.F_inv(z[d_:d_+1][None,:]))\n",
    "\n",
    "                out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "                return out\n",
    "\n",
    "            y = torch.rand((n, d)).to(device)\n",
    "            autograd_outs = zs_params_to_icdfs(y, params)\n",
    "            outs = model.icdf(y, params)\n",
    "            assert torch.allclose(autograd_outs, outs, atol=1e-1)\n",
    "\n",
    "            # try with single parameter, many zs\n",
    "\n",
    "            def zs_params_to_pdfs(zs, param):\n",
    "                out = []\n",
    "                for z in zs:\n",
    "                    for d_ in range(d):\n",
    "                        start_idx, end_idx = d_ * autograd_model.n_params, (d_ + 1) * autograd_model.n_params\n",
    "                        autograd_model.set_params(param[start_idx:end_idx])\n",
    "                        out.append(autograd_model.pdf(z[d_:d_+1][None,:]))\n",
    "\n",
    "                out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "                return out\n",
    "\n",
    "            autograd_outs = zs_params_to_pdfs(z, params[0])\n",
    "            outs = model.pdf(z, params[0:1])\n",
    "            assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "            # try with single z, many parameters\n",
    "\n",
    "            def zs_params_to_pdfs(z, params):\n",
    "                out = []\n",
    "                for param in params:\n",
    "                    for d_ in range(d):\n",
    "                        start_idx, end_idx = d_ * autograd_model.n_params, (d_ + 1) * autograd_model.n_params\n",
    "                        autograd_model.set_params(param[start_idx:end_idx])\n",
    "                        out.append(autograd_model.pdf(z[d_:d_+1][None,:]))\n",
    "\n",
    "                out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "                return out\n",
    "\n",
    "            autograd_outs = zs_params_to_pdfs(z[0], params)\n",
    "            outs = model.pdf(z[0:1], params)\n",
    "            assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "from nits.discretized_mol import *\n",
    "print(\"Testing arch = [1, 10, 1], 'neg_exp' constraint_type, 'softmax' final_layer_constraint \" \\\n",
    "      \"against discretized mixture of logistics.\")\n",
    "\n",
    "model = NITS(d=1, start=-1e5, end=1e5, arch=[1, 10, 1],\n",
    "                     monotonic_const=0., constraint_type='neg_exp',\n",
    "                     final_layer_constraint='softmax').to(device)\n",
    "params = torch.randn((n, model.n_params, 1, 1))\n",
    "z = torch.randn((n, 1, 1, 1))\n",
    "\n",
    "loss1 = discretized_mix_logistic_loss_1d(z, params)\n",
    "loss2 = discretized_nits_loss(z, params, arch=[1, 10, 1], nits_model=model)\n",
    "\n",
    "assert (loss1 - loss2).norm() < 1e-2, (loss1 - loss2).norm()\n",
    "\n",
    "model = NITS(d=1, start=-1e7, end=1e7, arch=[1, 10, 1],\n",
    "                     monotonic_const=0., constraint_type='neg_exp',\n",
    "                     final_layer_constraint='softmax').to(device)\n",
    "\n",
    "loss1 = discretized_mix_logistic_loss_1d(z, params)\n",
    "loss2 = discretized_nits_loss(z, params, arch=[1, 10, 1], nits_model=model)\n",
    "\n",
    "assert (loss1 - loss2).norm() < 1e-3, (loss1 - loss2).norm()\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Conditional NITS.\n",
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing Conditional NITS.\")\n",
    "start, end = -2., 2.\n",
    "monotonic_const = 1e-2\n",
    "d = 2\n",
    "c_arch = [d, 8, 1]\n",
    "constraint_type = 'exp'\n",
    "final_layer_constraint = 'softmax'\n",
    "device = 'cpu'\n",
    "\n",
    "c_model = ConditionalNITS(d=d, start=start, end=end, arch=c_arch,\n",
    "                          monotonic_const=monotonic_const, constraint_type=constraint_type,\n",
    "                          final_layer_constraint=final_layer_constraint,\n",
    "                          autoregressive=False).to(device)\n",
    "\n",
    "c_params = torch.randn((n, c_model.tot_params))\n",
    "z = torch.linspace(start, end, steps=n, device=device)[:,None].tile((1, d))\n",
    "\n",
    "def cond_zs_params_to_cdfs(zs, params):\n",
    "    out = []\n",
    "    for z, param in zip(zs, params):\n",
    "        for d_ in range(d):\n",
    "            c_autograd_model = ModelInverse(arch=c_arch, start=start, end=end, store_weights=False,\n",
    "                                           constraint_type=constraint_type, monotonic_const=monotonic_const,\n",
    "                                           final_layer_constraint=final_layer_constraint,\n",
    "                                           non_conditional_dim=d_)\n",
    "            start_idx, end_idx = d_ * c_autograd_model.n_params, (d_ + 1) * c_autograd_model.n_params\n",
    "            c_autograd_model.set_params(param[start_idx:end_idx])\n",
    "            out.append(c_autograd_model.cdf(z[None,:]))\n",
    "\n",
    "    out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "    return out\n",
    "\n",
    "autograd_outs = cond_zs_params_to_cdfs(z, c_params)\n",
    "outs = c_model.cdf(z, c_params)\n",
    "assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "def cond_zs_params_to_pdfs(zs, params):\n",
    "    out = []\n",
    "    for z, param in zip(zs, params):\n",
    "        for d_ in range(d):\n",
    "            c_autograd_model = ModelInverse(arch=c_arch, start=start, end=end, store_weights=False,\n",
    "                                           constraint_type=constraint_type, monotonic_const=monotonic_const,\n",
    "                                           final_layer_constraint=final_layer_constraint,\n",
    "                                           non_conditional_dim=d_)\n",
    "            start_idx, end_idx = d_ * c_autograd_model.n_params, (d_ + 1) * c_autograd_model.n_params\n",
    "            c_autograd_model.set_params(param[start_idx:end_idx])\n",
    "            out.append(c_autograd_model.pdf(z[None,:]))\n",
    "\n",
    "    out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "    return out\n",
    "\n",
    "autograd_outs = cond_zs_params_to_pdfs(z, c_params)\n",
    "outs = c_model.pdf(z, c_params)\n",
    "assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "# testing the inverse_cdf function\n",
    "\n",
    "def cond_zs_params_to_icdfs(ys, zs, params):\n",
    "    out = []\n",
    "    for y, z, param in zip(ys, zs, params):\n",
    "        for d_ in range(d):\n",
    "            c_autograd_model = ModelInverse(arch=c_arch, start=start, end=end, store_weights=False,\n",
    "                                           constraint_type=constraint_type, monotonic_const=monotonic_const,\n",
    "                                           final_layer_constraint=final_layer_constraint,\n",
    "                                           non_conditional_dim=d_)\n",
    "            start_idx, end_idx = d_ * c_autograd_model.n_params, (d_ + 1) * c_autograd_model.n_params\n",
    "            c_autograd_model.set_params(param[start_idx:end_idx])\n",
    "            out.append(c_autograd_model.F_inv(y[d_:d_+1][None,:], given_x=z[None,:]))\n",
    "\n",
    "    out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "    return out\n",
    "\n",
    "y = torch.rand((n, d)).to(device)\n",
    "autograd_outs = cond_zs_params_to_icdfs(y, z, c_params)\n",
    "outs = c_model.icdf(y, c_params, given_x=z)\n",
    "assert torch.allclose(autograd_outs, outs, atol=1e-1)\n",
    "    \n",
    "for i in range(d):\n",
    "    tmp = torch.cat([z[:,:i], outs[:,i:i+1], z[:,i+1:]], axis=1)\n",
    "    res = c_model.cdf(tmp, c_params)\n",
    "    assert torch.allclose(res[:,i], y[:,i], atol=1e-2)\n",
    "    \n",
    "for i in range(d):\n",
    "    tmp = torch.cat([z[:,:i], outs[:,i:i+1], z[:,i+1:]], axis=1)\n",
    "    res = cond_zs_params_to_cdfs(tmp, c_params)\n",
    "    assert torch.allclose(res[:,i], y[:,i], atol=1e-2)\n",
    "    \n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing autoregressive conditional NITS.\n",
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "print('Testing autoregressive conditional NITS.')\n",
    "start, end = -2., 2.\n",
    "monotonic_const = 1e-2\n",
    "d = 2\n",
    "c_arch = [d, 8, 1]\n",
    "constraint_type = 'exp'\n",
    "final_layer_constraint = 'softmax'\n",
    "device = 'cpu'\n",
    "\n",
    "c_model = ConditionalNITS(d=d, start=start, end=end, arch=c_arch,\n",
    "                          monotonic_const=monotonic_const, constraint_type=constraint_type,\n",
    "                          final_layer_constraint=final_layer_constraint,\n",
    "                          autoregressive=True).to(device)\n",
    "\n",
    "c_params = torch.randn((n, c_model.tot_params))\n",
    "z = torch.linspace(start, end, steps=n, device=device)[:,None].tile((1, d))\n",
    "\n",
    "def causal_mask(x, i):\n",
    "    x = x.clone()[None,:]\n",
    "    x[:,i+1:] = 0.\n",
    "    return x\n",
    "\n",
    "def cond_zs_params_to_cdfs(zs, params):\n",
    "    out = []\n",
    "    for z, param in zip(zs, params):\n",
    "        for d_ in range(d):\n",
    "            c_autograd_model = ModelInverse(arch=c_arch, start=start, end=end, store_weights=False,\n",
    "                                           constraint_type=constraint_type, monotonic_const=monotonic_const,\n",
    "                                           final_layer_constraint=final_layer_constraint,\n",
    "                                           non_conditional_dim=d_)\n",
    "            start_idx, end_idx = d_ * c_autograd_model.n_params, (d_ + 1) * c_autograd_model.n_params\n",
    "            c_autograd_model.set_params(param[start_idx:end_idx])\n",
    "            \n",
    "            # set mask and apply function\n",
    "            z_masked = causal_mask(z, d_)\n",
    "            out.append(c_autograd_model.cdf(z_masked))\n",
    "\n",
    "    out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "    return out\n",
    "\n",
    "autograd_outs = cond_zs_params_to_cdfs(z, c_params)\n",
    "outs = c_model.cdf(z, c_params)\n",
    "assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "def cond_zs_params_to_pdfs(zs, params):\n",
    "    out = []\n",
    "    for z, param in zip(zs, params):\n",
    "        for d_ in range(d):\n",
    "            c_autograd_model = ModelInverse(arch=c_arch, start=start, end=end, store_weights=False,\n",
    "                                           constraint_type=constraint_type, monotonic_const=monotonic_const,\n",
    "                                           final_layer_constraint=final_layer_constraint,\n",
    "                                           non_conditional_dim=d_)\n",
    "            start_idx, end_idx = d_ * c_autograd_model.n_params, (d_ + 1) * c_autograd_model.n_params\n",
    "            c_autograd_model.set_params(param[start_idx:end_idx])\n",
    "            \n",
    "            # set mask and apply function\n",
    "            z_masked = causal_mask(z, d_)\n",
    "            out.append(c_autograd_model.pdf(z_masked))\n",
    "\n",
    "    out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "    return out\n",
    "\n",
    "autograd_outs = cond_zs_params_to_pdfs(z, c_params)\n",
    "outs = c_model.pdf(z, c_params)\n",
    "assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "# testing the inverse_cdf function\n",
    "\n",
    "def cond_zs_params_to_icdfs(ys, zs, params):\n",
    "    out = []\n",
    "    for y, z, param in zip(ys, zs, params):\n",
    "        for d_ in range(d):\n",
    "            c_autograd_model = ModelInverse(arch=c_arch, start=start, end=end, store_weights=False,\n",
    "                                           constraint_type=constraint_type, monotonic_const=monotonic_const,\n",
    "                                           final_layer_constraint=final_layer_constraint,\n",
    "                                           non_conditional_dim=d_)\n",
    "            start_idx, end_idx = d_ * c_autograd_model.n_params, (d_ + 1) * c_autograd_model.n_params\n",
    "            c_autograd_model.set_params(param[start_idx:end_idx])\n",
    "            \n",
    "            # set mask and apply function\n",
    "            z_masked = torch.cat(out[len(out)-d_:] + [torch.zeros((1, d - d_))], axis=1)\n",
    "            out.append(c_autograd_model.F_inv(y[d_:d_+1][None,:], given_x=z_masked))\n",
    "\n",
    "    out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "    return out\n",
    "\n",
    "y = torch.rand((n, d)).to(device)\n",
    "autograd_outs = cond_zs_params_to_icdfs(y, z, c_params)\n",
    "outs = c_model.icdf(y, c_params)\n",
    "assert torch.allclose(autograd_outs, outs, atol=1e-1)\n",
    "\n",
    "assert torch.allclose(c_model.cdf(outs, c_params), y, atol=1e-3)  \n",
    "assert torch.allclose(cond_zs_params_to_cdfs(autograd_outs, c_params), y, atol=1e-3)\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalNITS(NITSPrimitive):\n",
    "    # TODO: for now, just implement ConditionalNITS such that it sequentially evaluates each dimension\n",
    "    # this process is (probably) possible to vectorize, but since we're currently only doing 3 dimensions,\n",
    "    # there's no need to speed things up, because we only gain a factor of 3 speedup\n",
    "    def __init__(self, d, arch, start=-2., end=2., constraint_type='neg_exp',\n",
    "                 monotonic_const=1e-2, final_layer_constraint='softmax',\n",
    "                 autoregressive=True):\n",
    "        super(ConditionalNITS, self).__init__(arch=arch, start=start, end=end,\n",
    "                                           constraint_type=constraint_type,\n",
    "                                           monotonic_const=monotonic_const,\n",
    "                                           final_layer_constraint=final_layer_constraint)\n",
    "        self.d = d\n",
    "        self.tot_params = self.n_params * d\n",
    "        self.final_layer_constraint = final_layer_constraint\n",
    "        self.autoregressive = autoregressive\n",
    "\n",
    "        self.register_buffer('start', torch.tensor(start).reshape(1, 1).tile(1, d))\n",
    "        self.register_buffer('end', torch.tensor(end).reshape(1, 1).tile(1, d))\n",
    "\n",
    "        assert arch[0] == d\n",
    "        self.nits_list = torch.nn.ModuleList()\n",
    "        for i in range(self.d):\n",
    "            model = NITSPrimitive(arch=arch, start=start, end=end,\n",
    "                         constraint_type=constraint_type,\n",
    "                         monotonic_const=monotonic_const,\n",
    "                         final_layer_constraint=final_layer_constraint,\n",
    "                         non_conditional_dim=i)\n",
    "            self.nits_list.append(model)\n",
    "\n",
    "    def causal_mask(self, x, i):\n",
    "        if self.autoregressive:\n",
    "            x = x.clone()\n",
    "            x[:,i+1:] = 0.\n",
    "        return x\n",
    "\n",
    "    def apply_conditional_func(self, func, x, params):\n",
    "        n = max(len(x), len(params))\n",
    "        result = func(x, params)\n",
    "\n",
    "        if isinstance(result, tuple):\n",
    "            return (result[0].reshape((n, -1)),) + result[1:]\n",
    "        else:\n",
    "            return result.reshape((n, -1))\n",
    "\n",
    "    def forward_(self, x, params, return_intermediaries=False):\n",
    "        result = []\n",
    "        for i in range(self.d):\n",
    "            x_masked = self.causal_mask(x, i)\n",
    "            start_idx, end_idx = i * self.n_params, (i + 1) * self.n_params\n",
    "            func = lambda x, params: self.nits_list[i].forward_(x, params, return_intermediaries)\n",
    "            result.append(self.apply_conditional_func(func, x_masked, params[:,start_idx:end_idx]))\n",
    "\n",
    "        result = torch.cat(result, axis=1)\n",
    "        return result\n",
    "\n",
    "    def backward_(self, x, params):\n",
    "        result = []\n",
    "        for i in range(self.d):\n",
    "            x_masked = self.causal_mask(x, i)\n",
    "            start_idx, end_idx = i * self.n_params, (i + 1) * self.n_params\n",
    "            func = self.nits_list[i].backward_\n",
    "            result.append(self.apply_conditional_func(func, x_masked, params[:,start_idx:end_idx]))\n",
    "\n",
    "        result = torch.cat(result, axis=1)\n",
    "        return result\n",
    "\n",
    "    def cdf(self, x, params):\n",
    "        result = []\n",
    "        for i in range(self.d):\n",
    "            x_masked = self.causal_mask(x, i)\n",
    "            start_idx, end_idx = i * self.n_params, (i + 1) * self.n_params\n",
    "            func = self.nits_list[i].cdf\n",
    "            result.append(self.apply_conditional_func(func, x_masked, params[:,start_idx:end_idx]))\n",
    "\n",
    "        result = torch.cat(result, axis=1)\n",
    "        return result\n",
    "\n",
    "    def pdf(self, x, params):\n",
    "        result = []\n",
    "        for i in range(self.d):\n",
    "            x_masked = self.causal_mask(x, i)\n",
    "            start_idx, end_idx = i * self.n_params, (i + 1) * self.n_params\n",
    "            func = self.nits_list[i].pdf\n",
    "            result.append(self.apply_conditional_func(func, x_masked, params[:,start_idx:end_idx]))\n",
    "\n",
    "        result = torch.cat(result, axis=1)\n",
    "        return result\n",
    "\n",
    "    def icdf(self, x, params, given_x=None):\n",
    "        if self.autoregressive and given_x is not None:\n",
    "            raise NotImplementedError('given_x cannot be supplied if autoregressive == True')\n",
    "        \n",
    "        result = []\n",
    "        for i in range(self.d):\n",
    "            if self.autoregressive:\n",
    "                print(result + [torch.zeros(len(x), self.d - len(result))])\n",
    "                given_x = torch.cat(result + [torch.zeros(len(x), self.d - len(result))], axis=1)\n",
    "            start_idx, end_idx = i * self.n_params, (i + 1) * self.n_params\n",
    "            func = lambda x, params: self.nits_list[i].icdf(x, params, given_x=given_x)\n",
    "            result.append(self.apply_conditional_func(func, x, params[:,start_idx:end_idx]))\n",
    "\n",
    "        result = torch.cat(result, axis=1)\n",
    "        return result\n",
    "\n",
    "    def sample(self, n, params):\n",
    "        if len(params) == 1:\n",
    "            params = params.reshape(self.d, self.n_params).tile((n, 1))\n",
    "        elif len(params) == n:\n",
    "            params = params.reshape(-1, self.n_params)\n",
    "            \n",
    "        z = torch.rand((n, self.d)).to(device)\n",
    "            \n",
    "        return self.icdf(z, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing autoregressive conditional NITS loss and sample functions.\n"
     ]
    }
   ],
   "source": [
    "print('Testing autoregressive conditional NITS loss and sample functions.')\n",
    "start, end = -2., 2.\n",
    "monotonic_const = 1e-2\n",
    "d = 3\n",
    "c_arch = [d, 8, 1]\n",
    "constraint_type = 'exp'\n",
    "final_layer_constraint = 'softmax'\n",
    "device = 'cpu'\n",
    "\n",
    "c_model = ConditionalNITS(d=d, start=start, end=end, arch=c_arch,\n",
    "                          monotonic_const=monotonic_const, constraint_type=constraint_type,\n",
    "                          final_layer_constraint=final_layer_constraint,\n",
    "                          autoregressive=True).to(device)\n",
    "\n",
    "c_params = torch.randn((n, c_model.tot_params, 1, 1))\n",
    "z = torch.randn((n, d, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretized_nits_loss(x, l, arch, nits_model):\n",
    "    \"\"\" log-likelihood for mixture of discretized logistics, assumes the data has been rescaled to [-1,1] interval \"\"\"\n",
    "    # Pytorch ordering\n",
    "    x = x.permute(0, 2, 3, 1)\n",
    "    l = l.permute(0, 2, 3, 1)\n",
    "    xs = [int(y) for y in x.size()]\n",
    "    ls = [int(y) for y in l.size()]\n",
    "\n",
    "    # here and below: getting the means and adjusting them based on preceding\n",
    "    # sub-pixels\n",
    "    x = x.contiguous()\n",
    "\n",
    "    nits_model = nits_model.to(x.device)\n",
    "    x = x.reshape(-1, nits_model.d)\n",
    "    params = l.reshape(-1, nits_model.tot_params)\n",
    "\n",
    "    x_plus = (x * 127.5 + .5).round() / 127.5\n",
    "    x_min = (x * 127.5 - .5).round() / 127.5\n",
    "\n",
    "    cdf_delta = nits_model.cdf(x_plus, params) - nits_model.cdf(x_min, params)\n",
    "    log_cdf_plus = nits_model.cdf(x_plus, params).log()\n",
    "    log_one_minus_cdf_min = (1 - nits_model.cdf(x_min, params)).log()\n",
    "    log_pdf_mid = nits_model.pdf(x, params).log()\n",
    "\n",
    "    inner_inner_cond = (cdf_delta > 1e-5).float()\n",
    "    inner_inner_out  = inner_inner_cond * torch.log(torch.clamp(cdf_delta, min=1e-12)) + (1. - inner_inner_cond) * (log_pdf_mid - np.log(127.5))\n",
    "    inner_cond       = (x > 0.999).float()\n",
    "    inner_out        = inner_cond * log_one_minus_cdf_min + (1. - inner_cond) * inner_inner_out\n",
    "    cond             = (x < -0.999).float()\n",
    "    log_probs        = cond * log_cdf_plus + (1. - cond) * inner_out\n",
    "\n",
    "    return -log_probs.sum()\n",
    "\n",
    "def nits_sample(params, arch, i, j, nits_model):\n",
    "    params = params.permute(0, 2, 3, 1)\n",
    "    batch_size, height, width, params_per_pixel = params.shape\n",
    "\n",
    "    nits_model = nits_model.to(params.device)\n",
    "\n",
    "    n_params = nits_model.n_params\n",
    "    n_channels = int(params_per_pixel / n_params)\n",
    "\n",
    "    data = torch.zeros((batch_size, n_channels, height, width))\n",
    "\n",
    "    imgs = nits_model.sample(1, params[:,i,j,:].reshape(-1, nits_model.tot_params)).clamp(min=-1., max=1.)\n",
    "    data[:,:,i,j] = imgs.reshape((batch_size, n_channels))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(120.5176)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discretized_nits_loss(z, c_params, arch, c_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[0., 0., 0.]])]\n",
      "[tensor([[0.4575],\n",
      "        [0.6313],\n",
      "        [0.7710],\n",
      "        [0.6577],\n",
      "        [0.7632],\n",
      "        [0.6108],\n",
      "        [0.3442],\n",
      "        [0.8647]]), tensor([[0., 0.]])]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 8 but got size 1 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-e808b1f8c9fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnits_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0march\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-42-f2b8205efd1b>\u001b[0m in \u001b[0;36mnits_sample\u001b[0;34m(params, arch, i, j, nits_model)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnits_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnits_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtot_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-b9eb2a1ee680>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, n, params)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0micdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-40-b9eb2a1ee680>\u001b[0m in \u001b[0;36micdf\u001b[0;34m(self, x, params, given_x)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautoregressive\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                 \u001b[0mgiven_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0mstart_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnits_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0micdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgiven_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgiven_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 8 but got size 1 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "nits_sample(c_params, arch, 0, 0, c_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
