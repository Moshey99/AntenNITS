{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class PositiveLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, constraint_type='clamp', store_weights=True):\n",
    "        super(PositiveLinear, self).__init__()\n",
    "        self.constraint_type = constraint_type\n",
    "        self.in_features, self.out_features = in_features, out_features\n",
    "\n",
    "        if store_weights:\n",
    "            # initialize weight in Unif(+eps, sqrt(k)), i.e. log_weight in Unif(log(eps), log(sqrt(k)))\n",
    "            # where k = 1 / in_features\n",
    "            pre_weight = torch.rand((out_features, in_features))\n",
    "\n",
    "            if self.constraint_type == 'exp':\n",
    "                init_min, init_max = np.log(1e-2), np.log(np.sqrt(1 / in_features))\n",
    "            elif self.constraint_type == 'clamp':\n",
    "                init_min, init_max = 1e-2, np.sqrt(1 / in_features)\n",
    "            elif self.constraint_type == '':\n",
    "                init_min, init_max = -np.sqrt(1 / in_features), np.sqrt(1 / in_features)\n",
    "            self.pre_weight = (pre_weight * (init_max - init_min)) + init_min\n",
    "\n",
    "            bias = torch.rand((out_features))\n",
    "            scale = 1 / in_features\n",
    "            self.bias = bias * 2 * scale - scale\n",
    "\n",
    "            self.pre_weight, self.bias = nn.Parameter(self.pre_weight), nn.Parameter(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.constraint_type == 'neg_exp':\n",
    "            weight = 1 / self.pre_weight.exp()\n",
    "            return x.mm(weight.T) - (self.bias.unsqueeze(-1) * weight).mean(axis=-1)\n",
    "        elif self.constraint_type == 'exp':\n",
    "            weight = self.pre_weight.exp()\n",
    "        elif self.constraint_type == 'softmax':\n",
    "            weight = F.softmax(self.pre_weight, dim=-1)\n",
    "        elif self.constraint_type == 'clamp':\n",
    "            weight = self.pre_weight.clamp(min=0.)\n",
    "        elif self.constraint_type == '':\n",
    "            weight = self.pre_weight\n",
    "        return x.mm(weight.T) + self.bias\n",
    "    \n",
    "def bisection_search(increasing_func, target, start, end, n_iter=20, eps=1e-3):\n",
    "    query = (start + end) / 2\n",
    "    result = increasing_func(query)\n",
    "\n",
    "    if n_iter == 0:\n",
    "        print(\"bottomed out recursion depth, return best guess epsilon =\", (result - target).norm())\n",
    "        return query\n",
    "    elif (result - target).norm() < eps:\n",
    "        return query\n",
    "    elif result > target:\n",
    "        return bisection_search(increasing_func, target, start, query, n_iter-1, eps)\n",
    "    else:\n",
    "        return bisection_search(increasing_func, target, query, end, n_iter-1, eps)\n",
    "\n",
    "class MonotonicInverse(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, self, input, given_x):\n",
    "        with torch.no_grad():\n",
    "            b = bisection_search(self.F, input, self.start_(given_x), self.end_(given_x), n_iter=20)\n",
    "\n",
    "        dy = 1 / torch.autograd.functional.jacobian(self.F, b, create_graph=True, vectorize=True)\n",
    "        ctx.save_for_backward(dy.reshape(len(input), -1))\n",
    "        return b[:, self.non_conditional_dim].unsqueeze(-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        dy, = ctx.saved_tensors\n",
    "        return None, dy\n",
    "\n",
    "class ModelInverse(nn.Module):\n",
    "    def __init__(self, arch, start=0., end=1., store_weights=True, \n",
    "                 constraint_type='exp', monotonic_const=1e-3, \n",
    "                 final_layer_constraint='exp', non_conditional_dim=0):\n",
    "        super(ModelInverse, self).__init__()\n",
    "        self.d = arch[0]\n",
    "        self.monotonic_const = monotonic_const\n",
    "        self.store_weights = store_weights\n",
    "        self.constraint_type = constraint_type\n",
    "        self.final_layer_constraint = final_layer_constraint\n",
    "        self.last_layer = len(arch) - 2\n",
    "        self.layers = self.build_layers(arch)\n",
    "\n",
    "        # set start and end tensors\n",
    "        assert non_conditional_dim < self.d\n",
    "        self.non_conditional_dim = non_conditional_dim\n",
    "        self.register_buffer('start_val', torch.tensor(start))\n",
    "        self.register_buffer('end_val', torch.tensor(end))\n",
    "        \n",
    "    def start_(self, x):\n",
    "        if x is None:\n",
    "            assert self.d == 1\n",
    "            start = torch.ones((1, 1)) * self.start_val\n",
    "        else:\n",
    "            start = x.clone().detach()\n",
    "            start[:, self.non_conditional_dim] = self.start_val\n",
    "        \n",
    "        return start\n",
    "    \n",
    "    def end_(self, x):\n",
    "        if x is None:\n",
    "            assert self.d == 1\n",
    "            end = torch.ones((1, 1)) * self.end_val\n",
    "        else:\n",
    "            end = x.clone().detach()\n",
    "            end[:, self.non_conditional_dim] = self.end_val\n",
    "        \n",
    "        return end\n",
    "\n",
    "    def build_layers(self, arch):\n",
    "        self.n_params = 0\n",
    "        layers = nn.ModuleList()\n",
    "\n",
    "        for i, (a1, a2) in enumerate(zip(arch[:-1], arch[1:])):\n",
    "            # add nonlinearities\n",
    "            self.n_params += (a1 * a2)\n",
    "            if i < self.last_layer:\n",
    "                layers.append(PositiveLinear(a1, a2, store_weights=self.store_weights,\n",
    "                                         constraint_type=self.constraint_type))\n",
    "                layers.append(nn.Sigmoid())\n",
    "                self.n_params += a2\n",
    "            else:\n",
    "                layers.append(PositiveLinear(a1, a2, store_weights=self.store_weights,\n",
    "                                         constraint_type=self.final_layer_constraint))\n",
    "                if self.final_layer_constraint != 'softmax':\n",
    "                    layers.append(nn.Sigmoid())\n",
    "                    self.n_params += a2\n",
    "\n",
    "        return layers\n",
    "\n",
    "    def set_params(self, param_tensor):\n",
    "        if self.store_weights:\n",
    "            raise NotImplementedError(\"set_parameters() should not be called if store_weights == True!\")\n",
    "\n",
    "        assert len(param_tensor) == self.n_params, \"{} =/= {}\".format(str(param_tensor.shape), str(self.n_params))\n",
    "\n",
    "        cur_idx = 0\n",
    "        i = 0\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, PositiveLinear):\n",
    "                weight_shape = (layer.out_features, layer.in_features)\n",
    "                n_params = np.prod(weight_shape)\n",
    "                layer.pre_weight = param_tensor[cur_idx:cur_idx+n_params].reshape(weight_shape)\n",
    "                cur_idx += n_params\n",
    "\n",
    "                if i < self.last_layer or self.final_layer_constraint != 'softmax':\n",
    "                    layer.bias = param_tensor[cur_idx:cur_idx+layer.out_features]\n",
    "                    cur_idx += layer.out_features\n",
    "                else:\n",
    "                    layer.bias = torch.zeros(layer.out_features).to(param_tensor.device)\n",
    "                    \n",
    "                i += 1\n",
    "\n",
    "    def apply_layers(self, x):\n",
    "        y = x\n",
    "        for l in self.layers:\n",
    "            y = l(y)\n",
    "\n",
    "        return y + self.monotonic_const * x[:,self.non_conditional_dim].unsqueeze(-1)\n",
    "\n",
    "    def scale(self, y, x):\n",
    "        start, end = self.apply_layers(self.start_(x)), self.apply_layers(self.end_(x))\n",
    "        return (y - start) / (end - start)\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError(\"forward() should not be used!\")\n",
    "        \n",
    "    def f_primitive(self, x, func):\n",
    "        # compute df/dx\n",
    "        dy = []\n",
    "        for x_ in x:\n",
    "            dy_ = torch.autograd.functional.jacobian(func, x_.reshape(-1, self.d), \n",
    "                                                     create_graph=True, vectorize=False)\n",
    "            dy.append(dy_.reshape(-1, self.d)[:,self.non_conditional_dim].unsqueeze(-1))\n",
    "\n",
    "        dy = torch.cat(dy, axis=0)\n",
    "        return dy\n",
    "\n",
    "    def f(self, x):\n",
    "        return self.f_primitive(x, self.F)\n",
    "\n",
    "    def f_(self, x):\n",
    "        return self.f_primitive(x, self.apply_layers)\n",
    "\n",
    "    def F(self, x):\n",
    "        return self.scale(self.apply_layers(x), x)\n",
    "\n",
    "    def pdf(self, x):\n",
    "        return self.f(x)\n",
    "\n",
    "    def cdf(self, x):\n",
    "        return self.F(x)\n",
    "\n",
    "    def F_inv(self, x, given_x=None):\n",
    "        inverse = MonotonicInverse.apply\n",
    "\n",
    "        z = []\n",
    "        for x_ in x:\n",
    "            z.append(inverse(self, x_.reshape(1, 1), given_x).reshape(1, 1))\n",
    "\n",
    "        z = torch.cat(z, axis=0)\n",
    "\n",
    "        return z\n",
    "\n",
    "    def sample(self, n, given_x=None, batch_size=1):\n",
    "        x = []\n",
    "        while n > batch_size:\n",
    "            start, end = self.start_(given_x), self.end_(given_x)\n",
    "            z = torch.rand(batch_size, device=self.start_val.device) * (end - start) + start\n",
    "            x.append(self.F_inv(z.reshape(-1, 1)))\n",
    "            n -= batch_size\n",
    "        else:\n",
    "            start, end = self.start_(given_x), self.end_(given_x)\n",
    "            z = torch.rand(n, device=self.start_val.device) * (end - start) + start\n",
    "            x.append(self.F_inv(z.reshape(-1, 1)))\n",
    "\n",
    "        return torch.cat(x, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class NITSMonotonicInverse(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, self, input, params, given_x):\n",
    "        with torch.no_grad():\n",
    "            b = self.bisection_search(input, params, given_x)\n",
    "\n",
    "        dy = 1 / self.pdf(b, params)\n",
    "        ctx.save_for_backward(dy.reshape(len(input), -1))\n",
    "        b = b[:, self.non_conditional_dim].unsqueeze(-1)\n",
    "        return b\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        dy, = ctx.saved_tensors\n",
    "        return None, dy\n",
    "\n",
    "class NITSPrimitive(nn.Module):\n",
    "    def __init__(self, arch, start=0., end=1., constraint_type='exp',\n",
    "                 monotonic_const=1e-3, activation='sigmoid', \n",
    "                 final_layer_constraint='exp', non_conditional_dim=0):\n",
    "        super(NITSPrimitive, self).__init__()\n",
    "        self.arch = arch\n",
    "        self.monotonic_const = monotonic_const\n",
    "        self.constraint_type = constraint_type\n",
    "        self.final_layer_constraint = final_layer_constraint\n",
    "        self.last_layer = len(arch) - 2\n",
    "        self.activation = activation\n",
    "\n",
    "        # count parameters\n",
    "        self.n_params = 0\n",
    "\n",
    "        for i, (a1, a2) in enumerate(zip(arch[:-1], arch[1:])):\n",
    "            self.n_params += (a1 * a2)\n",
    "            if i < self.last_layer or final_layer_constraint != 'softmax':\n",
    "                self.n_params += a2\n",
    "\n",
    "        # set start and end tensors\n",
    "        self.d = arch[0]\n",
    "        assert non_conditional_dim < self.d\n",
    "        self.non_conditional_dim = non_conditional_dim\n",
    "        self.start_val, self.end_val = start, end\n",
    "        \n",
    "    def start_(self, x):\n",
    "        if x is None:\n",
    "            assert self.d == 1\n",
    "            start = torch.ones((1, 1)) * self.start_val\n",
    "        else:\n",
    "            start = x.clone().detach()\n",
    "            start[:, self.non_conditional_dim] = self.start_val\n",
    "        \n",
    "        return start\n",
    "    \n",
    "    def end_(self, x):\n",
    "        if x is None:\n",
    "            assert self.d == 1\n",
    "            end = torch.ones((1, 1)) * self.end_val\n",
    "        else:\n",
    "            end = x.clone().detach()\n",
    "            end[:, self.non_conditional_dim] = self.end_val\n",
    "        \n",
    "        return end\n",
    "\n",
    "    def apply_constraint(self, A, constraint_type):\n",
    "        if constraint_type == 'neg_exp':\n",
    "            A = (-A).exp()\n",
    "        if constraint_type == 'exp':\n",
    "            A = A.exp()\n",
    "        elif constraint_type == 'clamp':\n",
    "            A = A.clamp(min=0.)\n",
    "        elif constraint_type == 'softmax':\n",
    "            A = F.softmax(A, dim=-1)\n",
    "        elif constraint_type == '':\n",
    "            pass\n",
    "\n",
    "        return A\n",
    "\n",
    "    def apply_act(self, x):\n",
    "        if self.activation == 'tanh':\n",
    "            return x.tanh()\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return x.sigmoid()\n",
    "        elif self.activation == 'linear':\n",
    "            return x\n",
    "\n",
    "    def forward_(self, x, params, return_intermediaries=False):\n",
    "        # the monotonic constant is only applied w.r.t. the first input dimension\n",
    "        monotonic_x = x[:,self.non_conditional_dim].unsqueeze(-1)\n",
    "\n",
    "        # store pre-activations and weight matrices\n",
    "        pre_activations = []\n",
    "        nonlinearities = []\n",
    "        As = []\n",
    "        bs = []\n",
    "\n",
    "        cur_idx = 0\n",
    "\n",
    "        # compute layers\n",
    "        for i, (in_features, out_features) in enumerate(zip(self.arch[:-1], self.arch[1:])):\n",
    "            # get linear weights\n",
    "            A_end = cur_idx + in_features * out_features\n",
    "            A = params[:,cur_idx:A_end].reshape(-1, out_features, in_features)\n",
    "            cur_idx = A_end\n",
    "\n",
    "            constraint = self.constraint_type if i < self.last_layer else self.final_layer_constraint\n",
    "            A = self.apply_constraint(A, constraint)\n",
    "            As.append(A)\n",
    "            x = torch.einsum('nij,nj->ni', A, x)\n",
    "\n",
    "            # get bias weights if not softmax layer\n",
    "            if i < self.last_layer or self.final_layer_constraint != 'softmax':\n",
    "                b_end = A_end + out_features\n",
    "                b = params[:,A_end:b_end].reshape(-1, out_features)\n",
    "                bs.append(b)\n",
    "                cur_idx = b_end\n",
    "                if i < self.last_layer and self.constraint_type == 'neg_exp':\n",
    "                    x = x - (b.unsqueeze(-1) * A).mean(axis=-1)\n",
    "                elif i == self.last_layer and self.final_layer_constraint == 'neg_exp':\n",
    "                    x = x - (b.unsqueeze(-1) * A).mean(axis=-1)\n",
    "                else:\n",
    "                    x = x + b\n",
    "                pre_activations.append(x)\n",
    "                x = self.apply_act(x)\n",
    "                nonlinearities.append(self.activation)\n",
    "            else:\n",
    "                pre_activations.append(x)\n",
    "                nonlinearities.append('linear')\n",
    "        \n",
    "        x = x + self.monotonic_const * monotonic_x\n",
    "\n",
    "        if return_intermediaries:\n",
    "            return x, pre_activations, As, bs, nonlinearities\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def cdf(self, x, params, return_intermediaries=False):\n",
    "        # get scaling factors\n",
    "        start = self.forward_(self.start_(x), params)\n",
    "        end = self.forward_(self.end_(x), params)\n",
    "\n",
    "        # compute pre-scaled cdf, then scale\n",
    "        y, pre_activations, As, bs, nonlinearities = self.forward_(x, params, return_intermediaries=True)\n",
    "        scale = 1 / (end - start)\n",
    "        y_scaled = (y - start) * scale\n",
    "\n",
    "        # accounting\n",
    "        pre_activations.append(y_scaled)\n",
    "        As.append(scale.reshape(-1, 1, 1))\n",
    "        nonlinearities.append('linear')\n",
    "\n",
    "        if return_intermediaries:\n",
    "            return y_scaled, pre_activations, As, bs, nonlinearities\n",
    "        else:\n",
    "            return y_scaled\n",
    "\n",
    "    def fc_gradient(self, grad, pre_activation, A, activation):\n",
    "        if activation == 'linear':\n",
    "            pass\n",
    "        elif activation == 'tanh':\n",
    "            grad = grad * (1 - pre_activation.tanh() ** 2)\n",
    "        elif activation == 'sigmoid':\n",
    "            sig_act = pre_activation.sigmoid()\n",
    "            grad = grad * sig_act * (1 - sig_act)\n",
    "\n",
    "        return torch.einsum('ni,nij->nj', grad, A)\n",
    "\n",
    "    def backward_primitive_(self, y, pre_activations, As, bs, nonlinearities):\n",
    "        pre_activations.reverse()\n",
    "        As.reverse()\n",
    "        nonlinearities.reverse()\n",
    "        grad = torch.ones_like(y, device=y.device)\n",
    "\n",
    "        for i, (A, pre_activation, nonlinearity) in enumerate(zip(As, pre_activations, nonlinearities)):\n",
    "            grad = self.fc_gradient(grad, pre_activation, A, activation=nonlinearity)\n",
    "        \n",
    "        # we only want gradients w.r.t. the first input dimension\n",
    "        return grad[:,self.non_conditional_dim].unsqueeze(-1)\n",
    "\n",
    "    def backward_(self, x, params):\n",
    "        y, pre_activations, As, bs, nonlinearities = self.forward_(x, params, return_intermediaries=True)\n",
    "\n",
    "        grad = self.backward_primitive_(y, pre_activations, As, bs, nonlinearities)\n",
    "\n",
    "        return grad + self.monotonic_const\n",
    "\n",
    "    def pdf(self, x, params):\n",
    "        y, pre_activations, As, bs, nonlinearities = self.cdf(x, params, return_intermediaries=True)\n",
    "\n",
    "        grad = self.backward_primitive_(y, pre_activations, As, bs, nonlinearities)\n",
    "\n",
    "        return grad + self.monotonic_const * As[0].reshape(-1, 1)\n",
    "\n",
    "    def sample(self, params, given_x=None):\n",
    "        z = torch.rand((len(params), 1), device=params.device)\n",
    "        x = self.icdf(z, params, given_x=given_x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def icdf(self, z, params, given_x=None):\n",
    "        func = NITSMonotonicInverse.apply\n",
    "\n",
    "        return func(self, z, params, given_x)\n",
    "\n",
    "    def bisection_search(self, y, params, given_x, eps=1e-3):\n",
    "        low = self.start_(given_x)\n",
    "        high = self.end_(given_x)\n",
    "        \n",
    "        while ((high - low) > eps).any():\n",
    "            x_hat = (low + high) / 2\n",
    "            y_hat = self.cdf(x_hat, params)\n",
    "            low = torch.where(y_hat > y, low, x_hat)\n",
    "            high = torch.where(y_hat > y, x_hat, high)\n",
    "        \n",
    "        result = ((high + low) / 2)\n",
    "        \n",
    "        return result\n",
    "\n",
    "class NITS(NITSPrimitive):\n",
    "    def __init__(self, d, arch, start=-2., end=2., constraint_type='neg_exp',\n",
    "                 monotonic_const=1e-2, final_layer_constraint='softmax'):\n",
    "        super(NITS, self).__init__(arch, start, end,\n",
    "                                           constraint_type=constraint_type,\n",
    "                                           monotonic_const=monotonic_const,\n",
    "                                           final_layer_constraint=final_layer_constraint)\n",
    "        self.d = d\n",
    "        self.tot_params = self.n_params * d\n",
    "        self.final_layer_constraint = final_layer_constraint\n",
    "        \n",
    "        self.register_buffer('start', torch.tensor(start).reshape(1, 1).tile(1, d))\n",
    "        self.register_buffer('end', torch.tensor(end).reshape(1, 1).tile(1, d))\n",
    "        \n",
    "        self.nits = NITSPrimitive(arch, start, end, \n",
    "                         constraint_type=constraint_type,\n",
    "                         monotonic_const=monotonic_const,\n",
    "                         final_layer_constraint=final_layer_constraint)\n",
    "\n",
    "    def multidim_reshape(self, x, params):\n",
    "        n = max(len(x), len(params))\n",
    "        _, d = x.shape\n",
    "        assert d == self.d\n",
    "        assert params.shape[1] == self.tot_params\n",
    "        assert len(x) == len(params) or len(x) == 1 or len(params) == 1\n",
    "        \n",
    "        if len(params) == 1:\n",
    "            params = params.reshape(self.d, self.n_params).tile((n, 1))\n",
    "        elif len(params) == n:\n",
    "            params = params.reshape(-1, self.n_params)\n",
    "        else:\n",
    "            raise NotImplementedError('len(params) should be 1 or {}, but it is {}.'.format(n, len(params)))\n",
    "        \n",
    "        if len(x) == 1:\n",
    "            x = x.reshape(1, self.d).tile((n, 1)).reshape(-1, 1)\n",
    "        elif len(x) == n:\n",
    "            x = x.reshape(-1, 1)\n",
    "        else:\n",
    "            raise NotImplementedError('len(params) should be 1 or {}, but it is {}.'.format(n, len(x)))\n",
    "\n",
    "        return x, params\n",
    "    \n",
    "    def apply_conditional_func(self, func, x, params):\n",
    "        n = max(len(x), len(params))\n",
    "        x, params = self.multidim_reshape(x, params)\n",
    "        result = func(x, params)\n",
    "        \n",
    "        if isinstance(result, tuple):\n",
    "            return (result[0].reshape((n, self.d)),) + result[1:]\n",
    "        else:\n",
    "            return result.reshape((n, self.d))\n",
    "\n",
    "    def forward_(self, x, params, return_intermediaries=False):\n",
    "        func = lambda x, params: self.nits.forward_(x, params, return_intermediaries)\n",
    "        return self.apply_conditional_func(func, x, params)\n",
    "\n",
    "    def backward_(self, x, params):\n",
    "        return self.apply_conditional_func(self.nits.backward_, x, params)\n",
    "\n",
    "    def cdf(self, x, params):\n",
    "        return self.apply_conditional_func(self.nits.cdf, x, params)\n",
    "\n",
    "    def icdf(self, x, params):\n",
    "        return self.apply_conditional_func(self.nits.icdf, x, params)\n",
    "\n",
    "    def pdf(self, x, params):\n",
    "        return self.apply_conditional_func(self.nits.pdf, x, params)\n",
    "\n",
    "    def sample(self, n, params):\n",
    "        if len(params) == 1:\n",
    "            params = params.reshape(self.d, self.n_params).tile((n, 1))\n",
    "        elif len(params) == n:\n",
    "            params = params.reshape(-1, self.n_params)\n",
    "\n",
    "        return self.nits.sample(params).reshape((-1, self.d))\n",
    "\n",
    "    def initialize_parameters(self, n, constraint_type):\n",
    "        params = torch.rand((self.d * n, self.n_params))\n",
    "\n",
    "        def init_constant(params, in_features, constraint_type):\n",
    "            const = np.sqrt(1 / in_features)\n",
    "            if constraint_type == 'clamp':\n",
    "                params = params.abs() * const\n",
    "            elif constraint_type == 'exp':\n",
    "                params = params * np.log(const)\n",
    "            elif constraint_type == 'tanh':\n",
    "                params = params * np.arctanh(const - 1)\n",
    "\n",
    "            return params\n",
    "\n",
    "        cur_idx = 0\n",
    "\n",
    "        for i, (a1, a2) in enumerate(zip(self.arch[:-1], self.arch[1:])):\n",
    "            next_idx = cur_idx + (a1 * a2)\n",
    "            if i < len(self.arch) - 2 or self.final_layer_constraint != 'softmax':\n",
    "                 next_idx = next_idx + a2\n",
    "            params[:,cur_idx:next_idx] = init_constant(params[:,cur_idx:next_idx], a2, constraint_type)\n",
    "            cur_idx = next_idx\n",
    "\n",
    "        return params.reshape((n, self.d * self.n_params))\n",
    "    \n",
    "class ConditionalNITS(NITSPrimitive):\n",
    "    # TODO: for now, just implement ConditionalNITS such that it sequentially evaluates each dimension\n",
    "    # this process is (probably) possible to vectorize, but since we're currently only doing 3 dimensions,\n",
    "    # there's no need to speed things up, because we only gain a factor of 3 speedup\n",
    "    def __init__(self, d, arch, start=-2., end=2., constraint_type='neg_exp',\n",
    "                 monotonic_const=1e-2, final_layer_constraint='softmax',\n",
    "                 autoregressive=True):\n",
    "        super(ConditionalNITS, self).__init__(arch=arch, start=start, end=end,\n",
    "                                           constraint_type=constraint_type,\n",
    "                                           monotonic_const=monotonic_const,\n",
    "                                           final_layer_constraint=final_layer_constraint)\n",
    "        self.d = d\n",
    "        self.tot_params = self.n_params * d\n",
    "        self.final_layer_constraint = final_layer_constraint\n",
    "        self.autoregressive = autoregressive\n",
    "        \n",
    "        self.register_buffer('start', torch.tensor(start).reshape(1, 1).tile(1, d))\n",
    "        self.register_buffer('end', torch.tensor(end).reshape(1, 1).tile(1, d))\n",
    "        \n",
    "        assert arch[0] == d\n",
    "        self.nits_list = torch.nn.ModuleList()\n",
    "        for i in range(self.d):\n",
    "            model = NITSPrimitive(arch=arch, start=start, end=end,\n",
    "                         constraint_type=constraint_type,\n",
    "                         monotonic_const=monotonic_const,\n",
    "                         final_layer_constraint=final_layer_constraint,\n",
    "                         non_conditional_dim=i)\n",
    "            self.nits_list.append(model)\n",
    "            \n",
    "    def causal_mask(self, x, i):\n",
    "        if self.autoregressive:\n",
    "            x = x.clone()\n",
    "            x[:,i+1:] = 0.\n",
    "        return x\n",
    "    \n",
    "    def apply_conditional_func(self, func, x, params):\n",
    "        n = max(len(x), len(params))\n",
    "        result = func(x, params)\n",
    "        \n",
    "        if isinstance(result, tuple):\n",
    "            return (result[0].reshape((n, -1)),) + result[1:]\n",
    "        else:\n",
    "            return result.reshape((n, -1))\n",
    "\n",
    "    def forward_(self, x, params, return_intermediaries=False):\n",
    "        result = []\n",
    "        for i in range(self.d):\n",
    "            x_masked = self.causal_mask(x, i)\n",
    "            start_idx, end_idx = i * self.n_params, (i + 1) * self.n_params\n",
    "            func = lambda x, params: self.nits_list[i].forward_(x, params, return_intermediaries)\n",
    "            result.append(self.apply_conditional_func(func, x_masked, params[:,start_idx:end_idx]))\n",
    "            \n",
    "        result = torch.cat(result, axis=1)\n",
    "        return result\n",
    "\n",
    "    def backward_(self, x, params):\n",
    "        result = []\n",
    "        for i in range(self.d):\n",
    "            x_masked = self.causal_mask(x, i)\n",
    "            start_idx, end_idx = i * self.n_params, (i + 1) * self.n_params\n",
    "            func = self.nits_list[i].backward_\n",
    "            result.append(self.apply_conditional_func(func, x_masked, params[:,start_idx:end_idx]))\n",
    "            \n",
    "        result = torch.cat(result, axis=1)\n",
    "        return result\n",
    "\n",
    "    def cdf(self, x, params):\n",
    "        result = []\n",
    "        for i in range(self.d):\n",
    "            x_masked = self.causal_mask(x, i)\n",
    "            start_idx, end_idx = i * self.n_params, (i + 1) * self.n_params\n",
    "            func = self.nits_list[i].cdf\n",
    "            result.append(self.apply_conditional_func(func, x_masked, params[:,start_idx:end_idx]))\n",
    "            \n",
    "        result = torch.cat(result, axis=1)\n",
    "        return result\n",
    "\n",
    "    def pdf(self, x, params):\n",
    "        result = []\n",
    "        for i in range(self.d):\n",
    "            x_masked = self.causal_mask(x, i)\n",
    "            start_idx, end_idx = i * self.n_params, (i + 1) * self.n_params\n",
    "            func = self.nits_list[i].pdf\n",
    "            result.append(self.apply_conditional_func(func, x_masked, params[:,start_idx:end_idx]))\n",
    "            \n",
    "        result = torch.cat(result, axis=1)\n",
    "        return result\n",
    "    \n",
    "    def icdf(self, x, params, given_x=None):\n",
    "        if self.autoregressive and given_x is not None:\n",
    "            raise NotImplementedError('given_x cannot be supplied if autoregressive == True')\n",
    "            \n",
    "        result = []\n",
    "        for i in range(self.d):\n",
    "            if self.autoregressive:\n",
    "                given_x = torch.cat(result + [torch.zeros(len(x), self.d - len(result))], axis=1)\n",
    "            start_idx, end_idx = i * self.n_params, (i + 1) * self.n_params\n",
    "            func = lambda x, params: self.nits_list[i].icdf(x, params, given_x=given_x)\n",
    "            result.append(self.apply_conditional_func(func, x, params[:,start_idx:end_idx]))\n",
    "            \n",
    "        result = torch.cat(result, axis=1)\n",
    "        return result\n",
    "\n",
    "    def sample(self, n, params):\n",
    "        if len(params) == 1:\n",
    "            params = params.reshape(self.d, self.n_params).tile((n, 1))\n",
    "        elif len(params) == n:\n",
    "            params = params.reshape(-1, self.n_params)\n",
    "        raise NotImplementedError()\n",
    "        return self.nits.sample(params).reshape((-1, self.d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            Testing configuration:\n",
      "                d: 1\n",
      "                constraint_type: neg_exp\n",
      "                final_layer_constraint: softmax\n",
      "                  \n",
      "\n",
      "            Testing configuration:\n",
      "                d: 1\n",
      "                constraint_type: neg_exp\n",
      "                final_layer_constraint: exp\n",
      "                  \n",
      "\n",
      "            Testing configuration:\n",
      "                d: 1\n",
      "                constraint_type: exp\n",
      "                final_layer_constraint: softmax\n",
      "                  \n",
      "\n",
      "            Testing configuration:\n",
      "                d: 1\n",
      "                constraint_type: exp\n",
      "                final_layer_constraint: exp\n",
      "                  \n",
      "\n",
      "            Testing configuration:\n",
      "                d: 2\n",
      "                constraint_type: neg_exp\n",
      "                final_layer_constraint: softmax\n",
      "                  \n",
      "\n",
      "            Testing configuration:\n",
      "                d: 2\n",
      "                constraint_type: neg_exp\n",
      "                final_layer_constraint: exp\n",
      "                  \n",
      "\n",
      "            Testing configuration:\n",
      "                d: 2\n",
      "                constraint_type: exp\n",
      "                final_layer_constraint: softmax\n",
      "                  \n",
      "\n",
      "            Testing configuration:\n",
      "                d: 2\n",
      "                constraint_type: exp\n",
      "                final_layer_constraint: exp\n",
      "                  \n",
      "\n",
      "            Testing configuration:\n",
      "                d: 10\n",
      "                constraint_type: neg_exp\n",
      "                final_layer_constraint: softmax\n",
      "                  \n",
      "\n",
      "            Testing configuration:\n",
      "                d: 10\n",
      "                constraint_type: neg_exp\n",
      "                final_layer_constraint: exp\n",
      "                  \n",
      "\n",
      "            Testing configuration:\n",
      "                d: 10\n",
      "                constraint_type: exp\n",
      "                final_layer_constraint: softmax\n",
      "                  \n",
      "\n",
      "            Testing configuration:\n",
      "                d: 10\n",
      "                constraint_type: exp\n",
      "                final_layer_constraint: exp\n",
      "                  \n",
      "Testing arch = [1, 10, 1], 'neg_exp' constraint_type, 'softmax' final_layer_constraint against discretized mixture of logistics.\n",
      "Finished unit tests. All passed!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from model import *\n",
    "from autograd_model import *\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "n = 128\n",
    "start, end = -2., 2.\n",
    "arch = [1, 8, 1]\n",
    "monotonic_const = 1e-2\n",
    "\n",
    "for d in [1, 2, 10]:\n",
    "    for constraint_type in ['neg_exp', 'exp']:\n",
    "        for final_layer_constraint in ['softmax', 'exp']:\n",
    "            print(\"\"\"\n",
    "            Testing configuration:\n",
    "                d: {}\n",
    "                constraint_type: {}\n",
    "                final_layer_constraint: {}\n",
    "                  \"\"\".format(d, constraint_type, final_layer_constraint))\n",
    "            ############################\n",
    "            # DEFINE MODELS            #\n",
    "            ############################\n",
    "\n",
    "            model = NITS(d=d, start=start, end=end, arch=arch,\n",
    "                                 monotonic_const=monotonic_const, constraint_type=constraint_type,\n",
    "                                 final_layer_constraint=final_layer_constraint).to(device)\n",
    "            params = torch.randn((n, d * model.n_params)).to(device)\n",
    "\n",
    "            ############################\n",
    "            # SANITY CHECKS            #\n",
    "            ############################\n",
    "\n",
    "            # check that the function integrates to 1\n",
    "            assert torch.allclose(torch.ones((n, d)).to(device),\n",
    "                                  model.cdf(model.end, params) - model.cdf(model.start, params), atol=1e-5)\n",
    "\n",
    "            # check that the pdf is all positive\n",
    "            z = torch.linspace(start, end, steps=n, device=device)[:,None].tile((1, d))\n",
    "            assert (model.pdf(z, params) >= 0).all()\n",
    "\n",
    "            # check that the cdf is the inverted\n",
    "            cdf = model.cdf(z, params[0:1])\n",
    "            icdf = model.icdf(cdf, params[0:1])\n",
    "            assert (z - icdf <= 1e-3).all()\n",
    "\n",
    "            ############################\n",
    "            # COMPARE TO AUTOGRAD NITS #\n",
    "            ############################\n",
    "            autograd_model = ModelInverse(arch=arch, start=start, end=end, store_weights=False,\n",
    "                                          constraint_type=constraint_type, monotonic_const=monotonic_const,\n",
    "                                          final_layer_constraint=final_layer_constraint)\n",
    "\n",
    "            def zs_params_to_forwards(zs, params):\n",
    "                out = []\n",
    "                for z, param in zip(zs, params):\n",
    "                    for d_ in range(d):\n",
    "                        start_idx, end_idx = d_ * autograd_model.n_params, (d_ + 1) * autograd_model.n_params\n",
    "                        autograd_model.set_params(param[start_idx:end_idx])\n",
    "                        out.append(autograd_model.apply_layers(z[d_:d_+1][None,:]))\n",
    "\n",
    "                out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "                return out\n",
    "\n",
    "            autograd_outs = zs_params_to_forwards(z, params)\n",
    "            outs = model.forward_(z, params)\n",
    "            assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "            def zs_params_to_cdfs(zs, params):\n",
    "                out = []\n",
    "                for z, param in zip(zs, params):\n",
    "                    for d_ in range(d):\n",
    "                        start_idx, end_idx = d_ * autograd_model.n_params, (d_ + 1) * autograd_model.n_params\n",
    "                        autograd_model.set_params(param[start_idx:end_idx])\n",
    "                        out.append(autograd_model.cdf(z[d_:d_+1][None,:]))\n",
    "\n",
    "                out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "                return out\n",
    "\n",
    "            autograd_outs = zs_params_to_cdfs(z, params)\n",
    "            outs = model.cdf(z, params)\n",
    "            assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "            def zs_params_to_pdfs(zs, params):\n",
    "                out = []\n",
    "                for z, param in zip(zs, params):\n",
    "                    for d_ in range(d):\n",
    "                        start_idx, end_idx = d_ * autograd_model.n_params, (d_ + 1) * autograd_model.n_params\n",
    "                        autograd_model.set_params(param[start_idx:end_idx])\n",
    "                        out.append(autograd_model.pdf(z[d_:d_+1][None,:]))\n",
    "\n",
    "                out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "                return out\n",
    "\n",
    "            autograd_outs = zs_params_to_pdfs(z, params)\n",
    "            outs = model.pdf(z, params)\n",
    "            assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "            \n",
    "            def zs_params_to_icdfs(zs, params):\n",
    "                out = []\n",
    "                for z, param in zip(zs, params):\n",
    "                    for d_ in range(d):\n",
    "                        start_idx, end_idx = d_ * autograd_model.n_params, (d_ + 1) * autograd_model.n_params\n",
    "                        autograd_model.set_params(param[start_idx:end_idx])\n",
    "                        out.append(autograd_model.F_inv(z[d_:d_+1][None,:]))\n",
    "\n",
    "                out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "                return out\n",
    "            \n",
    "            y = torch.rand((n, d)).to(device)\n",
    "            autograd_outs = zs_params_to_icdfs(y, params)\n",
    "            outs = model.icdf(y, params)\n",
    "            assert torch.allclose(autograd_outs, outs, atol=1e-1)\n",
    "\n",
    "            # try with single parameter, many zs\n",
    "\n",
    "            def zs_params_to_pdfs(zs, param):\n",
    "                out = []\n",
    "                for z in zs:\n",
    "                    for d_ in range(d):\n",
    "                        start_idx, end_idx = d_ * autograd_model.n_params, (d_ + 1) * autograd_model.n_params\n",
    "                        autograd_model.set_params(param[start_idx:end_idx])\n",
    "                        out.append(autograd_model.pdf(z[d_:d_+1][None,:]))\n",
    "\n",
    "                out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "                return out\n",
    "\n",
    "            autograd_outs = zs_params_to_pdfs(z, params[0])\n",
    "            outs = model.pdf(z, params[0:1])\n",
    "            assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "            # try with single z, many parameters\n",
    "\n",
    "            def zs_params_to_pdfs(z, params):\n",
    "                out = []\n",
    "                for param in params:\n",
    "                    for d_ in range(d):\n",
    "                        start_idx, end_idx = d_ * autograd_model.n_params, (d_ + 1) * autograd_model.n_params\n",
    "                        autograd_model.set_params(param[start_idx:end_idx])\n",
    "                        out.append(autograd_model.pdf(z[d_:d_+1][None,:]))\n",
    "\n",
    "                out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "                return out\n",
    "\n",
    "            autograd_outs = zs_params_to_pdfs(z[0], params)\n",
    "            outs = model.pdf(z[0:1], params)\n",
    "            assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "from discretized_mol import *\n",
    "print(\"Testing arch = [1, 10, 1], 'neg_exp' constraint_type, 'softmax' final_layer_constraint \" \\\n",
    "      \"against discretized mixture of logistics.\")\n",
    "\n",
    "model = NITS(d=1, start=-1e5, end=1e5, arch=[1, 10, 1],\n",
    "                     monotonic_const=0., constraint_type='neg_exp',\n",
    "                     final_layer_constraint='softmax').to(device)\n",
    "params = torch.randn((n, model.n_params, 1, 1))\n",
    "z = torch.randn((n, 1, 1, 1))\n",
    "\n",
    "loss1 = discretized_mix_logistic_loss_1d3(z, params)\n",
    "loss2 = discretized_nits_loss(z, params, arch=[1, 10, 1], nits_model=model)\n",
    "\n",
    "assert (loss1 - loss2).norm() < 1e-2, (loss1 - loss2).norm()\n",
    "\n",
    "model = NITS(d=1, start=-1e7, end=1e7, arch=[1, 10, 1],\n",
    "                     monotonic_const=0., constraint_type='neg_exp',\n",
    "                     final_layer_constraint='softmax').to(device)\n",
    "\n",
    "loss1 = discretized_mix_logistic_loss_1d3(z, params)\n",
    "loss2 = discretized_nits_loss(z, params, arch=[1, 10, 1], nits_model=model)\n",
    "\n",
    "assert (loss1 - loss2).norm() < 1e-3, (loss1 - loss2).norm()\n",
    "\n",
    "print(\"Finished unit tests. All passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Conditional NITS.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[128, 2]' is invalid for input of size 128",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-cb431c972fc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mautograd_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcond_zs_params_to_cdfs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mautograd_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/NITS/model.py\u001b[0m in \u001b[0;36mcdf\u001b[0;34m(self, x, params)\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0mstart_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnits_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_conditional_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_masked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/NITS/model.py\u001b[0m in \u001b[0;36mapply_conditional_func\u001b[0;34m(self, func, x, params)\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_intermediaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[128, 2]' is invalid for input of size 128"
     ]
    }
   ],
   "source": [
    "print(\"Testing Conditional NITS.\")\n",
    "start, end = -2., 2.\n",
    "monotonic_const = 1e-2\n",
    "d = 2\n",
    "c_arch = [d, 8, 1]\n",
    "constraint_type = 'exp'\n",
    "final_layer_constraint = 'softmax'\n",
    "device = 'cpu'\n",
    "\n",
    "c_model = ConditionalNITS(d=d, start=start, end=end, arch=c_arch,\n",
    "                          monotonic_const=monotonic_const, constraint_type=constraint_type,\n",
    "                          final_layer_constraint=final_layer_constraint,\n",
    "                          autoregressive=False).to(device)\n",
    "\n",
    "c_params = torch.randn((n, c_model.tot_params))\n",
    "z = torch.linspace(start, end, steps=n, device=device)[:,None].tile((1, d))\n",
    "\n",
    "def cond_zs_params_to_cdfs(zs, params):\n",
    "    out = []\n",
    "    for z, param in zip(zs, params):\n",
    "        for d_ in range(d):\n",
    "            c_autograd_model = ModelInverse(arch=c_arch, start=start, end=end, store_weights=False,\n",
    "                                           constraint_type=constraint_type, monotonic_const=monotonic_const,\n",
    "                                           final_layer_constraint=final_layer_constraint,\n",
    "                                           non_conditional_dim=d_)\n",
    "            start_idx, end_idx = d_ * c_autograd_model.n_params, (d_ + 1) * c_autograd_model.n_params\n",
    "            c_autograd_model.set_params(param[start_idx:end_idx])\n",
    "            out.append(c_autograd_model.cdf(z[None,:]))\n",
    "\n",
    "    out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "    return out\n",
    "\n",
    "autograd_outs = cond_zs_params_to_cdfs(z, c_params)\n",
    "outs = c_model.cdf(z, c_params)\n",
    "assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "def cond_zs_params_to_pdfs(zs, params):\n",
    "    out = []\n",
    "    for z, param in zip(zs, params):\n",
    "        for d_ in range(d):\n",
    "            c_autograd_model = ModelInverse(arch=c_arch, start=start, end=end, store_weights=False,\n",
    "                                           constraint_type=constraint_type, monotonic_const=monotonic_const,\n",
    "                                           final_layer_constraint=final_layer_constraint,\n",
    "                                           non_conditional_dim=d_)\n",
    "            start_idx, end_idx = d_ * c_autograd_model.n_params, (d_ + 1) * c_autograd_model.n_params\n",
    "            c_autograd_model.set_params(param[start_idx:end_idx])\n",
    "            out.append(c_autograd_model.pdf(z[None,:]))\n",
    "\n",
    "    out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "    return out\n",
    "\n",
    "autograd_outs = cond_zs_params_to_pdfs(z, c_params)\n",
    "outs = c_model.pdf(z, c_params)\n",
    "assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "# testing the inverse_cdf function\n",
    "\n",
    "def cond_zs_params_to_icdfs(ys, zs, params):\n",
    "    out = []\n",
    "    for y, z, param in zip(ys, zs, params):\n",
    "        for d_ in range(d):\n",
    "            c_autograd_model = ModelInverse(arch=c_arch, start=start, end=end, store_weights=False,\n",
    "                                           constraint_type=constraint_type, monotonic_const=monotonic_const,\n",
    "                                           final_layer_constraint=final_layer_constraint,\n",
    "                                           non_conditional_dim=d_)\n",
    "            start_idx, end_idx = d_ * c_autograd_model.n_params, (d_ + 1) * c_autograd_model.n_params\n",
    "            c_autograd_model.set_params(param[start_idx:end_idx])\n",
    "            out.append(c_autograd_model.F_inv(y[d_:d_+1][None,:], given_x=z[None,:]))\n",
    "\n",
    "    out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "    return out\n",
    "\n",
    "y = torch.rand((n, d)).to(device)\n",
    "autograd_outs = cond_zs_params_to_icdfs(y, z, c_params)\n",
    "outs = c_model.icdf(y, c_params, given_x=z)\n",
    "assert torch.allclose(autograd_outs, outs, atol=1e-1)\n",
    "    \n",
    "for i in range(d):\n",
    "    tmp = torch.cat([z[:,:i], outs[:,i:i+1], z[:,i+1:]], axis=1)\n",
    "    res = c_model.cdf(tmp, c_params)\n",
    "    assert torch.allclose(res[:,i], y[:,i], atol=1e-2)\n",
    "    \n",
    "for i in range(d):\n",
    "    tmp = torch.cat([z[:,:i], outs[:,i:i+1], z[:,i+1:]], axis=1)\n",
    "    res = cond_zs_params_to_cdfs(tmp, c_params)\n",
    "    assert torch.allclose(res[:,i], y[:,i], atol=1e-2)\n",
    "    \n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Testing autoregressive conditional NITS.')\n",
    "start, end = -2., 2.\n",
    "monotonic_const = 1e-2\n",
    "d = 2\n",
    "c_arch = [d, 8, 1]\n",
    "constraint_type = 'exp'\n",
    "final_layer_constraint = 'softmax'\n",
    "device = 'cpu'\n",
    "\n",
    "c_model = ConditionalNITS(d=d, start=start, end=end, arch=c_arch,\n",
    "                          monotonic_const=monotonic_const, constraint_type=constraint_type,\n",
    "                          final_layer_constraint=final_layer_constraint,\n",
    "                          autoregressive=True).to(device)\n",
    "\n",
    "c_params = torch.randn((n, c_model.tot_params))\n",
    "z = torch.linspace(start, end, steps=n, device=device)[:,None].tile((1, d))\n",
    "\n",
    "def causal_mask(x, i):\n",
    "    x = x.clone()[None,:]\n",
    "    x[:,i+1:] = 0.\n",
    "    return x\n",
    "\n",
    "def cond_zs_params_to_cdfs(zs, params):\n",
    "    out = []\n",
    "    for z, param in zip(zs, params):\n",
    "        for d_ in range(d):\n",
    "            c_autograd_model = ModelInverse(arch=c_arch, start=start, end=end, store_weights=False,\n",
    "                                           constraint_type=constraint_type, monotonic_const=monotonic_const,\n",
    "                                           final_layer_constraint=final_layer_constraint,\n",
    "                                           non_conditional_dim=d_)\n",
    "            start_idx, end_idx = d_ * c_autograd_model.n_params, (d_ + 1) * c_autograd_model.n_params\n",
    "            c_autograd_model.set_params(param[start_idx:end_idx])\n",
    "            \n",
    "            # set mask and apply function\n",
    "            z_masked = causal_mask(z, d_)\n",
    "            out.append(c_autograd_model.cdf(z_masked))\n",
    "\n",
    "    out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "    return out\n",
    "\n",
    "autograd_outs = cond_zs_params_to_cdfs(z, c_params)\n",
    "outs = c_model.cdf(z, c_params)\n",
    "assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "def cond_zs_params_to_pdfs(zs, params):\n",
    "    out = []\n",
    "    for z, param in zip(zs, params):\n",
    "        for d_ in range(d):\n",
    "            c_autograd_model = ModelInverse(arch=c_arch, start=start, end=end, store_weights=False,\n",
    "                                           constraint_type=constraint_type, monotonic_const=monotonic_const,\n",
    "                                           final_layer_constraint=final_layer_constraint,\n",
    "                                           non_conditional_dim=d_)\n",
    "            start_idx, end_idx = d_ * c_autograd_model.n_params, (d_ + 1) * c_autograd_model.n_params\n",
    "            c_autograd_model.set_params(param[start_idx:end_idx])\n",
    "            \n",
    "            # set mask and apply function\n",
    "            z_masked = causal_mask(z, d_)\n",
    "            out.append(c_autograd_model.pdf(z_masked))\n",
    "\n",
    "    out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "    return out\n",
    "\n",
    "autograd_outs = cond_zs_params_to_pdfs(z, c_params)\n",
    "outs = c_model.pdf(z, c_params)\n",
    "assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "# testing the inverse_cdf function\n",
    "\n",
    "def cond_zs_params_to_icdfs(ys, zs, params):\n",
    "    out = []\n",
    "    for y, z, param in zip(ys, zs, params):\n",
    "        for d_ in range(d):\n",
    "            c_autograd_model = ModelInverse(arch=c_arch, start=start, end=end, store_weights=False,\n",
    "                                           constraint_type=constraint_type, monotonic_const=monotonic_const,\n",
    "                                           final_layer_constraint=final_layer_constraint,\n",
    "                                           non_conditional_dim=d_)\n",
    "            start_idx, end_idx = d_ * c_autograd_model.n_params, (d_ + 1) * c_autograd_model.n_params\n",
    "            c_autograd_model.set_params(param[start_idx:end_idx])\n",
    "            \n",
    "            # set mask and apply function\n",
    "            z_masked = torch.cat(out[len(out)-d_:] + [torch.zeros((1, d - d_))], axis=1)\n",
    "            out.append(c_autograd_model.F_inv(y[d_:d_+1][None,:], given_x=z_masked))\n",
    "\n",
    "    out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "    return out\n",
    "\n",
    "y = torch.rand((n, d)).to(device)\n",
    "autograd_outs = cond_zs_params_to_icdfs(y, z, c_params)\n",
    "outs = c_model.icdf(y, c_params)\n",
    "assert torch.allclose(autograd_outs, outs, atol=1e-1)\n",
    "\n",
    "assert torch.allclose(c_model.cdf(outs, c_params), y, atol=1e-3)  \n",
    "assert torch.allclose(cond_zs_params_to_cdfs(autograd_outs, c_params), y, atol=1e-3)\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
