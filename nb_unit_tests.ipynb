{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing NITS.\n",
      "Testing arch = [1, 10, 1], 'neg_exp' constraint_type, 'softmax' final_layer_constraint against discretized mixture of logistics.\n",
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from nits.model import *\n",
    "from nits.autograd_model import *\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "n = 8\n",
    "start, end = -2., 2.\n",
    "arch = [1, 8, 1]\n",
    "monotonic_const = 1e-2\n",
    "\n",
    "print(\"Testing NITS.\")\n",
    "\n",
    "for d in [1, 2, 10]:\n",
    "    for constraint_type in ['neg_exp', 'exp']:\n",
    "        for final_layer_constraint in ['softmax', 'exp']:\n",
    "#             print(\"\"\"\n",
    "#             Testing configuration:\n",
    "#                 d: {}\n",
    "#                 constraint_type: {}\n",
    "#                 final_layer_constraint: {}\n",
    "#                   \"\"\".format(d, constraint_type, final_layer_constraint))\n",
    "            ############################\n",
    "            # DEFINE MODELS            #\n",
    "            ############################\n",
    "\n",
    "            model = NITS(d=d, start=start, end=end, arch=arch,\n",
    "                                 monotonic_const=monotonic_const, constraint_type=constraint_type,\n",
    "                                 final_layer_constraint=final_layer_constraint).to(device)\n",
    "            params = torch.randn((n, d * model.n_params)).to(device)\n",
    "\n",
    "            ############################\n",
    "            # SANITY CHECKS            #\n",
    "            ############################\n",
    "\n",
    "            # check that the function integrates to 1\n",
    "            assert torch.allclose(torch.ones((n, d)).to(device),\n",
    "                                  model.cdf(model.end, params) - model.cdf(model.start, params), atol=1e-5)\n",
    "\n",
    "            # check that the pdf is all positive\n",
    "            z = torch.linspace(start, end, steps=n, device=device)[:,None].tile((1, d))\n",
    "            assert (model.pdf(z, params) >= 0).all()\n",
    "\n",
    "            # check that the cdf is the inverted\n",
    "            cdf = model.cdf(z, params[0:1])\n",
    "            icdf = model.icdf(cdf, params[0:1])\n",
    "            assert (z - icdf <= 1e-3).all()\n",
    "\n",
    "            ############################\n",
    "            # COMPARE TO AUTOGRAD NITS #\n",
    "            ############################\n",
    "            autograd_model = ModelInverse(arch=arch, start=start, end=end, store_weights=False,\n",
    "                                          constraint_type=constraint_type, monotonic_const=monotonic_const,\n",
    "                                          final_layer_constraint=final_layer_constraint)\n",
    "\n",
    "            def zs_params_to_forwards(zs, params):\n",
    "                out = []\n",
    "                for z, param in zip(zs, params):\n",
    "                    for d_ in range(d):\n",
    "                        start_idx, end_idx = d_ * autograd_model.n_params, (d_ + 1) * autograd_model.n_params\n",
    "                        autograd_model.set_params(param[start_idx:end_idx])\n",
    "                        out.append(autograd_model.apply_layers(z[d_:d_+1][None,:]))\n",
    "\n",
    "                out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "                return out\n",
    "\n",
    "            autograd_outs = zs_params_to_forwards(z, params)\n",
    "            outs = model.forward_(z, params)\n",
    "            assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "            def zs_params_to_cdfs(zs, params):\n",
    "                out = []\n",
    "                for z, param in zip(zs, params):\n",
    "                    for d_ in range(d):\n",
    "                        start_idx, end_idx = d_ * autograd_model.n_params, (d_ + 1) * autograd_model.n_params\n",
    "                        autograd_model.set_params(param[start_idx:end_idx])\n",
    "                        out.append(autograd_model.cdf(z[d_:d_+1][None,:]))\n",
    "\n",
    "                out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "                return out\n",
    "\n",
    "            autograd_outs = zs_params_to_cdfs(z, params)\n",
    "            outs = model.cdf(z, params)\n",
    "            assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "            def zs_params_to_pdfs(zs, params):\n",
    "                out = []\n",
    "                for z, param in zip(zs, params):\n",
    "                    for d_ in range(d):\n",
    "                        start_idx, end_idx = d_ * autograd_model.n_params, (d_ + 1) * autograd_model.n_params\n",
    "                        autograd_model.set_params(param[start_idx:end_idx])\n",
    "                        out.append(autograd_model.pdf(z[d_:d_+1][None,:]))\n",
    "\n",
    "                out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "                return out\n",
    "\n",
    "            autograd_outs = zs_params_to_pdfs(z, params)\n",
    "            outs = model.pdf(z, params)\n",
    "            assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "            def zs_params_to_icdfs(zs, params):\n",
    "                out = []\n",
    "                for z, param in zip(zs, params):\n",
    "                    for d_ in range(d):\n",
    "                        start_idx, end_idx = d_ * autograd_model.n_params, (d_ + 1) * autograd_model.n_params\n",
    "                        autograd_model.set_params(param[start_idx:end_idx])\n",
    "                        out.append(autograd_model.F_inv(z[d_:d_+1][None,:]))\n",
    "\n",
    "                out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "                return out\n",
    "\n",
    "            y = torch.rand((n, d)).to(device)\n",
    "            autograd_outs = zs_params_to_icdfs(y, params)\n",
    "            outs = model.icdf(y, params)\n",
    "            assert torch.allclose(autograd_outs, outs, atol=1e-1)\n",
    "\n",
    "            # try with single parameter, many zs\n",
    "\n",
    "            def zs_params_to_pdfs(zs, param):\n",
    "                out = []\n",
    "                for z in zs:\n",
    "                    for d_ in range(d):\n",
    "                        start_idx, end_idx = d_ * autograd_model.n_params, (d_ + 1) * autograd_model.n_params\n",
    "                        autograd_model.set_params(param[start_idx:end_idx])\n",
    "                        out.append(autograd_model.pdf(z[d_:d_+1][None,:]))\n",
    "\n",
    "                out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "                return out\n",
    "\n",
    "            autograd_outs = zs_params_to_pdfs(z, params[0])\n",
    "            outs = model.pdf(z, params[0:1])\n",
    "            assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "            # try with single z, many parameters\n",
    "\n",
    "            def zs_params_to_pdfs(z, params):\n",
    "                out = []\n",
    "                for param in params:\n",
    "                    for d_ in range(d):\n",
    "                        start_idx, end_idx = d_ * autograd_model.n_params, (d_ + 1) * autograd_model.n_params\n",
    "                        autograd_model.set_params(param[start_idx:end_idx])\n",
    "                        out.append(autograd_model.pdf(z[d_:d_+1][None,:]))\n",
    "\n",
    "                out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "                return out\n",
    "\n",
    "            autograd_outs = zs_params_to_pdfs(z[0], params)\n",
    "            outs = model.pdf(z[0:1], params)\n",
    "            assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "from nits.discretized_mol import *\n",
    "print(\"Testing arch = [1, 10, 1], 'neg_exp' constraint_type, 'softmax' final_layer_constraint \" \\\n",
    "      \"against discretized mixture of logistics.\")\n",
    "\n",
    "model = NITS(d=1, start=-1e5, end=1e5, arch=[1, 10, 1],\n",
    "                     monotonic_const=0., constraint_type='neg_exp',\n",
    "                     final_layer_constraint='softmax').to(device)\n",
    "params = torch.randn((n, model.n_params, 1, 1))\n",
    "z = torch.randn((n, 1, 1, 1))\n",
    "\n",
    "loss1 = discretized_mix_logistic_loss_1d(z, params)\n",
    "loss2 = discretized_nits_loss(z, params, arch=[1, 10, 1], nits_model=model)\n",
    "\n",
    "assert (loss1 - loss2).norm() < 1e-2, (loss1 - loss2).norm()\n",
    "\n",
    "model = NITS(d=1, start=-1e7, end=1e7, arch=[1, 10, 1],\n",
    "                     monotonic_const=0., constraint_type='neg_exp',\n",
    "                     final_layer_constraint='softmax').to(device)\n",
    "\n",
    "loss1 = discretized_mix_logistic_loss_1d(z, params)\n",
    "loss2 = discretized_nits_loss(z, params, arch=[1, 10, 1], nits_model=model)\n",
    "\n",
    "assert (loss1 - loss2).norm() < 1e-3, (loss1 - loss2).norm()\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Conditional NITS.\n",
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing Conditional NITS.\")\n",
    "start, end = -2., 2.\n",
    "monotonic_const = 1e-2\n",
    "d = 2\n",
    "c_arch = [d, 8, 1]\n",
    "constraint_type = 'exp'\n",
    "final_layer_constraint = 'softmax'\n",
    "device = 'cpu'\n",
    "\n",
    "c_model = ConditionalNITS(d=d, start=start, end=end, arch=c_arch,\n",
    "                          monotonic_const=monotonic_const, constraint_type=constraint_type,\n",
    "                          final_layer_constraint=final_layer_constraint,\n",
    "                          autoregressive=False).to(device)\n",
    "\n",
    "c_params = torch.randn((n, c_model.tot_params))\n",
    "z = torch.linspace(start, end, steps=n, device=device)[:,None].tile((1, d))\n",
    "\n",
    "def cond_zs_params_to_cdfs(zs, params):\n",
    "    out = []\n",
    "    for z, param in zip(zs, params):\n",
    "        for d_ in range(d):\n",
    "            c_autograd_model = ModelInverse(arch=c_arch, start=start, end=end, store_weights=False,\n",
    "                                           constraint_type=constraint_type, monotonic_const=monotonic_const,\n",
    "                                           final_layer_constraint=final_layer_constraint,\n",
    "                                           non_conditional_dim=d_)\n",
    "            start_idx, end_idx = d_ * c_autograd_model.n_params, (d_ + 1) * c_autograd_model.n_params\n",
    "            c_autograd_model.set_params(param[start_idx:end_idx])\n",
    "            out.append(c_autograd_model.cdf(z[None,:]))\n",
    "\n",
    "    out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "    return out\n",
    "\n",
    "autograd_outs = cond_zs_params_to_cdfs(z, c_params)\n",
    "outs = c_model.cdf(z, c_params)\n",
    "assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "def cond_zs_params_to_pdfs(zs, params):\n",
    "    out = []\n",
    "    for z, param in zip(zs, params):\n",
    "        for d_ in range(d):\n",
    "            c_autograd_model = ModelInverse(arch=c_arch, start=start, end=end, store_weights=False,\n",
    "                                           constraint_type=constraint_type, monotonic_const=monotonic_const,\n",
    "                                           final_layer_constraint=final_layer_constraint,\n",
    "                                           non_conditional_dim=d_)\n",
    "            start_idx, end_idx = d_ * c_autograd_model.n_params, (d_ + 1) * c_autograd_model.n_params\n",
    "            c_autograd_model.set_params(param[start_idx:end_idx])\n",
    "            out.append(c_autograd_model.pdf(z[None,:]))\n",
    "\n",
    "    out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "    return out\n",
    "\n",
    "autograd_outs = cond_zs_params_to_pdfs(z, c_params)\n",
    "outs = c_model.pdf(z, c_params)\n",
    "assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "# testing the inverse_cdf function\n",
    "\n",
    "def cond_zs_params_to_icdfs(ys, zs, params):\n",
    "    out = []\n",
    "    for y, z, param in zip(ys, zs, params):\n",
    "        for d_ in range(d):\n",
    "            c_autograd_model = ModelInverse(arch=c_arch, start=start, end=end, store_weights=False,\n",
    "                                           constraint_type=constraint_type, monotonic_const=monotonic_const,\n",
    "                                           final_layer_constraint=final_layer_constraint,\n",
    "                                           non_conditional_dim=d_)\n",
    "            start_idx, end_idx = d_ * c_autograd_model.n_params, (d_ + 1) * c_autograd_model.n_params\n",
    "            c_autograd_model.set_params(param[start_idx:end_idx])\n",
    "            out.append(c_autograd_model.F_inv(y[d_:d_+1][None,:], given_x=z[None,:]))\n",
    "\n",
    "    out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "    return out\n",
    "\n",
    "y = torch.rand((n, d)).to(device)\n",
    "autograd_outs = cond_zs_params_to_icdfs(y, z, c_params)\n",
    "outs = c_model.icdf(y, c_params, given_x=z)\n",
    "assert torch.allclose(autograd_outs, outs, atol=1e-1)\n",
    "    \n",
    "for i in range(d):\n",
    "    tmp = torch.cat([z[:,:i], outs[:,i:i+1], z[:,i+1:]], axis=1)\n",
    "    res = c_model.cdf(tmp, c_params)\n",
    "    assert torch.allclose(res[:,i], y[:,i], atol=1e-2)\n",
    "    \n",
    "for i in range(d):\n",
    "    tmp = torch.cat([z[:,:i], outs[:,i:i+1], z[:,i+1:]], axis=1)\n",
    "    res = cond_zs_params_to_cdfs(tmp, c_params)\n",
    "    assert torch.allclose(res[:,i], y[:,i], atol=1e-2)\n",
    "    \n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing autoregressive conditional NITS.\n",
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "print('Testing autoregressive conditional NITS.')\n",
    "start, end = -2., 2.\n",
    "monotonic_const = 1e-2\n",
    "d = 2\n",
    "c_arch = [d, 8, 1]\n",
    "constraint_type = 'exp'\n",
    "final_layer_constraint = 'softmax'\n",
    "device = 'cpu'\n",
    "\n",
    "c_model = ConditionalNITS(d=d, start=start, end=end, arch=c_arch,\n",
    "                          monotonic_const=monotonic_const, constraint_type=constraint_type,\n",
    "                          final_layer_constraint=final_layer_constraint,\n",
    "                          autoregressive=True).to(device)\n",
    "\n",
    "c_params = torch.randn((n, c_model.tot_params))\n",
    "z = torch.linspace(start, end, steps=n, device=device)[:,None].tile((1, d))\n",
    "\n",
    "def causal_mask(x, i):\n",
    "    x = x.clone()[None,:]\n",
    "    x[:,i+1:] = 0.\n",
    "    return x\n",
    "\n",
    "def cond_zs_params_to_cdfs(zs, params):\n",
    "    out = []\n",
    "    for z, param in zip(zs, params):\n",
    "        for d_ in range(d):\n",
    "            c_autograd_model = ModelInverse(arch=c_arch, start=start, end=end, store_weights=False,\n",
    "                                           constraint_type=constraint_type, monotonic_const=monotonic_const,\n",
    "                                           final_layer_constraint=final_layer_constraint,\n",
    "                                           non_conditional_dim=d_)\n",
    "            start_idx, end_idx = d_ * c_autograd_model.n_params, (d_ + 1) * c_autograd_model.n_params\n",
    "            c_autograd_model.set_params(param[start_idx:end_idx])\n",
    "            \n",
    "            # set mask and apply function\n",
    "            z_masked = causal_mask(z, d_)\n",
    "            out.append(c_autograd_model.cdf(z_masked))\n",
    "\n",
    "    out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "    return out\n",
    "\n",
    "autograd_outs = cond_zs_params_to_cdfs(z, c_params)\n",
    "outs = c_model.cdf(z, c_params)\n",
    "assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "def cond_zs_params_to_pdfs(zs, params):\n",
    "    out = []\n",
    "    for z, param in zip(zs, params):\n",
    "        for d_ in range(d):\n",
    "            c_autograd_model = ModelInverse(arch=c_arch, start=start, end=end, store_weights=False,\n",
    "                                           constraint_type=constraint_type, monotonic_const=monotonic_const,\n",
    "                                           final_layer_constraint=final_layer_constraint,\n",
    "                                           non_conditional_dim=d_)\n",
    "            start_idx, end_idx = d_ * c_autograd_model.n_params, (d_ + 1) * c_autograd_model.n_params\n",
    "            c_autograd_model.set_params(param[start_idx:end_idx])\n",
    "            \n",
    "            # set mask and apply function\n",
    "            z_masked = causal_mask(z, d_)\n",
    "            out.append(c_autograd_model.pdf(z_masked))\n",
    "\n",
    "    out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "    return out\n",
    "\n",
    "autograd_outs = cond_zs_params_to_pdfs(z, c_params)\n",
    "outs = c_model.pdf(z, c_params)\n",
    "assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "# testing the inverse_cdf function\n",
    "\n",
    "def cond_zs_params_to_icdfs(ys, zs, params):\n",
    "    out = []\n",
    "    for y, z, param in zip(ys, zs, params):\n",
    "        for d_ in range(d):\n",
    "            c_autograd_model = ModelInverse(arch=c_arch, start=start, end=end, store_weights=False,\n",
    "                                           constraint_type=constraint_type, monotonic_const=monotonic_const,\n",
    "                                           final_layer_constraint=final_layer_constraint,\n",
    "                                           non_conditional_dim=d_)\n",
    "            start_idx, end_idx = d_ * c_autograd_model.n_params, (d_ + 1) * c_autograd_model.n_params\n",
    "            c_autograd_model.set_params(param[start_idx:end_idx])\n",
    "            \n",
    "            # set mask and apply function\n",
    "            z_masked = torch.cat(out[len(out)-d_:] + [torch.zeros((1, d - d_))], axis=1)\n",
    "            out.append(c_autograd_model.F_inv(y[d_:d_+1][None,:], given_x=z_masked))\n",
    "\n",
    "    out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "    return out\n",
    "\n",
    "y = torch.rand((n, d)).to(device)\n",
    "autograd_outs = cond_zs_params_to_icdfs(y, z, c_params)\n",
    "outs = c_model.icdf(y, c_params)\n",
    "assert torch.allclose(autograd_outs, outs, atol=1e-1)\n",
    "\n",
    "assert torch.allclose(c_model.cdf(outs, c_params), y, atol=1e-3)  \n",
    "assert torch.allclose(cond_zs_params_to_cdfs(autograd_outs, c_params), y, atol=1e-3)\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
