{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from nits.model import *\n",
    "from nits.autograd_model import *\n",
    "\n",
    "device = 'cpu'\n",
    "# device = 'cuda:2'\n",
    "\n",
    "base_arch = [4, 4, 1]\n",
    "\n",
    "n = 32\n",
    "start, end = -2., 2.\n",
    "monotonic_const = 1e-2\n",
    "\n",
    "print(\"Testing NITS.\")\n",
    "\n",
    "for add_residual_connections in [True, False]:\n",
    "    for A_constraint in ['neg_exp', 'exp']:\n",
    "        for final_layer_constraint in ['softmax', 'exp']:\n",
    "    #             print(\"\"\"\n",
    "    #             Testing configuration:\n",
    "    #                 d: {}\n",
    "    #                 A_constraint: {}\n",
    "    #                 final_layer_constraint: {}\n",
    "    #                   \"\"\".format(d, A_constraint, final_layer_constraint))\n",
    "            ############################\n",
    "            # DEFINE MODELS            #\n",
    "            ############################\n",
    "            d = 1\n",
    "            arch = [d] + base_arch\n",
    "            model = NITS(d=d, start=start, end=end, arch=arch,\n",
    "                         monotonic_const=monotonic_const, A_constraint=A_constraint,\n",
    "                         final_layer_constraint=final_layer_constraint,\n",
    "                         add_residual_connections=add_residual_connections).to(device)\n",
    "            params = torch.randn((n, d * model.n_params)).to(device)\n",
    "\n",
    "            ############################\n",
    "            # SANITY CHECKS            #\n",
    "            ############################\n",
    "\n",
    "            # check that the function integrates to 1\n",
    "            assert torch.allclose(torch.ones((n, d)).to(device),\n",
    "                                  model.cdf(model.end, params) - model.cdf(model.start, params), atol=1e-5)\n",
    "\n",
    "            # check that the pdf is all positive\n",
    "            z = torch.linspace(start, end, steps=n, device=device)[:,None].tile((1, d))\n",
    "            assert (model.pdf(z, params) >= 0).all()\n",
    "\n",
    "            # check that the cdf is the inverted\n",
    "            cdf = model.cdf(z, params[0:1])\n",
    "            icdf = model.icdf(cdf, params[0:1])\n",
    "            assert (z - icdf <= 1e-3).all()\n",
    "\n",
    "            ############################\n",
    "            # COMPARE TO AUTOGRAD NITS #\n",
    "            ############################\n",
    "            autograd_model = ModelInverse(arch=arch, start=start, end=end, store_weights=False,\n",
    "                                          A_constraint=A_constraint, monotonic_const=monotonic_const,\n",
    "                                          final_layer_constraint=final_layer_constraint,\n",
    "                                          add_residual_connections=add_residual_connections).to(device)\n",
    "\n",
    "            def zs_params_to_forwards(zs, params):\n",
    "                out = []\n",
    "                for z, param in zip(zs, params):\n",
    "                    for d_ in range(d):\n",
    "                        start_idx, end_idx = d_ * autograd_model.n_params, (d_ + 1) * autograd_model.n_params\n",
    "                        autograd_model.set_params(param[start_idx:end_idx])\n",
    "                        out.append(autograd_model.apply_layers(z[d_:d_+1][None,:]))\n",
    "\n",
    "                out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "                return out\n",
    "\n",
    "            autograd_outs = zs_params_to_forwards(z, params)\n",
    "            outs = model.forward_(z, params)\n",
    "            assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "            def zs_params_to_cdfs(zs, params):\n",
    "                out = []\n",
    "                for z, param in zip(zs, params):\n",
    "                    for d_ in range(d):\n",
    "                        start_idx, end_idx = d_ * autograd_model.n_params, (d_ + 1) * autograd_model.n_params\n",
    "                        autograd_model.set_params(param[start_idx:end_idx])\n",
    "                        out.append(autograd_model.cdf(z[d_:d_+1][None,:]))\n",
    "\n",
    "                out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "                return out\n",
    "\n",
    "            autograd_outs = zs_params_to_cdfs(z, params)\n",
    "            outs = model.cdf(z, params)\n",
    "            assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "            def zs_params_to_backwards(zs, params):\n",
    "                out = []\n",
    "                for z, param in zip(zs, params):\n",
    "                    for d_ in range(d):\n",
    "                        start_idx, end_idx = d_ * autograd_model.n_params, (d_ + 1) * autograd_model.n_params\n",
    "                        autograd_model.set_params(param[start_idx:end_idx])\n",
    "                        out.append(autograd_model.f_(z[d_:d_+1][None,:]))\n",
    "\n",
    "                out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "                return out\n",
    "\n",
    "            autograd_outs = zs_params_to_backwards(z, params)\n",
    "            outs = model.backward_(z, params)\n",
    "            assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "            def zs_params_to_pdfs(zs, params):\n",
    "                out = []\n",
    "                for z, param in zip(zs, params):\n",
    "                    for d_ in range(d):\n",
    "                        start_idx, end_idx = d_ * autograd_model.n_params, (d_ + 1) * autograd_model.n_params\n",
    "                        autograd_model.set_params(param[start_idx:end_idx])\n",
    "                        out.append(autograd_model.pdf(z[d_:d_+1][None,:]))\n",
    "\n",
    "                out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "                return out\n",
    "\n",
    "            autograd_outs = zs_params_to_pdfs(z, params)\n",
    "            outs = model.pdf(z, params)\n",
    "            assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "            def zs_params_to_icdfs(zs, params):\n",
    "                out = []\n",
    "                for z, param in zip(zs, params):\n",
    "                    for d_ in range(d):\n",
    "                        start_idx, end_idx = d_ * autograd_model.n_params, (d_ + 1) * autograd_model.n_params\n",
    "                        autograd_model.set_params(param[start_idx:end_idx])\n",
    "                        out.append(autograd_model.F_inv(z[d_:d_+1][None,:]))\n",
    "\n",
    "                out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "                return out\n",
    "\n",
    "            y = torch.rand((n, d)).to(device)\n",
    "            autograd_outs = zs_params_to_icdfs(y, params)\n",
    "            outs = model.icdf(y, params)\n",
    "            assert torch.allclose(autograd_outs, outs, atol=1e-1)\n",
    "\n",
    "            # try with single parameter, many zs\n",
    "\n",
    "            def zs_params_to_pdfs(zs, param):\n",
    "                out = []\n",
    "                for z in zs:\n",
    "                    for d_ in range(d):\n",
    "                        start_idx, end_idx = d_ * autograd_model.n_params, (d_ + 1) * autograd_model.n_params\n",
    "                        autograd_model.set_params(param[start_idx:end_idx])\n",
    "                        out.append(autograd_model.pdf(z[d_:d_+1][None,:]))\n",
    "\n",
    "                out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "                return out\n",
    "\n",
    "            autograd_outs = zs_params_to_pdfs(z, params[0])\n",
    "            outs = model.pdf(z, params[0:1])\n",
    "            assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "            # try with single z, many parameters\n",
    "\n",
    "            def zs_params_to_pdfs(z, params):\n",
    "                out = []\n",
    "                for param in params:\n",
    "                    for d_ in range(d):\n",
    "                        start_idx, end_idx = d_ * autograd_model.n_params, (d_ + 1) * autograd_model.n_params\n",
    "                        autograd_model.set_params(param[start_idx:end_idx])\n",
    "                        out.append(autograd_model.pdf(z[d_:d_+1][None,:]))\n",
    "\n",
    "                out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "                return out\n",
    "\n",
    "            autograd_outs = zs_params_to_pdfs(z[0], params)\n",
    "            outs = model.pdf(z[0:1], params)\n",
    "            assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "from nits.discretized_mol import *\n",
    "print(\"Testing arch = [1, 10, 1], 'neg_exp' A_constraint, 'softmax' final_layer_constraint \" \\\n",
    "      \"against discretized mixture of logistics.\")\n",
    "\n",
    "model = NITS(d=1, start=-1e5, end=1e5, arch=[1, 10, 1],\n",
    "                     monotonic_const=0., A_constraint='neg_exp',\n",
    "                     final_layer_constraint='softmax').to(device)\n",
    "params = torch.randn((n, model.n_params, 1, 1))\n",
    "z = torch.randn((n, 1, 1, 1))\n",
    "\n",
    "loss1 = discretized_mix_logistic_loss_1d(z, params)\n",
    "loss2 = discretized_nits_loss(z, params, nits_model=model)\n",
    "\n",
    "assert (loss1 - loss2).norm() < 1e-2, (loss1 - loss2).norm()\n",
    "\n",
    "model = NITS(d=1, start=-1e7, end=1e7, arch=[1, 10, 1],\n",
    "                     monotonic_const=0., A_constraint='neg_exp',\n",
    "                     final_layer_constraint='softmax').to(device)\n",
    "\n",
    "loss1 = discretized_mix_logistic_loss_1d(z, params)\n",
    "loss2 = discretized_nits_loss(z, params, nits_model=model)\n",
    "\n",
    "assert (loss1 - loss2).norm() < 1e-3, (loss1 - loss2).norm()\n",
    "\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "print(\"Testing Conditional NITS.\")\n",
    "start, end = -2., 2.\n",
    "monotonic_const = 1e-2\n",
    "d = 2\n",
    "c_arch = [d] + base_arch\n",
    "A_constraint = 'exp'\n",
    "final_layer_constraint = 'softmax'\n",
    "device = 'cpu'\n",
    "\n",
    "c_model = ConditionalNITS(d=d, start=start, end=end, arch=c_arch,\n",
    "                          monotonic_const=monotonic_const, A_constraint=A_constraint,\n",
    "                          final_layer_constraint=final_layer_constraint,\n",
    "                          autoregressive=False).to(device)\n",
    "\n",
    "c_params = torch.randn((n, c_model.tot_params)).to(device)\n",
    "z = torch.linspace(start, end, steps=n, device=device)[:,None].tile((1, d)).to(device)\n",
    "\n",
    "def cond_zs_params_to_cdfs(zs, params):\n",
    "    out = []\n",
    "    for z, param in zip(zs, params):\n",
    "        for d_ in range(d):\n",
    "            c_autograd_model = ModelInverse(arch=c_arch, start=start, end=end, store_weights=False,\n",
    "                                           A_constraint=A_constraint, monotonic_const=monotonic_const,\n",
    "                                           final_layer_constraint=final_layer_constraint,\n",
    "                                           non_conditional_dim=d_).to(device)\n",
    "            start_idx, end_idx = d_ * c_autograd_model.n_params, (d_ + 1) * c_autograd_model.n_params\n",
    "            c_autograd_model.set_params(param[start_idx:end_idx])\n",
    "            out.append(c_autograd_model.cdf(z[None,:]))\n",
    "\n",
    "    out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "    return out\n",
    "\n",
    "autograd_outs = cond_zs_params_to_cdfs(z, c_params)\n",
    "outs = c_model.cdf(z, c_params)\n",
    "assert torch.allclose(autograd_outs, outs, atol=1e-3), (autograd_outs - outs).norm()\n",
    "\n",
    "def cond_zs_params_to_pdfs(zs, params):\n",
    "    out = []\n",
    "    for z, param in zip(zs, params):\n",
    "        for d_ in range(d):\n",
    "            c_autograd_model = ModelInverse(arch=c_arch, start=start, end=end, store_weights=False,\n",
    "                                           A_constraint=A_constraint, monotonic_const=monotonic_const,\n",
    "                                           final_layer_constraint=final_layer_constraint,\n",
    "                                           non_conditional_dim=d_).to(device)\n",
    "            start_idx, end_idx = d_ * c_autograd_model.n_params, (d_ + 1) * c_autograd_model.n_params\n",
    "            c_autograd_model.set_params(param[start_idx:end_idx])\n",
    "            out.append(c_autograd_model.pdf(z[None,:]))\n",
    "\n",
    "    out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "    return out\n",
    "\n",
    "autograd_outs = cond_zs_params_to_pdfs(z, c_params)\n",
    "outs = c_model.pdf(z, c_params)\n",
    "assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "# testing the inverse_cdf function\n",
    "\n",
    "def cond_zs_params_to_icdfs(ys, zs, params):\n",
    "    out = []\n",
    "    for y, z, param in zip(ys, zs, params):\n",
    "        for d_ in range(d):\n",
    "            c_autograd_model = ModelInverse(arch=c_arch, start=start, end=end, store_weights=False,\n",
    "                                           A_constraint=A_constraint, monotonic_const=monotonic_const,\n",
    "                                           final_layer_constraint=final_layer_constraint,\n",
    "                                           non_conditional_dim=d_).to(device)\n",
    "            start_idx, end_idx = d_ * c_autograd_model.n_params, (d_ + 1) * c_autograd_model.n_params\n",
    "            c_autograd_model.set_params(param[start_idx:end_idx])\n",
    "            out.append(c_autograd_model.F_inv(y[d_:d_+1][None,:], given_x=z[None,:]))\n",
    "\n",
    "    out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "    return out\n",
    "\n",
    "y = torch.rand((n, d)).to(device)\n",
    "autograd_outs = cond_zs_params_to_icdfs(y, z, c_params)\n",
    "outs = c_model.icdf(y, c_params, given_x=z)\n",
    "assert torch.allclose(autograd_outs, outs, atol=1e-1)\n",
    "\n",
    "for i in range(d):\n",
    "    tmp = torch.cat([z[:,:i], outs[:,i:i+1], z[:,i+1:]], axis=1)\n",
    "    res = c_model.cdf(tmp, c_params)\n",
    "    assert torch.allclose(res[:,i], y[:,i], atol=1e-2)\n",
    "\n",
    "for i in range(d):\n",
    "    tmp = torch.cat([z[:,:i], outs[:,i:i+1], z[:,i+1:]], axis=1)\n",
    "    res = cond_zs_params_to_cdfs(tmp, c_params)\n",
    "    assert torch.allclose(res[:,i], y[:,i], atol=1e-2)\n",
    "\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "print('Testing autoregressive conditional NITS.')\n",
    "start, end = -2., 2.\n",
    "monotonic_const = 1e-2\n",
    "A_constraint = 'neg_exp'\n",
    "final_layer_constraint = 'softmax'\n",
    "device = 'cpu'\n",
    "\n",
    "c_model = ConditionalNITS(d=d, start=start, end=end, arch=c_arch,\n",
    "                          monotonic_const=monotonic_const, A_constraint=A_constraint,\n",
    "                          final_layer_constraint=final_layer_constraint,\n",
    "                          autoregressive=True).to(device)\n",
    "\n",
    "c_params = torch.randn((n, c_model.tot_params)).to(device)\n",
    "z = torch.linspace(start, end, steps=n, device=device)[:,None].tile((1, d)).to(device)\n",
    "\n",
    "def causal_mask(x, i):\n",
    "    x = x.clone()[None,:]\n",
    "    x[:,i+1:] = 0.\n",
    "    return x\n",
    "\n",
    "def cond_zs_params_to_cdfs(zs, params):\n",
    "    out = []\n",
    "    for z, param in zip(zs, params):\n",
    "        for d_ in range(d):\n",
    "            c_autograd_model = ModelInverse(arch=c_arch, start=start, end=end, store_weights=False,\n",
    "                                           A_constraint=A_constraint, monotonic_const=monotonic_const,\n",
    "                                           final_layer_constraint=final_layer_constraint,\n",
    "                                           non_conditional_dim=d_, b_constraint='').to(device)\n",
    "            start_idx, end_idx = d_ * c_autograd_model.n_params, (d_ + 1) * c_autograd_model.n_params\n",
    "            c_autograd_model.set_params(param[start_idx:end_idx])\n",
    "\n",
    "            # set mask and apply function\n",
    "            z_masked = causal_mask(z, d_)\n",
    "            out.append(c_autograd_model.cdf(z_masked))\n",
    "\n",
    "    out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "    return out\n",
    "\n",
    "autograd_outs = cond_zs_params_to_cdfs(z, c_params)\n",
    "outs = c_model.cdf(z, c_params)\n",
    "assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "def cond_zs_params_to_pdfs(zs, params):\n",
    "    out = []\n",
    "    for z, param in zip(zs, params):\n",
    "        for d_ in range(d):\n",
    "            c_autograd_model = ModelInverse(arch=c_arch, start=start, end=end, store_weights=False,\n",
    "                                           A_constraint=A_constraint, monotonic_const=monotonic_const,\n",
    "                                           final_layer_constraint=final_layer_constraint,\n",
    "                                           non_conditional_dim=d_, b_constraint='').to(device)\n",
    "            start_idx, end_idx = d_ * c_autograd_model.n_params, (d_ + 1) * c_autograd_model.n_params\n",
    "            c_autograd_model.set_params(param[start_idx:end_idx])\n",
    "\n",
    "            # set mask and apply function\n",
    "            z_masked = causal_mask(z, d_)\n",
    "            out.append(c_autograd_model.pdf(z_masked))\n",
    "\n",
    "    out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "    return out\n",
    "\n",
    "autograd_outs = cond_zs_params_to_pdfs(z, c_params)\n",
    "outs = c_model.pdf(z, c_params)\n",
    "assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "# testing the inverse_cdf function\n",
    "\n",
    "def cond_zs_params_to_icdfs(ys, zs, params):\n",
    "    out = []\n",
    "    for y, z, param in zip(ys, zs, params):\n",
    "        for d_ in range(d):\n",
    "            c_autograd_model = ModelInverse(arch=c_arch, start=start, end=end, store_weights=False,\n",
    "                                           A_constraint=A_constraint, monotonic_const=monotonic_const,\n",
    "                                           final_layer_constraint=final_layer_constraint,\n",
    "                                           non_conditional_dim=d_, b_constraint='').to(device)\n",
    "            start_idx, end_idx = d_ * c_autograd_model.n_params, (d_ + 1) * c_autograd_model.n_params\n",
    "            c_autograd_model.set_params(param[start_idx:end_idx])\n",
    "\n",
    "            # set mask and apply function\n",
    "            z_masked = torch.cat(out[len(out)-d_:] + [torch.zeros((1, d - d_))], axis=1)\n",
    "            out.append(c_autograd_model.F_inv(y[d_:d_+1][None,:], given_x=z_masked))\n",
    "\n",
    "    out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "    return out\n",
    "\n",
    "y = torch.rand((n, d)).to(device)\n",
    "autograd_outs = cond_zs_params_to_icdfs(y, z, c_params)\n",
    "outs = c_model.icdf(y, c_params)\n",
    "assert torch.allclose(autograd_outs, outs, atol=1e-1)\n",
    "\n",
    "assert torch.allclose(c_model.cdf(outs, c_params), y, atol=1e-3)\n",
    "assert torch.allclose(cond_zs_params_to_cdfs(autograd_outs, c_params), y, atol=1e-3)\n",
    "\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "print('Testing pixelrnn-like autoregressive conditional NITS.')\n",
    "\n",
    "start, end = -2., 2.\n",
    "monotonic_const = 0.\n",
    "A_constraint = 'neg_exp'\n",
    "final_layer_constraint = 'softmax'\n",
    "batch_size = 1024\n",
    "\n",
    "c_model = ConditionalNITS(d=3, start=start, end=end, arch=[1, 10, 1],\n",
    "                          monotonic_const=monotonic_const, A_constraint=A_constraint,\n",
    "                          final_layer_constraint=final_layer_constraint,\n",
    "                          autoregressive=True, pixelrnn=True, normalize_inverse=False).to(device)\n",
    "\n",
    "c_params = torch.randn(batch_size, c_model.tot_params, 2, 2, device=device)\n",
    "z = torch.rand(batch_size, 3, 2, 2, device=device) * 2 - 1\n",
    "\n",
    "# make sure outputs align with pixelrnn\n",
    "loss1 = discretized_mix_logistic_loss(z, c_params, bad_loss=True)\n",
    "loss2 = discretized_nits_loss(z, c_params, c_model)\n",
    "\n",
    "dist_per_dim = (loss1 - loss2).abs() / np.prod(z.shape)\n",
    "\n",
    "assert dist_per_dim < 1e-6\n",
    "\n",
    "# make sure that cdf and icdf return the correct result\n",
    "c_params = torch.randn(batch_size, c_model.tot_params, device=device)\n",
    "z = torch.rand(batch_size, 3, device=device) * 2 - 1\n",
    "cdf_ = c_model.forward_(z, c_params)\n",
    "icdf_ = c_model.icdf(cdf_, c_params)\n",
    "\n",
    "assert (cdf_ <= 1.).all() and (cdf_ >= 0).all()\n",
    "assert (cdf_ <= 1.).all() and (cdf_ >= 0).all()\n",
    "assert (z - icdf_).abs().max() < 1e-2\n",
    "\n",
    "\n",
    "# test icdf, when normalize_inverse == True (i.e. not EXACTLY pixelrnn anymore)\n",
    "c_model = ConditionalNITS(d=3, start=start, end=end, arch=[1, 10, 1],\n",
    "                          monotonic_const=monotonic_const, A_constraint=A_constraint,\n",
    "                          final_layer_constraint=final_layer_constraint,\n",
    "                          autoregressive=True, pixelrnn=True, normalize_inverse=True).to(device)\n",
    "\n",
    "# make sure that cdf and icdf return the correct result\n",
    "c_params = torch.randn(batch_size, c_model.tot_params, device=device)\n",
    "z = torch.rand(batch_size, 3, device=device) * 2 - 1\n",
    "cdf_ = c_model.cdf(z, c_params)\n",
    "icdf_ = c_model.icdf(cdf_, c_params)\n",
    "\n",
    "assert (cdf_ <= 1.).all() and (cdf_ >= 0).all()\n",
    "assert (cdf_ <= 1.).all() and (cdf_ >= 0).all()\n",
    "assert (z - icdf_).abs().max() < 1e-2\n",
    "\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "print(\"Passed all unit tests!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4663, -1.9263,  1.9634],\n",
       "        [-0.1646,  0.4937,  0.3862],\n",
       "        [ 1.0708,  1.0962, -0.3530],\n",
       "        ...,\n",
       "        [-0.8794,  0.5669, -1.9038],\n",
       "        [-0.2319,  0.3296, -0.4185],\n",
       "        [-1.2300, -0.8315,  0.3276]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_model.sample(1, c_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
