{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing NITS.\n",
      "Testing arch = [1, 10, 1], 'neg_exp' constraint_type, 'softmax' final_layer_constraint against discretized mixture of logistics.\n",
      "All tests passed!\n",
      "Testing Conditional NITS.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-55bf55ce05b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;31m# check that the function integrates to 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m assert torch.allclose(torch.ones((n, d)).to(device),\n\u001b[0;32m--> 200\u001b[0;31m                       model.cdf(c_model.end, c_params) - model.cdf(c_model.start, c_params), atol=1e-5)\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;31m# check that the pdf is all positive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/NITS/nits/model.py\u001b[0m in \u001b[0;36mcdf\u001b[0;34m(self, x, params)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_conditional_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0micdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/NITS/nits/model.py\u001b[0m in \u001b[0;36mapply_conditional_func\u001b[0;34m(self, func, x, params)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_conditional_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultidim_reshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/NITS/nits/model.py\u001b[0m in \u001b[0;36mmultidim_reshape\u001b[0;34m(self, x, params)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtot_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from nits.model import *\n",
    "from nits.autograd_model import *\n",
    "\n",
    "# device = 'cpu'\n",
    "device = 'cuda:5'\n",
    "\n",
    "n = 32\n",
    "start, end = -2., 2.\n",
    "arch = [1, 8, 8, 1]\n",
    "monotonic_const = 1e-2\n",
    "\n",
    "print(\"Testing NITS.\")\n",
    "\n",
    "for d in [1, 2, 10]:\n",
    "    for constraint_type in ['neg_exp', 'exp']:\n",
    "        for final_layer_constraint in ['softmax', 'exp']:\n",
    "#             print(\"\"\"\n",
    "#             Testing configuration:\n",
    "#                 d: {}\n",
    "#                 constraint_type: {}\n",
    "#                 final_layer_constraint: {}\n",
    "#                   \"\"\".format(d, constraint_type, final_layer_constraint))\n",
    "            ############################\n",
    "            # DEFINE MODELS            #\n",
    "            ############################\n",
    "\n",
    "            model = NITS(d=d, start=start, end=end, arch=arch,\n",
    "                                 monotonic_const=monotonic_const, constraint_type=constraint_type,\n",
    "                                 final_layer_constraint=final_layer_constraint).to(device)\n",
    "            params = torch.randn((n, d * model.n_params)).to(device)\n",
    "\n",
    "            ############################\n",
    "            # SANITY CHECKS            #\n",
    "            ############################\n",
    "\n",
    "            # check that the function integrates to 1\n",
    "            assert torch.allclose(torch.ones((n, d)).to(device),\n",
    "                                  model.cdf(model.end, params) - model.cdf(model.start, params), atol=1e-5)\n",
    "\n",
    "            # check that the pdf is all positive\n",
    "            z = torch.linspace(start, end, steps=n, device=device)[:,None].tile((1, d))\n",
    "            assert (model.pdf(z, params) >= 0).all()\n",
    "\n",
    "            # check that the cdf is the inverted\n",
    "            cdf = model.cdf(z, params[0:1])\n",
    "            icdf = model.icdf(cdf, params[0:1])\n",
    "            assert (z - icdf <= 1e-3).all()\n",
    "\n",
    "            ############################\n",
    "            # COMPARE TO AUTOGRAD NITS #\n",
    "            ############################\n",
    "            autograd_model = ModelInverse(arch=arch, start=start, end=end, store_weights=False,\n",
    "                                          constraint_type=constraint_type, monotonic_const=monotonic_const,\n",
    "                                          final_layer_constraint=final_layer_constraint).to(device)\n",
    "\n",
    "            def zs_params_to_forwards(zs, params):\n",
    "                out = []\n",
    "                for z, param in zip(zs, params):\n",
    "                    for d_ in range(d):\n",
    "                        start_idx, end_idx = d_ * autograd_model.n_params, (d_ + 1) * autograd_model.n_params\n",
    "                        autograd_model.set_params(param[start_idx:end_idx])\n",
    "                        out.append(autograd_model.apply_layers(z[d_:d_+1][None,:]))\n",
    "\n",
    "                out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "                return out\n",
    "\n",
    "            autograd_outs = zs_params_to_forwards(z, params)\n",
    "            outs = model.forward_(z, params)\n",
    "            assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "            def zs_params_to_cdfs(zs, params):\n",
    "                out = []\n",
    "                for z, param in zip(zs, params):\n",
    "                    for d_ in range(d):\n",
    "                        start_idx, end_idx = d_ * autograd_model.n_params, (d_ + 1) * autograd_model.n_params\n",
    "                        autograd_model.set_params(param[start_idx:end_idx])\n",
    "                        out.append(autograd_model.cdf(z[d_:d_+1][None,:]))\n",
    "\n",
    "                out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "                return out\n",
    "\n",
    "            autograd_outs = zs_params_to_cdfs(z, params)\n",
    "            outs = model.cdf(z, params)\n",
    "            assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "            def zs_params_to_pdfs(zs, params):\n",
    "                out = []\n",
    "                for z, param in zip(zs, params):\n",
    "                    for d_ in range(d):\n",
    "                        start_idx, end_idx = d_ * autograd_model.n_params, (d_ + 1) * autograd_model.n_params\n",
    "                        autograd_model.set_params(param[start_idx:end_idx])\n",
    "                        out.append(autograd_model.pdf(z[d_:d_+1][None,:]))\n",
    "\n",
    "                out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "                return out\n",
    "\n",
    "            autograd_outs = zs_params_to_pdfs(z, params)\n",
    "            outs = model.pdf(z, params)\n",
    "            assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "            def zs_params_to_icdfs(zs, params):\n",
    "                out = []\n",
    "                for z, param in zip(zs, params):\n",
    "                    for d_ in range(d):\n",
    "                        start_idx, end_idx = d_ * autograd_model.n_params, (d_ + 1) * autograd_model.n_params\n",
    "                        autograd_model.set_params(param[start_idx:end_idx])\n",
    "                        out.append(autograd_model.F_inv(z[d_:d_+1][None,:]))\n",
    "\n",
    "                out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "                return out\n",
    "\n",
    "            y = torch.rand((n, d)).to(device)\n",
    "            autograd_outs = zs_params_to_icdfs(y, params)\n",
    "            outs = model.icdf(y, params)\n",
    "            assert torch.allclose(autograd_outs, outs, atol=1e-1)\n",
    "\n",
    "            # try with single parameter, many zs\n",
    "\n",
    "            def zs_params_to_pdfs(zs, param):\n",
    "                out = []\n",
    "                for z in zs:\n",
    "                    for d_ in range(d):\n",
    "                        start_idx, end_idx = d_ * autograd_model.n_params, (d_ + 1) * autograd_model.n_params\n",
    "                        autograd_model.set_params(param[start_idx:end_idx])\n",
    "                        out.append(autograd_model.pdf(z[d_:d_+1][None,:]))\n",
    "\n",
    "                out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "                return out\n",
    "\n",
    "            autograd_outs = zs_params_to_pdfs(z, params[0])\n",
    "            outs = model.pdf(z, params[0:1])\n",
    "            assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "            # try with single z, many parameters\n",
    "\n",
    "            def zs_params_to_pdfs(z, params):\n",
    "                out = []\n",
    "                for param in params:\n",
    "                    for d_ in range(d):\n",
    "                        start_idx, end_idx = d_ * autograd_model.n_params, (d_ + 1) * autograd_model.n_params\n",
    "                        autograd_model.set_params(param[start_idx:end_idx])\n",
    "                        out.append(autograd_model.pdf(z[d_:d_+1][None,:]))\n",
    "\n",
    "                out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "                return out\n",
    "\n",
    "            autograd_outs = zs_params_to_pdfs(z[0], params)\n",
    "            outs = model.pdf(z[0:1], params)\n",
    "            assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "from nits.discretized_mol import *\n",
    "print(\"Testing arch = [1, 10, 1], 'neg_exp' constraint_type, 'softmax' final_layer_constraint \" \\\n",
    "      \"against discretized mixture of logistics.\")\n",
    "\n",
    "model = NITS(d=1, start=-1e5, end=1e5, arch=[1, 10, 1],\n",
    "                     monotonic_const=0., constraint_type='neg_exp',\n",
    "                     final_layer_constraint='softmax').to(device)\n",
    "params = torch.randn((n, model.n_params, 1, 1))\n",
    "z = torch.randn((n, 1, 1, 1))\n",
    "\n",
    "loss1 = discretized_mix_logistic_loss_1d(z, params)\n",
    "loss2 = discretized_nits_loss(z, params, arch=[1, 10, 1], nits_model=model)\n",
    "\n",
    "assert (loss1 - loss2).norm() < 1e-2, (loss1 - loss2).norm()\n",
    "\n",
    "model = NITS(d=1, start=-1e7, end=1e7, arch=[1, 10, 1],\n",
    "                     monotonic_const=0., constraint_type='neg_exp',\n",
    "                     final_layer_constraint='softmax').to(device)\n",
    "\n",
    "loss1 = discretized_mix_logistic_loss_1d(z, params)\n",
    "loss2 = discretized_nits_loss(z, params, arch=[1, 10, 1], nits_model=model)\n",
    "\n",
    "assert (loss1 - loss2).norm() < 1e-3, (loss1 - loss2).norm()\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Conditional NITS.\n",
      "All tests passed!\n",
      "Testing autoregressive conditional NITS.\n",
      "All tests passed!\n",
      "Passed all unit tests!\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing Conditional NITS.\")\n",
    "n = 64\n",
    "start, end = -2., 2.\n",
    "monotonic_const = 1e-2\n",
    "d = 4\n",
    "c_arch = [d, 8, 8, 1]\n",
    "constraint_type = 'exp'\n",
    "final_layer_constraint = 'softmax'\n",
    "device = 'cpu'\n",
    "\n",
    "c_model = ConditionalNITS(d=d, start=start, end=end, arch=c_arch,\n",
    "                          monotonic_const=monotonic_const, constraint_type=constraint_type,\n",
    "                          final_layer_constraint=final_layer_constraint,\n",
    "                          autoregressive=False).to(device)\n",
    "\n",
    "c_params = torch.randn((n, c_model.tot_params)).to(device)\n",
    "z = torch.linspace(start, end, steps=n, device=device)[:,None].tile((1, d))\n",
    "\n",
    "# check that the function integrates to 1\n",
    "assert torch.allclose(torch.ones((n, d)).to(device),\n",
    "                      c_model.cdf(c_model.end, c_params) - c_model.cdf(c_model.start, c_params), atol=1e-5)\n",
    "\n",
    "# check that the pdf is all positive\n",
    "assert (c_model.pdf(z, c_params) >= 0).all()\n",
    "\n",
    "def cond_zs_params_to_cdfs(zs, params):\n",
    "    out = []\n",
    "    for z, param in zip(zs, params):\n",
    "        for d_ in range(d):\n",
    "            c_autograd_model = ModelInverse(arch=c_arch, start=start, end=end, store_weights=False,\n",
    "                                           constraint_type=constraint_type, monotonic_const=monotonic_const,\n",
    "                                           final_layer_constraint=final_layer_constraint,\n",
    "                                           non_conditional_dim=d_).to(device)\n",
    "            start_idx, end_idx = d_ * c_autograd_model.n_params, (d_ + 1) * c_autograd_model.n_params\n",
    "            c_autograd_model.set_params(param[start_idx:end_idx])\n",
    "            out.append(c_autograd_model.cdf(z[None,:]))\n",
    "\n",
    "    out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "    return out\n",
    "\n",
    "autograd_outs = cond_zs_params_to_cdfs(z, c_params)\n",
    "outs = c_model.cdf(z, c_params)\n",
    "assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "def cond_zs_params_to_pdfs(zs, params):\n",
    "    out = []\n",
    "    for z, param in zip(zs, params):\n",
    "        for d_ in range(d):\n",
    "            c_autograd_model = ModelInverse(arch=c_arch, start=start, end=end, store_weights=False,\n",
    "                                           constraint_type=constraint_type, monotonic_const=monotonic_const,\n",
    "                                           final_layer_constraint=final_layer_constraint,\n",
    "                                           non_conditional_dim=d_).to(device)\n",
    "            start_idx, end_idx = d_ * c_autograd_model.n_params, (d_ + 1) * c_autograd_model.n_params\n",
    "            c_autograd_model.set_params(param[start_idx:end_idx])\n",
    "            out.append(c_autograd_model.pdf(z[None,:]))\n",
    "\n",
    "    out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "    return out\n",
    "\n",
    "autograd_outs = cond_zs_params_to_pdfs(z, c_params)\n",
    "outs = c_model.pdf(z, c_params)\n",
    "assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "# testing the inverse_cdf function\n",
    "\n",
    "def cond_zs_params_to_icdfs(ys, zs, params):\n",
    "    out = []\n",
    "    for y, z, param in zip(ys, zs, params):\n",
    "        for d_ in range(d):\n",
    "            c_autograd_model = ModelInverse(arch=c_arch, start=start, end=end, store_weights=False,\n",
    "                                           constraint_type=constraint_type, monotonic_const=monotonic_const,\n",
    "                                           final_layer_constraint=final_layer_constraint,\n",
    "                                           non_conditional_dim=d_).to(device)\n",
    "            start_idx, end_idx = d_ * c_autograd_model.n_params, (d_ + 1) * c_autograd_model.n_params\n",
    "            c_autograd_model.set_params(param[start_idx:end_idx])\n",
    "            out.append(c_autograd_model.F_inv(y[d_:d_+1][None,:], given_x=z[None,:]))\n",
    "\n",
    "    out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "    return out\n",
    "\n",
    "y = torch.rand((n, d)).to(device)\n",
    "autograd_outs = cond_zs_params_to_icdfs(y, z, c_params)\n",
    "outs = c_model.icdf(y, c_params, given_x=z)\n",
    "assert torch.allclose(autograd_outs, outs, atol=1e-1)\n",
    "\n",
    "for i in range(d):\n",
    "    tmp = torch.cat([z[:,:i], outs[:,i:i+1], z[:,i+1:]], axis=1)\n",
    "    res = c_model.cdf(tmp, c_params)\n",
    "    assert torch.allclose(res[:,i], y[:,i], atol=1e-2)\n",
    "\n",
    "for i in range(d):\n",
    "    tmp = torch.cat([z[:,:i], outs[:,i:i+1], z[:,i+1:]], axis=1)\n",
    "    res = cond_zs_params_to_cdfs(tmp, c_params)\n",
    "    assert torch.allclose(res[:,i], y[:,i], atol=1e-2)\n",
    "\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "print('Testing autoregressive conditional NITS.')\n",
    "start, end = -2., 2.\n",
    "monotonic_const = 1e-2\n",
    "constraint_type = 'exp'\n",
    "final_layer_constraint = 'softmax'\n",
    "device = 'cpu'\n",
    "\n",
    "c_model = ConditionalNITS(d=d, start=start, end=end, arch=c_arch,\n",
    "                          monotonic_const=monotonic_const, constraint_type=constraint_type,\n",
    "                          final_layer_constraint=final_layer_constraint,\n",
    "                          autoregressive=True).to(device)\n",
    "\n",
    "c_params = torch.randn((n, c_model.tot_params)).to(device)\n",
    "z = torch.linspace(start, end, steps=n, device=device)[:,None].tile((1, d)).to(device)\n",
    "\n",
    "# check that the function integrates to 1\n",
    "assert torch.allclose(torch.ones((n, d)).to(device),\n",
    "                      c_model.cdf(c_model.end, c_params) - c_model.cdf(c_model.start, c_params), atol=1e-5)\n",
    "\n",
    "# check that the pdf is all positive\n",
    "assert (c_model.pdf(z, c_params) >= 0).all()\n",
    "\n",
    "# check that the cdf is the inverted\n",
    "cdf = c_model.cdf(z, c_params[0:1])\n",
    "icdf = c_model.icdf(cdf, c_params[0:1])\n",
    "assert (z - icdf <= 1e-2).all()\n",
    "\n",
    "def causal_mask(x, i):\n",
    "    x = x.clone()[None,:]\n",
    "    x[:,i+1:] = 0.\n",
    "    return x\n",
    "\n",
    "def cond_zs_params_to_cdfs(zs, params):\n",
    "    out = []\n",
    "    for z, param in zip(zs, params):\n",
    "        for d_ in range(d):\n",
    "            c_autograd_model = ModelInverse(arch=c_arch, start=start, end=end, store_weights=False,\n",
    "                                           constraint_type=constraint_type, monotonic_const=monotonic_const,\n",
    "                                           final_layer_constraint=final_layer_constraint,\n",
    "                                           non_conditional_dim=d_).to(device)\n",
    "            start_idx, end_idx = d_ * c_autograd_model.n_params, (d_ + 1) * c_autograd_model.n_params\n",
    "            c_autograd_model.set_params(param[start_idx:end_idx])\n",
    "\n",
    "            # set mask and apply function\n",
    "            z_masked = causal_mask(z, d_)\n",
    "            out.append(c_autograd_model.cdf(z_masked))\n",
    "\n",
    "    out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "    return out\n",
    "\n",
    "autograd_outs = cond_zs_params_to_cdfs(z, c_params)\n",
    "outs = c_model.cdf(z, c_params)\n",
    "assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "def cond_zs_params_to_pdfs(zs, params):\n",
    "    out = []\n",
    "    for z, param in zip(zs, params):\n",
    "        for d_ in range(d):\n",
    "            c_autograd_model = ModelInverse(arch=c_arch, start=start, end=end, store_weights=False,\n",
    "                                           constraint_type=constraint_type, monotonic_const=monotonic_const,\n",
    "                                           final_layer_constraint=final_layer_constraint,\n",
    "                                           non_conditional_dim=d_).to(device)\n",
    "            start_idx, end_idx = d_ * c_autograd_model.n_params, (d_ + 1) * c_autograd_model.n_params\n",
    "            c_autograd_model.set_params(param[start_idx:end_idx])\n",
    "\n",
    "            # set mask and apply function\n",
    "            z_masked = causal_mask(z, d_)\n",
    "            out.append(c_autograd_model.pdf(z_masked))\n",
    "\n",
    "    out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "    return out\n",
    "\n",
    "autograd_outs = cond_zs_params_to_pdfs(z, c_params)\n",
    "outs = c_model.pdf(z, c_params)\n",
    "assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "# testing the inverse_cdf function\n",
    "\n",
    "def cond_zs_params_to_icdfs(ys, zs, params):\n",
    "    out = []\n",
    "    for y, z, param in zip(ys, zs, params):\n",
    "        for d_ in range(d):\n",
    "            c_autograd_model = ModelInverse(arch=c_arch, start=start, end=end, store_weights=False,\n",
    "                                           constraint_type=constraint_type, monotonic_const=monotonic_const,\n",
    "                                           final_layer_constraint=final_layer_constraint,\n",
    "                                           non_conditional_dim=d_).to(device)\n",
    "            start_idx, end_idx = d_ * c_autograd_model.n_params, (d_ + 1) * c_autograd_model.n_params\n",
    "            c_autograd_model.set_params(param[start_idx:end_idx])\n",
    "\n",
    "            # set mask and apply function\n",
    "            z_masked = torch.cat(out[len(out)-d_:] + [torch.zeros((1, d - d_))], axis=1)\n",
    "            out.append(c_autograd_model.F_inv(y[d_:d_+1][None,:], given_x=z_masked))\n",
    "\n",
    "    out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "    return out\n",
    "\n",
    "y = torch.rand((n, d)).to(device)\n",
    "autograd_outs = cond_zs_params_to_icdfs(y, z, c_params)\n",
    "outs = c_model.icdf(y, c_params)\n",
    "assert torch.allclose(autograd_outs, outs, atol=1e-1)\n",
    "\n",
    "assert torch.allclose(c_model.cdf(outs, c_params), y, atol=1e-3)\n",
    "assert torch.allclose(cond_zs_params_to_cdfs(autograd_outs, c_params), y, atol=1e-3)\n",
    "\n",
    "print(\"All tests passed!\")\n",
    "\n",
    "print(\"Passed all unit tests!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing autoregressive conditional NITS loss and sample functions.\n"
     ]
    }
   ],
   "source": [
    "print('Testing autoregressive conditional NITS loss and sample functions.')\n",
    "start, end = -2., 2.\n",
    "monotonic_const = 1e-2\n",
    "d = 3\n",
    "c_arch = [d, 8, 1]\n",
    "constraint_type = 'exp'\n",
    "final_layer_constraint = 'softmax'\n",
    "device = 'cpu'\n",
    "\n",
    "c_model = ConditionalNITS(d=d, start=start, end=end, arch=c_arch,\n",
    "                          monotonic_const=monotonic_const, constraint_type=constraint_type,\n",
    "                          final_layer_constraint=final_layer_constraint,\n",
    "                          autoregressive=True).to(device)\n",
    "\n",
    "c_params = torch.randn((n, c_model.tot_params, 1, 1))\n",
    "z = torch.rand((n, d, 1, 1)) * 2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretized_nits_loss(x, l, arch, nits_model):\n",
    "    \"\"\" log-likelihood for mixture of discretized logistics, assumes the data has been rescaled to [-1,1] interval \"\"\"\n",
    "    # Pytorch ordering\n",
    "    x = x.permute(0, 2, 3, 1)\n",
    "    l = l.permute(0, 2, 3, 1)\n",
    "    xs = [int(y) for y in x.size()]\n",
    "    ls = [int(y) for y in l.size()]\n",
    "\n",
    "    # here and below: getting the means and adjusting them based on preceding\n",
    "    # sub-pixels\n",
    "    x = x.contiguous()\n",
    "\n",
    "    nits_model = nits_model.to(x.device)\n",
    "    x = x.reshape(-1, nits_model.d)\n",
    "    params = l.reshape(-1, nits_model.tot_params)\n",
    "\n",
    "    x_plus = (x * 127.5 + .5).round() / 127.5\n",
    "    x_min = (x * 127.5 - .5).round() / 127.5\n",
    "\n",
    "    cdf_delta = nits_model.cdf(x_plus, params) - nits_model.cdf(x_min, params)\n",
    "    log_cdf_plus = nits_model.cdf(x_plus, params).log()\n",
    "    log_one_minus_cdf_min = (1 - nits_model.cdf(x_min, params)).log()\n",
    "    log_pdf_mid = nits_model.pdf(x, params).log()\n",
    "\n",
    "    inner_inner_cond = (cdf_delta > 1e-5).float()\n",
    "    inner_inner_out  = inner_inner_cond * torch.log(torch.clamp(cdf_delta, min=1e-12)) + (1. - inner_inner_cond) * (log_pdf_mid - np.log(127.5))\n",
    "    inner_cond       = (x > 0.999).float()\n",
    "    inner_out        = inner_cond * log_one_minus_cdf_min + (1. - inner_cond) * inner_inner_out\n",
    "    cond             = (x < -0.999).float()\n",
    "    log_probs        = cond * log_cdf_plus + (1. - cond) * inner_out\n",
    "\n",
    "    return -log_probs.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not discretized_nits_loss(z, c_params, arch, c_model).isnan().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3]) torch.Size([64, 120])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.0000]],\n",
       "\n",
       "         [[ 1.0000]],\n",
       "\n",
       "         [[ 0.1392]]],\n",
       "\n",
       "\n",
       "        [[[ 0.7114]],\n",
       "\n",
       "         [[ 0.8208]],\n",
       "\n",
       "         [[-0.9878]]],\n",
       "\n",
       "\n",
       "        [[[-1.0000]],\n",
       "\n",
       "         [[ 0.6528]],\n",
       "\n",
       "         [[-0.4966]]],\n",
       "\n",
       "\n",
       "        [[[-0.4106]],\n",
       "\n",
       "         [[ 1.0000]],\n",
       "\n",
       "         [[-1.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0000]],\n",
       "\n",
       "         [[-0.4380]],\n",
       "\n",
       "         [[ 0.0347]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2241]],\n",
       "\n",
       "         [[ 0.2280]],\n",
       "\n",
       "         [[-1.0000]]],\n",
       "\n",
       "\n",
       "        [[[-0.1294]],\n",
       "\n",
       "         [[ 0.7817]],\n",
       "\n",
       "         [[-0.3188]]],\n",
       "\n",
       "\n",
       "        [[[-1.0000]],\n",
       "\n",
       "         [[ 1.0000]],\n",
       "\n",
       "         [[ 0.2788]]],\n",
       "\n",
       "\n",
       "        [[[-0.6108]],\n",
       "\n",
       "         [[ 0.6831]],\n",
       "\n",
       "         [[ 1.0000]]],\n",
       "\n",
       "\n",
       "        [[[-0.0493]],\n",
       "\n",
       "         [[ 0.3521]],\n",
       "\n",
       "         [[-0.0835]]],\n",
       "\n",
       "\n",
       "        [[[-0.1733]],\n",
       "\n",
       "         [[ 1.0000]],\n",
       "\n",
       "         [[ 0.3647]]],\n",
       "\n",
       "\n",
       "        [[[-1.0000]],\n",
       "\n",
       "         [[ 1.0000]],\n",
       "\n",
       "         [[-0.6948]]],\n",
       "\n",
       "\n",
       "        [[[ 0.6245]],\n",
       "\n",
       "         [[ 1.0000]],\n",
       "\n",
       "         [[-1.0000]]],\n",
       "\n",
       "\n",
       "        [[[-0.1616]],\n",
       "\n",
       "         [[-0.1431]],\n",
       "\n",
       "         [[-0.6567]]],\n",
       "\n",
       "\n",
       "        [[[-0.8599]],\n",
       "\n",
       "         [[-0.8579]],\n",
       "\n",
       "         [[-0.4966]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1382]],\n",
       "\n",
       "         [[ 1.0000]],\n",
       "\n",
       "         [[ 0.4673]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1274]],\n",
       "\n",
       "         [[ 1.0000]],\n",
       "\n",
       "         [[ 0.5386]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1304]],\n",
       "\n",
       "         [[-0.5581]],\n",
       "\n",
       "         [[ 0.1079]]],\n",
       "\n",
       "\n",
       "        [[[-0.0571]],\n",
       "\n",
       "         [[ 0.2729]],\n",
       "\n",
       "         [[-0.3667]]],\n",
       "\n",
       "\n",
       "        [[[-1.0000]],\n",
       "\n",
       "         [[-1.0000]],\n",
       "\n",
       "         [[ 1.0000]]],\n",
       "\n",
       "\n",
       "        [[[-0.3130]],\n",
       "\n",
       "         [[-1.0000]],\n",
       "\n",
       "         [[ 1.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0000]],\n",
       "\n",
       "         [[-1.0000]],\n",
       "\n",
       "         [[-1.0000]]],\n",
       "\n",
       "\n",
       "        [[[-0.3442]],\n",
       "\n",
       "         [[ 0.2017]],\n",
       "\n",
       "         [[ 1.0000]]],\n",
       "\n",
       "\n",
       "        [[[-0.5913]],\n",
       "\n",
       "         [[ 0.8325]],\n",
       "\n",
       "         [[-0.0435]]],\n",
       "\n",
       "\n",
       "        [[[ 0.3940]],\n",
       "\n",
       "         [[-1.0000]],\n",
       "\n",
       "         [[-0.6470]]],\n",
       "\n",
       "\n",
       "        [[[ 0.5347]],\n",
       "\n",
       "         [[ 1.0000]],\n",
       "\n",
       "         [[-0.8862]]],\n",
       "\n",
       "\n",
       "        [[[ 0.5337]],\n",
       "\n",
       "         [[ 1.0000]],\n",
       "\n",
       "         [[-0.9087]]],\n",
       "\n",
       "\n",
       "        [[[-1.0000]],\n",
       "\n",
       "         [[ 1.0000]],\n",
       "\n",
       "         [[ 0.2329]]],\n",
       "\n",
       "\n",
       "        [[[ 0.5483]],\n",
       "\n",
       "         [[ 0.4683]],\n",
       "\n",
       "         [[ 0.1196]]],\n",
       "\n",
       "\n",
       "        [[[ 0.3364]],\n",
       "\n",
       "         [[-1.0000]],\n",
       "\n",
       "         [[-0.8862]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0000]],\n",
       "\n",
       "         [[-0.8774]],\n",
       "\n",
       "         [[-0.1929]]],\n",
       "\n",
       "\n",
       "        [[[-0.2827]],\n",
       "\n",
       "         [[-0.0132]],\n",
       "\n",
       "         [[ 1.0000]]],\n",
       "\n",
       "\n",
       "        [[[-0.3315]],\n",
       "\n",
       "         [[ 1.0000]],\n",
       "\n",
       "         [[ 0.5083]]],\n",
       "\n",
       "\n",
       "        [[[-0.6636]],\n",
       "\n",
       "         [[-0.0239]],\n",
       "\n",
       "         [[ 0.6567]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0000]],\n",
       "\n",
       "         [[-1.0000]],\n",
       "\n",
       "         [[-0.1968]]],\n",
       "\n",
       "\n",
       "        [[[ 0.4067]],\n",
       "\n",
       "         [[ 1.0000]],\n",
       "\n",
       "         [[-1.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0532]],\n",
       "\n",
       "         [[-0.1255]],\n",
       "\n",
       "         [[ 0.7573]]],\n",
       "\n",
       "\n",
       "        [[[-0.1558]],\n",
       "\n",
       "         [[ 1.0000]],\n",
       "\n",
       "         [[-1.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.3530]],\n",
       "\n",
       "         [[ 0.9829]],\n",
       "\n",
       "         [[-0.6187]]],\n",
       "\n",
       "\n",
       "        [[[-0.6499]],\n",
       "\n",
       "         [[ 1.0000]],\n",
       "\n",
       "         [[ 1.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0000]],\n",
       "\n",
       "         [[ 0.6147]],\n",
       "\n",
       "         [[-1.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.6421]],\n",
       "\n",
       "         [[ 0.6138]],\n",
       "\n",
       "         [[-1.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.5181]],\n",
       "\n",
       "         [[-1.0000]],\n",
       "\n",
       "         [[ 0.2212]]],\n",
       "\n",
       "\n",
       "        [[[-1.0000]],\n",
       "\n",
       "         [[ 0.6147]],\n",
       "\n",
       "         [[-0.1157]]],\n",
       "\n",
       "\n",
       "        [[[-1.0000]],\n",
       "\n",
       "         [[ 1.0000]],\n",
       "\n",
       "         [[-0.8462]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0000]],\n",
       "\n",
       "         [[-0.2261]],\n",
       "\n",
       "         [[-0.2339]]],\n",
       "\n",
       "\n",
       "        [[[-1.0000]],\n",
       "\n",
       "         [[ 1.0000]],\n",
       "\n",
       "         [[ 0.6421]]],\n",
       "\n",
       "\n",
       "        [[[-1.0000]],\n",
       "\n",
       "         [[-0.8979]],\n",
       "\n",
       "         [[ 0.1841]]],\n",
       "\n",
       "\n",
       "        [[[-0.0767]],\n",
       "\n",
       "         [[-1.0000]],\n",
       "\n",
       "         [[ 1.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1206]],\n",
       "\n",
       "         [[ 1.0000]],\n",
       "\n",
       "         [[-0.0669]]],\n",
       "\n",
       "\n",
       "        [[[ 0.4038]],\n",
       "\n",
       "         [[ 0.2192]],\n",
       "\n",
       "         [[-1.0000]]],\n",
       "\n",
       "\n",
       "        [[[-0.3052]],\n",
       "\n",
       "         [[ 0.3960]],\n",
       "\n",
       "         [[ 0.2974]]],\n",
       "\n",
       "\n",
       "        [[[ 0.7798]],\n",
       "\n",
       "         [[ 0.6841]],\n",
       "\n",
       "         [[ 0.6079]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0405]],\n",
       "\n",
       "         [[-1.0000]],\n",
       "\n",
       "         [[ 1.0000]]],\n",
       "\n",
       "\n",
       "        [[[-0.2964]],\n",
       "\n",
       "         [[-0.3257]],\n",
       "\n",
       "         [[ 0.2485]]],\n",
       "\n",
       "\n",
       "        [[[ 0.3433]],\n",
       "\n",
       "         [[ 0.0942]],\n",
       "\n",
       "         [[-0.3247]]],\n",
       "\n",
       "\n",
       "        [[[ 0.7944]],\n",
       "\n",
       "         [[-0.0845]],\n",
       "\n",
       "         [[-0.1919]]],\n",
       "\n",
       "\n",
       "        [[[-0.0122]],\n",
       "\n",
       "         [[-0.6362]],\n",
       "\n",
       "         [[ 0.8628]]],\n",
       "\n",
       "\n",
       "        [[[ 0.5845]],\n",
       "\n",
       "         [[-0.1968]],\n",
       "\n",
       "         [[-0.9517]]],\n",
       "\n",
       "\n",
       "        [[[-1.0000]],\n",
       "\n",
       "         [[ 1.0000]],\n",
       "\n",
       "         [[-1.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0000]],\n",
       "\n",
       "         [[ 0.8022]],\n",
       "\n",
       "         [[-1.0000]]],\n",
       "\n",
       "\n",
       "        [[[-0.1460]],\n",
       "\n",
       "         [[-1.0000]],\n",
       "\n",
       "         [[ 0.4995]]],\n",
       "\n",
       "\n",
       "        [[[-1.0000]],\n",
       "\n",
       "         [[ 1.0000]],\n",
       "\n",
       "         [[ 0.6050]]],\n",
       "\n",
       "\n",
       "        [[[ 0.4263]],\n",
       "\n",
       "         [[-0.8853]],\n",
       "\n",
       "         [[ 1.0000]]]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nits_sample(c_params, arch, 0, 0, c_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
