{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class PositiveLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, constraint_type='clamp', store_weights=True):\n",
    "        super(PositiveLinear, self).__init__()\n",
    "        self.constraint_type = constraint_type\n",
    "        self.in_features, self.out_features = in_features, out_features\n",
    "\n",
    "        if store_weights:\n",
    "            # initialize weight in Unif(+eps, sqrt(k)), i.e. log_weight in Unif(log(eps), log(sqrt(k)))\n",
    "            # where k = 1 / in_features\n",
    "            pre_weight = torch.rand((out_features, in_features))\n",
    "\n",
    "            if self.constraint_type == 'exp':\n",
    "                init_min, init_max = np.log(1e-2), np.log(np.sqrt(1 / in_features))\n",
    "            elif self.constraint_type == 'clamp':\n",
    "                init_min, init_max = 1e-2, np.sqrt(1 / in_features)\n",
    "            elif self.constraint_type == '':\n",
    "                init_min, init_max = -np.sqrt(1 / in_features), np.sqrt(1 / in_features)\n",
    "            self.pre_weight = (pre_weight * (init_max - init_min)) + init_min\n",
    "\n",
    "            bias = torch.rand((out_features))\n",
    "            scale = 1 / in_features\n",
    "            self.bias = bias * 2 * scale - scale\n",
    "\n",
    "            self.pre_weight, self.bias = nn.Parameter(self.pre_weight), nn.Parameter(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.constraint_type == 'neg_exp':\n",
    "            weight = 1 / self.pre_weight.exp()\n",
    "            return x.mm(weight.T) - (self.bias.unsqueeze(-1) * weight).mean(axis=-1)\n",
    "        elif self.constraint_type == 'exp':\n",
    "            weight = self.pre_weight.exp()\n",
    "        elif self.constraint_type == 'softmax':\n",
    "            weight = F.softmax(self.pre_weight, dim=-1)\n",
    "        elif self.constraint_type == 'clamp':\n",
    "            weight = self.pre_weight.clamp(min=0.)\n",
    "        elif self.constraint_type == '':\n",
    "            weight = self.pre_weight\n",
    "        return x.mm(weight.T) + self.bias\n",
    "\n",
    "class MonotonicInverse(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, self, input):\n",
    "        with torch.no_grad():\n",
    "            b = bisection_search(self.F, input, self.start, self.end, n_iter=20)\n",
    "\n",
    "        dy = 1 / torch.autograd.functional.jacobian(self.F, b, create_graph=True, vectorize=True)\n",
    "        ctx.save_for_backward(dy.reshape(len(input), 1))\n",
    "        return b\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        dy, = ctx.saved_tensors\n",
    "        return None, dy\n",
    "\n",
    "class ModelInverse(nn.Module):\n",
    "    def __init__(self, arch, start=0., end=1., store_weights=True, \n",
    "                 constraint_type='exp', monotonic_const=1e-2, final_layer_constraint='softmax'):\n",
    "        super(ModelInverse, self).__init__()\n",
    "        self.d = arch[0]\n",
    "        self.monotonic_const = monotonic_const\n",
    "        self.store_weights = store_weights\n",
    "        self.constraint_type = constraint_type\n",
    "        self.final_layer_constraint = final_layer_constraint\n",
    "        self.last_layer = len(arch) - 2\n",
    "        self.layers = self.build_layers(arch)\n",
    "\n",
    "        self.register_buffer('start', torch.tensor(start).reshape(1, 1))\n",
    "        self.register_buffer('end', torch.tensor(end).reshape(1, 1))\n",
    "\n",
    "    def build_layers(self, arch):\n",
    "        self.n_params = 0\n",
    "        layers = nn.ModuleList()\n",
    "\n",
    "        for i, (a1, a2) in enumerate(zip(arch[:-1], arch[1:])):\n",
    "            # add nonlinearities\n",
    "            self.n_params += (a1 * a2)\n",
    "            if i < self.last_layer:\n",
    "                layers.append(PositiveLinear(a1, a2, store_weights=self.store_weights,\n",
    "                                         constraint_type=self.constraint_type))\n",
    "                layers.append(nn.Sigmoid())\n",
    "                self.n_params += a2\n",
    "            else:\n",
    "                layers.append(PositiveLinear(a1, a2, store_weights=self.store_weights,\n",
    "                                         constraint_type=self.final_layer_constraint))\n",
    "                if self.final_layer_constraint != 'softmax':\n",
    "                    layers.append(nn.Sigmoid())\n",
    "                    self.n_params += a2\n",
    "\n",
    "        return layers\n",
    "\n",
    "    def set_params(self, param_tensor):\n",
    "        if self.store_weights:\n",
    "            raise NotImplementedError(\"set_parameters() should not be called if store_weights == True!\")\n",
    "\n",
    "        assert len(param_tensor) == self.n_params, \"{} =/= {}\".format(str(param_tensor.shape), str(self.n_params))\n",
    "\n",
    "        cur_idx = 0\n",
    "        i = 0\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, PositiveLinear):\n",
    "                weight_shape = (layer.out_features, layer.in_features)\n",
    "                n_params = np.prod(weight_shape)\n",
    "                layer.pre_weight = param_tensor[cur_idx:cur_idx+n_params].reshape(weight_shape)\n",
    "                cur_idx += n_params\n",
    "\n",
    "                if i < self.last_layer or self.final_layer_constraint != 'softmax':\n",
    "                    layer.bias = param_tensor[cur_idx:cur_idx+layer.out_features]\n",
    "                    cur_idx += layer.out_features\n",
    "                else:\n",
    "                    layer.bias = torch.zeros(layer.out_features).to(param_tensor.device)\n",
    "                    \n",
    "                i += 1\n",
    "\n",
    "    def apply_layers(self, x):\n",
    "        y = x\n",
    "        for l in self.layers:\n",
    "            y = l(y)\n",
    "\n",
    "        return y + self.monotonic_const * x\n",
    "\n",
    "    def scale(self, y):\n",
    "        start, end = self.apply_layers(self.start), self.apply_layers(self.end)\n",
    "        return (y - start) / (end - start)\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError(\"forward() should not be used!\")\n",
    "\n",
    "    def f(self, x):\n",
    "        # compute df/dx\n",
    "        dy = []\n",
    "        for x_ in x:\n",
    "            dy_ = torch.autograd.functional.jacobian(self.F, x_.reshape(-1, 1), create_graph=True, vectorize=False)\n",
    "            dy.append(dy_.reshape(1, 1))\n",
    "\n",
    "        dy = torch.cat(dy, axis=0)\n",
    "        return dy\n",
    "\n",
    "    def f_(self, x):\n",
    "        # compute df/dx\n",
    "        dy = []\n",
    "        for x_ in x:\n",
    "            dy_ = torch.autograd.functional.jacobian(self.apply_layers, x_.reshape(-1, 1),\n",
    "                                                     create_graph=True, vectorize=False)\n",
    "            dy.append(dy_.reshape(1, 1))\n",
    "\n",
    "        dy = torch.cat(dy, axis=0)\n",
    "        return dy\n",
    "\n",
    "    def F(self, x):\n",
    "        return self.scale(self.apply_layers(x))\n",
    "\n",
    "    def pdf(self, x):\n",
    "        return self.f(x)\n",
    "\n",
    "    def cdf(self, x):\n",
    "        return self.F(x)\n",
    "\n",
    "    def F_inv(self, x):\n",
    "        inverse = MonotonicInverse.apply\n",
    "\n",
    "        z = []\n",
    "        for x_ in x:\n",
    "            z.append(inverse(self, x_).reshape(1, 1))\n",
    "\n",
    "        z = torch.cat(z, axis=0)\n",
    "\n",
    "        return z\n",
    "\n",
    "    def sample(self, n, batch_size=1):\n",
    "        x = []\n",
    "        while n > batch_size:\n",
    "            z = torch.rand(batch_size, device=self.start.device) * (self.end - self.start) + self.start\n",
    "            x.append(self.F_inv(z.reshape(-1, 1)))\n",
    "            n -= batch_size\n",
    "        else:\n",
    "            z = torch.rand(n, device=self.start.device) * (self.end - self.start) + self.start\n",
    "            x.append(self.F_inv(z.reshape(-1, 1)))\n",
    "\n",
    "        return torch.cat(x, axis=0)\n",
    "\n",
    "def bisection_search(increasing_func, target, start, end, n_iter=20, eps=1e-3):\n",
    "    query = (start + end) / 2\n",
    "    result = increasing_func(query)\n",
    "\n",
    "    if n_iter == 0:\n",
    "        print(\"bottomed out recursion depth, return best guess epsilon =\", (result - target).norm())\n",
    "        return query\n",
    "    elif (result - target).norm() < eps:\n",
    "        return query\n",
    "    elif result > target:\n",
    "        return bisection_search(increasing_func, target, start, query, n_iter-1, eps)\n",
    "    else:\n",
    "        return bisection_search(increasing_func, target, query, end, n_iter-1, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class NITSMonotonicInverse(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, self, input, params):\n",
    "        with torch.no_grad():\n",
    "            b = self.bisection_search(input, params)\n",
    "\n",
    "        dy = 1 / self.pdf(b, params)\n",
    "        ctx.save_for_backward(dy.reshape(len(input), -1))\n",
    "        return b\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        dy, = ctx.saved_tensors\n",
    "        return None, dy\n",
    "\n",
    "class NITS(nn.Module):\n",
    "    def __init__(self, arch, start=0., end=1., constraint_type='neg_exp',\n",
    "                 monotonic_const=1e-3, activation='sigmoid', final_layer_constraint='softmax'):\n",
    "        super(NITS, self).__init__()\n",
    "        self.arch = arch\n",
    "        self.monotonic_const = monotonic_const\n",
    "        self.constraint_type = constraint_type\n",
    "        self.final_layer_constraint = final_layer_constraint\n",
    "        self.last_layer = len(arch) - 2\n",
    "        self.activation = activation\n",
    "\n",
    "        # count parameters\n",
    "        self.n_params = 0\n",
    "\n",
    "        for i, (a1, a2) in enumerate(zip(arch[:-1], arch[1:])):\n",
    "            self.n_params += (a1 * a2)\n",
    "            if i < self.last_layer or final_layer_constraint != 'softmax':\n",
    "                self.n_params += a2\n",
    "\n",
    "        # set start and end tensors\n",
    "        self.register_buffer('start', torch.tensor(start).reshape(1, arch[0]))\n",
    "        self.register_buffer('end', torch.tensor(end).reshape(1, arch[0]))\n",
    "\n",
    "    def apply_constraint(self, A, constraint_type):\n",
    "        if constraint_type == 'neg_exp':\n",
    "            A = (-A).exp()\n",
    "        if constraint_type == 'exp':\n",
    "            A = A.exp()\n",
    "        elif constraint_type == 'clamp':\n",
    "            A = A.clamp(min=0.)\n",
    "        elif constraint_type == 'softmax':\n",
    "            A = F.softmax(A, dim=-1)\n",
    "        elif constraint_type == '':\n",
    "            pass\n",
    "\n",
    "        return A\n",
    "\n",
    "    def apply_act(self, x):\n",
    "        if self.activation == 'tanh':\n",
    "            return x.tanh()\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return x.sigmoid()\n",
    "        elif self.activation == 'linear':\n",
    "            return x\n",
    "\n",
    "    def forward_(self, x, params, return_intermediaries=False):\n",
    "        orig_x = x\n",
    "\n",
    "        # store pre-activations and weight matrices\n",
    "        pre_activations = []\n",
    "        nonlinearities = []\n",
    "        As = []\n",
    "        bs = []\n",
    "\n",
    "        cur_idx = 0\n",
    "\n",
    "        # compute layers\n",
    "        for i, (in_features, out_features) in enumerate(zip(self.arch[:-1], self.arch[1:])):\n",
    "            # get linear weights\n",
    "            A_end = cur_idx + in_features * out_features\n",
    "            A = params[:,cur_idx:A_end].reshape(-1, out_features, in_features)\n",
    "            cur_idx = A_end\n",
    "\n",
    "            constraint = self.constraint_type if i < self.last_layer else self.final_layer_constraint\n",
    "            A = self.apply_constraint(A, constraint)\n",
    "            As.append(A)\n",
    "            x = torch.einsum('nij,nj->ni', A, x)\n",
    "\n",
    "            # get bias weights if not softmax layer\n",
    "            if i < self.last_layer or self.final_layer_constraint != 'softmax':\n",
    "                b_end = A_end + out_features\n",
    "                b = params[:,A_end:b_end].reshape(-1, out_features)\n",
    "                bs.append(b)\n",
    "                cur_idx = b_end\n",
    "                if i < self.last_layer and self.constraint_type == 'neg_exp':\n",
    "                    x = x - (b.unsqueeze(-1) * A).mean(axis=-1)\n",
    "                elif i == self.last_layer and self.final_layer_constraint == 'neg_exp':\n",
    "                    x = x - (b.unsqueeze(-1) * A).mean(axis=-1)\n",
    "                else:\n",
    "                    x = x + b\n",
    "                pre_activations.append(x)\n",
    "                x = self.apply_act(x)\n",
    "                nonlinearities.append(self.activation)\n",
    "            else:\n",
    "                pre_activations.append(x)\n",
    "                nonlinearities.append('linear')\n",
    "        \n",
    "        x = x + self.monotonic_const * orig_x\n",
    "\n",
    "        if return_intermediaries:\n",
    "            return x, pre_activations, As, bs, nonlinearities\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def cdf(self, x, params, return_intermediaries=False):\n",
    "        # get scaling factors\n",
    "        start = self.forward_(self.start, params)\n",
    "        end = self.forward_(self.end, params)\n",
    "\n",
    "        # compute pre-scaled cdf, then scale\n",
    "        y, pre_activations, As, bs, nonlinearities = self.forward_(x, params, return_intermediaries=True)\n",
    "        scale = 1 / (end - start)\n",
    "        y_scaled = (y - start) * scale\n",
    "\n",
    "        # accounting\n",
    "        pre_activations.append(y_scaled)\n",
    "        As.append(scale.reshape(-1, 1, 1))\n",
    "        nonlinearities.append('linear')\n",
    "\n",
    "        if return_intermediaries:\n",
    "            return y_scaled, pre_activations, As, bs, nonlinearities\n",
    "        else:\n",
    "            return y_scaled\n",
    "\n",
    "    def fc_gradient(self, grad, pre_activation, A, activation):\n",
    "        if activation == 'linear':\n",
    "            pass\n",
    "        elif activation == 'tanh':\n",
    "            grad = grad * (1 - pre_activation.tanh() ** 2)\n",
    "        elif activation == 'sigmoid':\n",
    "            sig_act = pre_activation.sigmoid()\n",
    "            grad = grad * sig_act * (1 - sig_act)\n",
    "\n",
    "        return torch.einsum('ni,nij->nj', grad, A)\n",
    "\n",
    "    def backward_primitive_(self, y, pre_activations, As, bs, nonlinearities):\n",
    "        pre_activations.reverse()\n",
    "        As.reverse()\n",
    "        nonlinearities.reverse()\n",
    "        grad = torch.ones_like(y, device=y.device)\n",
    "\n",
    "        for i, (A, pre_activation, nonlinearity) in enumerate(zip(As, pre_activations, nonlinearities)):\n",
    "            grad = self.fc_gradient(grad, pre_activation, A, activation=nonlinearity)\n",
    "\n",
    "        return grad\n",
    "\n",
    "    def backward_(self, x, params):\n",
    "        y, pre_activations, As, bs, nonlinearities = self.forward_(x, params, return_intermediaries=True)\n",
    "\n",
    "        grad = self.backward_primitive_(y, pre_activations, As, bs, nonlinearities)\n",
    "\n",
    "        return grad + self.monotonic_const\n",
    "\n",
    "    def pdf(self, x, params):\n",
    "        y, pre_activations, As, bs, nonlinearities = self.cdf(x, params, return_intermediaries=True)\n",
    "\n",
    "        grad = self.backward_primitive_(y, pre_activations, As, bs, nonlinearities)\n",
    "\n",
    "        return grad + self.monotonic_const * As[0].reshape(-1, 1)\n",
    "\n",
    "    def sample(self, params):\n",
    "        z = torch.rand((len(params), 1), device=params.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = self.icdf(z, params)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def icdf(self, z, params):\n",
    "        func = NITSMonotonicInverse.apply\n",
    "\n",
    "        return func(self, z, params)\n",
    "\n",
    "    def bisection_search(self, y, params, eps=1e-3):\n",
    "        low = torch.ones((len(y), 1), device=y.device) * self.start\n",
    "        high = torch.ones((len(y), 1), device=y.device) * self.end\n",
    "\n",
    "        while ((high - low) > eps).any():\n",
    "            x_hat = (low + high) / 2\n",
    "            y_hat = self.cdf(x_hat, params)\n",
    "            low = torch.where(y_hat > y, low, x_hat)\n",
    "            high = torch.where(y_hat > y, x_hat, high)\n",
    "\n",
    "        return high\n",
    "\n",
    "class MultiDimNITS(NITS):\n",
    "    def __init__(self, d, arch, start=-2., end=2., constraint_type='neg_exp',\n",
    "                 monotonic_const=1e-2, final_layer_constraint='softmax'):\n",
    "        super(MultiDimNITS, self).__init__(arch, start, end,\n",
    "                                           constraint_type=constraint_type,\n",
    "                                           monotonic_const=monotonic_const,\n",
    "                                           final_layer_constraint=final_layer_constraint)\n",
    "        self.d = d\n",
    "        self.tot_params = self.n_params * d\n",
    "        self.final_layer_constraint = final_layer_constraint\n",
    "        \n",
    "        self.register_buffer('start', torch.tensor(start).reshape(1, 1).tile(1, d))\n",
    "        self.register_buffer('end', torch.tensor(end).reshape(1, 1).tile(1, d))\n",
    "        \n",
    "        self.nits = NITS(arch, start, end, \n",
    "                         constraint_type=constraint_type,\n",
    "                         monotonic_const=monotonic_const,\n",
    "                         final_layer_constraint=final_layer_constraint)\n",
    "\n",
    "    def multidim_reshape(self, x, params):\n",
    "        n = max(len(x), len(params))\n",
    "        _, d = x.shape\n",
    "        assert d == self.d\n",
    "        assert params.shape[1] == self.tot_params\n",
    "        assert len(x) == len(params) or len(x) == 1 or len(params) == 1\n",
    "        \n",
    "        if len(params) == 1:\n",
    "            params = params.reshape(self.d, self.n_params).tile((n, 1))\n",
    "        elif len(params) == n:\n",
    "            params = params.reshape(-1, self.n_params)\n",
    "        else:\n",
    "            raise NotImplementedError('len(params) should be 1 or {}, but it is {}.'.format(n, len(params)))\n",
    "        \n",
    "        if len(x) == 1:\n",
    "            x = x.reshape(1, self.d).tile((n, 1)).reshape(-1, 1)\n",
    "        elif len(x) == n:\n",
    "            x = x.reshape(-1, 1)\n",
    "        else:\n",
    "            raise NotImplementedError('len(params) should be 1 or {}, but it is {}.'.format(n, len(x)))\n",
    "            \n",
    "\n",
    "        return x, params\n",
    "\n",
    "    def forward_(self, x, params, return_intermediaries=False):\n",
    "        n = max(len(x), len(params))\n",
    "        x, params = self.multidim_reshape(x, params)\n",
    "\n",
    "        if return_intermediaries:\n",
    "            x, pre_activations, As, bs, nonlinearities = self.nits.forward_(x, params, return_intermediaries)\n",
    "            x = x.reshape((n, self.d))\n",
    "            return x, pre_activations, As, bs, nonlinearities\n",
    "        else:\n",
    "            x = self.nits.forward_(x, params, return_intermediaries)\n",
    "            x = x.reshape((n, self.d))\n",
    "            return x\n",
    "\n",
    "    def backward_(self, x, params):\n",
    "        n = max(len(x), len(params))\n",
    "        x, params = self.multidim_reshape(x, params)\n",
    "\n",
    "        return self.nits.backward_(x, params).reshape((n, self.d))\n",
    "\n",
    "    def cdf(self, x, params):\n",
    "        n = max(len(x), len(params))\n",
    "        x, params = self.multidim_reshape(x, params)\n",
    "\n",
    "        return self.nits.cdf(x, params).reshape((n, self.d))\n",
    "\n",
    "    def icdf(self, x, params):\n",
    "        n = max(len(x), len(params))\n",
    "        x, params = self.multidim_reshape(x, params)\n",
    "\n",
    "        return self.nits.icdf(x, params).reshape((n, self.d))\n",
    "\n",
    "    def pdf(self, x, params):\n",
    "        n = max(len(x), len(params))\n",
    "        x, params = self.multidim_reshape(x, params)\n",
    "\n",
    "        return self.nits.pdf(x, params).reshape((n, self.d))\n",
    "\n",
    "    def sample(self, n, params):\n",
    "        if len(params) == 1:\n",
    "            params = params.reshape(self.d, self.n_params).tile((n, 1))\n",
    "        elif len(params) == n:\n",
    "            params = params.reshape(-1, self.n_params)\n",
    "\n",
    "        return self.nits.sample(params).reshape((-1, self.d))\n",
    "\n",
    "    def initialize_parameters(self, n, constraint_type):\n",
    "        params = torch.rand((self.d * n, self.n_params))\n",
    "\n",
    "        def init_constant(params, in_features, constraint_type):\n",
    "            const = np.sqrt(1 / in_features)\n",
    "            if constraint_type == 'clamp':\n",
    "                params = params.abs() * const\n",
    "            elif constraint_type == 'exp':\n",
    "                params = params * np.log(const)\n",
    "            elif constraint_type == 'tanh':\n",
    "                params = params * np.arctanh(const - 1)\n",
    "\n",
    "            return params\n",
    "\n",
    "        cur_idx = 0\n",
    "\n",
    "        for i, (a1, a2) in enumerate(zip(self.arch[:-1], self.arch[1:])):\n",
    "            next_idx = cur_idx + (a1 * a2)\n",
    "            if i < len(self.arch) - 2 or self.final_layer_constraint != 'softmax':\n",
    "                 next_idx = next_idx + a2\n",
    "            params[:,cur_idx:next_idx] = init_constant(params[:,cur_idx:next_idx], a2, constraint_type)\n",
    "            cur_idx = next_idx\n",
    "\n",
    "        return params.reshape((n, self.d * self.n_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "# from model import *\n",
    "# from autograd_model import *\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "n = 1024\n",
    "start, end = -2., 2.\n",
    "arch = [1, 8, 1]\n",
    "monotonic_const = 1e-2\n",
    "\n",
    "for d in [1, 2, 10]:\n",
    "    for constraint_type in ['neg_exp', 'exp']:\n",
    "        for final_layer_constraint in ['softmax', 'exp']:\n",
    "            print(\"\"\"\n",
    "            Testing configuration:\n",
    "                d: {}\n",
    "                constraint_type: {}\n",
    "                final_layer_constraint: {}\n",
    "                  \"\"\".format(d, constraint_type, final_layer_constraint))\n",
    "            ############################\n",
    "            # DEFINE MODELS            #\n",
    "            ############################\n",
    "            \n",
    "            model = MultiDimNITS(d=d, start=start, end=end, arch=arch,\n",
    "                                 monotonic_const=monotonic_const, constraint_type=constraint_type,\n",
    "                                 final_layer_constraint=final_layer_constraint).to(device)\n",
    "            params = torch.randn((n, d * model.n_params)).to(device)\n",
    "            \n",
    "            ############################\n",
    "            # SANITY CHECKS            #\n",
    "            ############################\n",
    "\n",
    "            # check that the function integrates to 1\n",
    "            assert torch.allclose(torch.ones((n, d)).to(device), \n",
    "                                  model.cdf(model.end, params) - model.cdf(model.start, params), atol=1e-5)\n",
    "\n",
    "            # check that the pdf is all positive\n",
    "            z = torch.linspace(start, end, steps=n, device=device)[:,None].tile((1, d))\n",
    "            assert (model.pdf(z, params) >= 0).all()\n",
    "\n",
    "            # check that the cdf is the inverted\n",
    "            cdf = model.cdf(z, params[0:1])\n",
    "            icdf = model.icdf(cdf, params[0:1])\n",
    "            assert (z - icdf <= 1e-3).all()\n",
    "\n",
    "            ############################\n",
    "            # COMPARE TO AUTOGRAD NITS #\n",
    "            ############################\n",
    "            autograd_model = ModelInverse(arch=arch, start=start, end=end, store_weights=False, \n",
    "                                          constraint_type=constraint_type, monotonic_const=monotonic_const,\n",
    "                                          final_layer_constraint=final_layer_constraint)\n",
    "\n",
    "            def zs_params_to_forwards(zs, params):\n",
    "                out = []\n",
    "                for z, param in zip(zs, params):\n",
    "                    for d_ in range(d):\n",
    "                        start_idx, end_idx = d_ * autograd_model.n_params, (d_ + 1) * autograd_model.n_params\n",
    "                        autograd_model.set_params(param[start_idx:end_idx])\n",
    "                        out.append(autograd_model.apply_layers(z[d_:d_+1][None,:]))\n",
    "\n",
    "                out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "                return out\n",
    "\n",
    "            autograd_outs = zs_params_to_forwards(z, params)\n",
    "            outs = model.forward_(z, params)\n",
    "            assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "            def zs_params_to_cdfs(zs, params):\n",
    "                out = []\n",
    "                for z, param in zip(zs, params):\n",
    "                    for d_ in range(d):\n",
    "                        start_idx, end_idx = d_ * autograd_model.n_params, (d_ + 1) * autograd_model.n_params\n",
    "                        autograd_model.set_params(param[start_idx:end_idx])\n",
    "                        out.append(autograd_model.cdf(z[d_:d_+1][None,:]))\n",
    "\n",
    "                out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "                return out\n",
    "\n",
    "            autograd_outs = zs_params_to_cdfs(z, params)\n",
    "            outs = model.cdf(z, params)\n",
    "            assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "            def zs_params_to_pdfs(zs, params):\n",
    "                out = []\n",
    "                for z, param in zip(zs, params):\n",
    "                    for d_ in range(d):\n",
    "                        start_idx, end_idx = d_ * autograd_model.n_params, (d_ + 1) * autograd_model.n_params\n",
    "                        autograd_model.set_params(param[start_idx:end_idx])\n",
    "                        out.append(autograd_model.pdf(z[d_:d_+1][None,:]))\n",
    "\n",
    "                out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "                return out\n",
    "\n",
    "            autograd_outs = zs_params_to_pdfs(z, params)\n",
    "            outs = model.pdf(z, params)\n",
    "            assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "            # try with single parameter, many zs\n",
    "\n",
    "            def zs_params_to_pdfs(zs, param):\n",
    "                out = []\n",
    "                for z in zs:\n",
    "                    for d_ in range(d):\n",
    "                        start_idx, end_idx = d_ * autograd_model.n_params, (d_ + 1) * autograd_model.n_params\n",
    "                        autograd_model.set_params(param[start_idx:end_idx])\n",
    "                        out.append(autograd_model.pdf(z[d_:d_+1][None,:]))\n",
    "\n",
    "                out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "                return out\n",
    "\n",
    "            autograd_outs = zs_params_to_pdfs(z, params[0])\n",
    "            outs = model.pdf(z, params[0:1])\n",
    "            assert torch.allclose(autograd_outs, outs, atol=1e-4)\n",
    "\n",
    "            # try with single z, many parameters\n",
    "\n",
    "            def zs_params_to_pdfs(z, params):\n",
    "                out = []\n",
    "                for param in params:\n",
    "                    for d_ in range(d):\n",
    "                        start_idx, end_idx = d_ * autograd_model.n_params, (d_ + 1) * autograd_model.n_params\n",
    "                        autograd_model.set_params(param[start_idx:end_idx])\n",
    "                        out.append(autograd_model.pdf(z[d_:d_+1][None,:]))\n",
    "\n",
    "                out = torch.cat(out, axis=0).reshape(-1, d)\n",
    "                return out\n",
    "\n",
    "            autograd_outs = zs_params_to_pdfs(z[0], params)\n",
    "            outs = model.pdf(z[0:1], params)\n",
    "            assert torch.allclose(autograd_outs, outs, atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing arch = [1, 10, 1], 'neg_exp' constraint_type, 'softmax' final_layer_constraint against discretized mixture of logistics.\n",
      "Finished unit tests. All passed!\n"
     ]
    }
   ],
   "source": [
    "from discretized_mol import *\n",
    "print(\"Testing arch = [1, 10, 1], 'neg_exp' constraint_type, 'softmax' final_layer_constraint \" \\\n",
    "      \"against discretized mixture of logistics.\")\n",
    "\n",
    "model = MultiDimNITS(d=1, start=-1e5, end=1e5, arch=[1, 10, 1],\n",
    "                     monotonic_const=0., constraint_type='neg_exp',\n",
    "                     final_layer_constraint='softmax').to(device)\n",
    "params = torch.randn((n, model.n_params, 1, 1))\n",
    "z = torch.randn((n, 1, 1, 1))\n",
    "\n",
    "loss1 = discretized_mix_logistic_loss_1d3(z, params)\n",
    "loss2 = discretized_nits_loss(z, params, arch=[1, 10, 1], nits_model=model)\n",
    "\n",
    "assert (loss1 - loss2).norm() < 1e-3\n",
    "            \n",
    "print(\"Finished unit tests. All passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_layer_constraint = 'softmax'\n",
    "model = MultiDimNITS(d=d, start=start, end=end, arch=arch,\n",
    "                                 monotonic_const=monotonic_const, constraint_type=constraint_type,\n",
    "                                 final_layer_constraint=final_layer_constraint).to(device)\n",
    "params = torch.randn((n, d * model.n_params)).to(device)\n",
    "autograd_model = ModelInverse(arch=arch, start=start, end=end, store_weights=False, \n",
    "                                          constraint_type=constraint_type, monotonic_const=monotonic_const,\n",
    "                                          final_layer_constraint=final_layer_constraint)\n",
    "\n",
    "print(\"\"\"Testing configuration:\n",
    "                     d: {}\n",
    "                     constraint_type: {}\n",
    "                     final_layer_constraint: {}\n",
    "                  \"\"\".format(d, constraint_type, final_layer_constraint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autograd_model.set_params(params[0])\n",
    "autograd_model.apply_layers(z[40:41])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 / autograd_model.layers[0].pre_weight.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, pre_activations, As, bs, nonlinearities = model.forward_(z[40:41], params[0:1], return_intermediaries=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "As"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
