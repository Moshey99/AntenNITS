{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan 20 18:57:31 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.42.01    Driver Version: 470.42.01    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:1B:00.0 Off |                  N/A |\n",
      "| 29%   37C    P8     1W / 250W |   5955MiB / 11016MiB |      9%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:1C:00.0 Off |                  N/A |\n",
      "| 43%   70C    P2   263W / 250W |   8251MiB / 11019MiB |     93%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  Off  | 00000000:1D:00.0 Off |                  N/A |\n",
      "| 54%   87C    P2   230W / 250W |   8403MiB / 11019MiB |     94%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce ...  Off  | 00000000:1E:00.0 Off |                  N/A |\n",
      "| 52%   83C    P2   234W / 250W |   8159MiB / 11019MiB |     95%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA GeForce ...  Off  | 00000000:3D:00.0 Off |                  N/A |\n",
      "| 49%   76C    P0   113W / 250W |      4MiB / 11019MiB |     33%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA GeForce ...  Off  | 00000000:3F:00.0 Off |                  N/A |\n",
      "| 29%   33C    P8    22W / 250W |   8876MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA GeForce ...  Off  | 00000000:40:00.0 Off |                  N/A |\n",
      "| 51%   82C    P2   252W / 250W |   9209MiB / 11019MiB |     97%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA GeForce ...  Off  | 00000000:41:00.0 Off |                  N/A |\n",
      "| 29%   44C    P2    53W / 250W |   2129MiB / 11019MiB |     14%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     19849      C   python                           1439MiB |\n",
      "|    3   N/A  N/A     19849      C   python                           8155MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16,10,1]\n",
      "device: cuda:4\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os, shutil\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, transforms, utils\n",
    "from tensorboardX import SummaryWriter\n",
    "from nits.pixelcnn_model import *\n",
    "from nits.model import NITS, ConditionalNITS\n",
    "from nits.discretized_mol import discretized_nits_loss, nits_sample\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def list_str_to_list(s):\n",
    "    print(s)\n",
    "    assert s[0] == '[' and s[-1] == ']'\n",
    "    s = s[1:-1]\n",
    "    s = s.replace(' ', '')\n",
    "    s = s.split(',')\n",
    "\n",
    "    s = [int(x) for x in s]\n",
    "\n",
    "    return s\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "# data I/O\n",
    "parser.add_argument('-g', '--gpu', type=str,\n",
    "                    default='', help='Location for the dataset')\n",
    "parser.add_argument('-i', '--data_dir', type=str,\n",
    "                    default='/data/pixelcnn/data/', help='Location for the dataset')\n",
    "parser.add_argument('-o', '--save_dir', type=str, default='/data/pixelcnn/models/',\n",
    "                    help='Location for parameter checkpoints and samples')\n",
    "parser.add_argument('-d', '--dataset', type=str,\n",
    "                    default='cifar', help='Can be cifar / mnist')\n",
    "parser.add_argument('-p', '--print_every', type=int, default=50,\n",
    "                    help='how many iterations between print statements')\n",
    "parser.add_argument('-t', '--save_interval', type=int, default=10,\n",
    "                    help='Every how many epochs to write checkpoint/samples?')\n",
    "parser.add_argument('-r', '--load_params', type=str, default=None,\n",
    "                    help='Restore training from previous model checkpoint?')\n",
    "\n",
    "# pixelcnn model\n",
    "parser.add_argument('-q', '--nr_resnet', type=int, default=5,\n",
    "                    help='Number of residual blocks per stage of the model')\n",
    "parser.add_argument('-n', '--nr_filters', type=int, default=160,\n",
    "                    help='Number of filters to use across the model. Higher = larger model.')\n",
    "parser.add_argument('-m', '--nr_logistic_mix', type=int, default=10,\n",
    "                    help='Number of logistic components in the mixture. Higher = more flexible model')\n",
    "parser.add_argument('-l', '--lr', type=float,\n",
    "                    default=0.0002, help='Base learning rate')\n",
    "parser.add_argument('-e', '--lr_decay', type=float, default=(1 - 5e-6),\n",
    "                    help='Learning rate decay, applied every step of the optimization')\n",
    "parser.add_argument('-b', '--batch_size', type=int, default=16,\n",
    "                    help='Batch size during training per GPU')\n",
    "parser.add_argument('-x', '--max_epochs', type=int,\n",
    "                    default=5000, help='How many epochs to run in total?')\n",
    "parser.add_argument('-s', '--seed', type=int, default=1,\n",
    "                    help='Random seed to use')\n",
    "\n",
    "# nits model\n",
    "parser.add_argument('-a', '--nits_arch', type=list_str_to_list, default='[8,8,1]',\n",
    "                    help='Architecture of NITS model')\n",
    "parser.add_argument('-nb', '--nits_bound', type=float, default=5.,\n",
    "                    help='Upper and lower bound of NITS model')\n",
    "parser.add_argument('-c', '--constraint', type=str, default='neg_exp',\n",
    "                    help='Upper and lower bound of NITS model')\n",
    "parser.add_argument('-fc', '--final_constraint', type=str, default='softmax',\n",
    "                    help='Upper and lower bound of NITS model')\n",
    "\n",
    "\n",
    "args = parser.parse_args(['-a', '[16,10,1]', '-g', '4'])\n",
    "\n",
    "device = 'cuda:' + args.gpu if args.gpu else 'cpu'\n",
    "print('device:', device)\n",
    "\n",
    "# HOUSEKEEPING\n",
    "\n",
    "# reproducibility\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "model_name = 'lr_{:.5f}_nr_resnet{}_nr_filters{}_nits_arch{}_constraint{}_final_constraint{}'.format(\n",
    "    args.lr, args.nr_resnet, args.nr_filters, args.nits_arch, args.constraint, args.final_constraint)\n",
    "if os.path.exists(os.path.join('runs_test', model_name)):\n",
    "    shutil.rmtree(os.path.join('runs_test', model_name))\n",
    "\n",
    "sample_batch_size = 25\n",
    "obs = (1, 28, 28) if 'mnist' in args.dataset else (3, 32, 32)\n",
    "input_channels = obs[0]\n",
    "rescaling     = lambda x : (x - .5) * 2.\n",
    "rescaling_inv = lambda x : .5 * x  + .5\n",
    "kwargs = {'num_workers':1, 'pin_memory':True, 'drop_last':True}\n",
    "ds_transforms = transforms.Compose([transforms.ToTensor(), rescaling])\n",
    "\n",
    "if 'mnist' in args.dataset :\n",
    "    train_loader = torch.utils.data.DataLoader(datasets.MNIST(args.data_dir, download=True,\n",
    "                        train=True, transform=ds_transforms), batch_size=args.batch_size,\n",
    "                            shuffle=True, **kwargs)\n",
    "\n",
    "    test_loader  = torch.utils.data.DataLoader(datasets.MNIST(args.data_dir, train=False,\n",
    "                    transform=ds_transforms), batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "elif 'cifar' in args.dataset :\n",
    "    train_loader = torch.utils.data.DataLoader(datasets.CIFAR10(args.data_dir, train=True,\n",
    "        download=True, transform=ds_transforms), batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    test_loader  = torch.utils.data.DataLoader(datasets.CIFAR10(args.data_dir, train=False,\n",
    "                    transform=ds_transforms), batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "else :\n",
    "    raise Exception('{} dataset not in {mnist, cifar10}'.format(args.dataset))\n",
    "\n",
    "\n",
    "# INITIALIZE NITS MODEL\n",
    "if 'mnist' in args.dataset:\n",
    "    arch = [1] + args.nits_arch\n",
    "    nits_model = NITS(d=1, start=-args.nits_bound, end=args.nits_bound, monotonic_const=1e-5,\n",
    "                      A_constraint=args.constraint, arch=arch,\n",
    "                      final_layer_constraint=args.final_constraint).to(device)\n",
    "elif 'cifar' in args.dataset:\n",
    "    arch = [1] + args.nits_arch\n",
    "    nits_model = ConditionalNITS(d=3, start=-args.nits_bound, end=args.nits_bound, monotonic_const=1e-5,\n",
    "                                 A_constraint=args.constraint, arch=arch, autoregressive=True,\n",
    "                                 pixelrnn=True, normalize_inverse=True,\n",
    "                                 final_layer_constraint=args.final_constraint,\n",
    "                                 softmax_temperature=True).to(device)\n",
    "tot_params = nits_model.tot_params\n",
    "loss_op = lambda real, params: discretized_nits_loss(real, params, nits_model)\n",
    "sample_op = lambda params: nits_sample(params, nits_model)\n",
    "\n",
    "# INITIALIZE PIXELCNN MODEL\n",
    "model = PixelCNN(nr_resnet=args.nr_resnet, nr_filters=args.nr_filters,\n",
    "                 input_channels=input_channels, nr_logistic_mix=tot_params, num_mix=1)\n",
    "model = model.to(device)\n",
    "\n",
    "if args.load_params:\n",
    "    load_part_of_model(model, args.load_params)\n",
    "    print('model parameters loaded')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=args.lr_decay)\n",
    "test_losses = []\n",
    "\n",
    "def sample(model):\n",
    "    model.train(False)\n",
    "    with torch.no_grad():\n",
    "        data = torch.zeros(sample_batch_size, obs[0], obs[1], obs[2])\n",
    "        data = data.to(device)\n",
    "        for i in range(obs[1]):\n",
    "            for j in range(obs[2]):\n",
    "                data_v = Variable(data)\n",
    "                out   = model(data_v, sample=True)\n",
    "                out_sample = sample_op(out)\n",
    "                data[:, :, i, j] = out_sample.data[:, :, i, j]\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n",
      "loss : 3.0668, time : 28.3734\n",
      "loss : 3.0410, time : 28.3665\n",
      "loss : 3.0675, time : 28.4162\n",
      "loss : 3.0764, time : 28.4139\n",
      "loss : 3.0381, time : 28.4378\n",
      "loss : 3.0558, time : 28.5071\n",
      "loss : 3.1054, time : 28.4681\n",
      "loss : 3.1033, time : 28.4500\n",
      "loss : 3.1288, time : 28.4636\n",
      "loss : 3.0957, time : 28.5035\n",
      "loss : 3.1207, time : 28.5214\n",
      "loss : 3.0875, time : 28.4775\n",
      "loss : 3.0969, time : 28.4749\n",
      "loss : 3.0554, time : 28.5275\n",
      "loss : 3.0987, time : 28.4911\n",
      "loss : 3.1119, time : 28.4661\n",
      "loss : 3.0588, time : 28.4763\n",
      "loss : 3.0877, time : 28.4838\n",
      "loss : 3.0757, time : 28.4802\n",
      "loss : 3.1376, time : 28.4779\n",
      "loss : 3.1525, time : 28.4722\n",
      "loss : 3.1153, time : 28.4694\n",
      "loss : 3.0585, time : 28.4835\n",
      "loss : 3.0988, time : 28.4736\n",
      "loss : 3.0847, time : 28.4710\n",
      "loss : 3.0616, time : 28.5035\n",
      "loss : 3.1251, time : 28.4637\n",
      "loss : 3.1265, time : 28.4621\n",
      "loss : 3.1016, time : 28.4645\n",
      "loss : 3.0890, time : 28.4701\n",
      "loss : 3.0740, time : 28.5466\n",
      "loss : 3.1018, time : 28.4863\n",
      "loss : 3.0500, time : 28.4662\n",
      "loss : 3.0700, time : 28.4736\n",
      "loss : 3.1133, time : 28.4745\n",
      "loss : 3.0816, time : 28.4682\n",
      "loss : 3.1040, time : 28.4970\n",
      "loss : 3.1012, time : 28.5242\n",
      "loss : 3.0771, time : 28.4915\n",
      "loss : 3.1083, time : 28.5015\n",
      "loss : 3.0367, time : 28.5097\n",
      "loss : 3.0651, time : 28.4790\n",
      "loss : 3.1056, time : 28.4824\n",
      "loss : 3.0954, time : 28.4800\n",
      "loss : 3.0750, time : 28.4791\n",
      "loss : 3.0701, time : 28.4706\n",
      "loss : 3.1055, time : 28.4646\n",
      "loss : 3.1143, time : 28.4781\n",
      "loss : 3.0794, time : 28.4770\n",
      "loss : 3.0676, time : 28.5058\n",
      "loss : 3.0793, time : 28.4705\n",
      "loss : 3.1102, time : 28.4918\n",
      "loss : 3.1049, time : 28.4897\n",
      "loss : 3.1183, time : 28.4643\n",
      "loss : 3.0315, time : 28.4764\n",
      "loss : 3.0926, time : 28.4824\n",
      "loss : 3.1295, time : 28.4551\n",
      "loss : 3.0782, time : 28.4537\n",
      "test loss : 3.045336, lr : 1.999490e-04\n",
      "loss : 3.0472, time : 28.6430\n",
      "loss : 3.1042, time : 28.4950\n",
      "loss : 3.0888, time : 28.5308\n",
      "loss : 3.1506, time : 28.4904\n",
      "loss : 3.1005, time : 28.4900\n",
      "loss : 3.0575, time : 28.4782\n",
      "loss : 3.0938, time : 28.4814\n",
      "loss : 3.0826, time : 28.4101\n",
      "loss : 3.0918, time : 28.3866\n",
      "loss : 3.0896, time : 28.3822\n",
      "loss : 3.1162, time : 28.4336\n",
      "loss : 3.1009, time : 28.4078\n",
      "loss : 3.1257, time : 28.4606\n",
      "loss : 3.0940, time : 28.4536\n",
      "loss : 3.0687, time : 28.4856\n",
      "loss : 3.0593, time : 28.4423\n",
      "loss : 3.0883, time : 28.4372\n",
      "loss : 3.0946, time : 28.4477\n",
      "loss : 3.0861, time : 28.4478\n",
      "loss : 3.0662, time : 28.4537\n",
      "loss : 3.1141, time : 28.4540\n",
      "loss : 3.0922, time : 28.4541\n",
      "loss : 3.0597, time : 28.4498\n",
      "loss : 3.0578, time : 28.4584\n",
      "loss : 3.0939, time : 28.4646\n",
      "loss : 3.0593, time : 28.4399\n",
      "loss : 3.0702, time : 28.4653\n",
      "loss : 3.0515, time : 28.4278\n",
      "loss : 3.0806, time : 28.4588\n",
      "loss : 3.0675, time : 28.5290\n",
      "loss : 3.0706, time : 28.4655\n",
      "loss : 3.0410, time : 28.4493\n",
      "loss : 3.1094, time : 28.4488\n",
      "loss : 3.0482, time : 28.4448\n",
      "loss : 3.1117, time : 28.4356\n",
      "loss : 3.0960, time : 28.4461\n",
      "loss : 3.1098, time : 28.4393\n",
      "loss : 3.0644, time : 28.4374\n",
      "loss : 3.1253, time : 28.4816\n",
      "loss : 3.0406, time : 28.4453\n",
      "loss : 3.0762, time : 28.4896\n",
      "loss : 3.0906, time : 28.4406\n",
      "loss : 3.0882, time : 28.4486\n",
      "loss : 3.0701, time : 28.4354\n",
      "loss : 3.1049, time : 28.5660\n",
      "loss : 3.0281, time : 28.4534\n",
      "loss : 3.1588, time : 28.4489\n",
      "loss : 3.0964, time : 28.4818\n",
      "loss : 3.1228, time : 28.4630\n",
      "loss : 3.1243, time : 28.4639\n",
      "loss : 3.0594, time : 28.5260\n",
      "loss : 3.0974, time : 28.4795\n",
      "loss : 3.1290, time : 28.4584\n",
      "loss : 3.1184, time : 28.4633\n",
      "loss : 3.0693, time : 28.4804\n",
      "loss : 3.0473, time : 28.4799\n",
      "loss : 3.0947, time : 28.4876\n",
      "loss : 3.0793, time : 28.4776\n",
      "loss : 3.0937, time : 28.4687\n",
      "loss : 3.0805, time : 28.4870\n",
      "loss : 3.0669, time : 28.5142\n",
      "loss : 3.0582, time : 28.4967\n",
      "test loss : 3.049679, lr : 1.999480e-04\n",
      "loss : 3.1418, time : 28.6594\n",
      "loss : 3.0926, time : 28.4922\n",
      "loss : 3.0981, time : 28.4701\n",
      "loss : 3.0987, time : 28.4885\n",
      "loss : 3.1164, time : 28.4660\n",
      "loss : 3.1552, time : 28.4586\n",
      "loss : 3.1080, time : 28.4346\n",
      "loss : 3.1073, time : 28.4874\n",
      "loss : 3.0839, time : 28.4315\n",
      "loss : 3.0917, time : 28.4487\n",
      "loss : 3.0707, time : 28.4424\n",
      "loss : 3.0775, time : 28.5308\n",
      "loss : 3.1171, time : 28.4229\n",
      "loss : 3.0923, time : 28.4299\n",
      "loss : 3.0740, time : 28.4421\n",
      "loss : 3.0961, time : 28.4440\n",
      "loss : 3.0947, time : 28.4576\n",
      "loss : 3.0906, time : 28.4397\n",
      "loss : 3.0721, time : 28.4357\n",
      "loss : 3.0856, time : 28.4380\n",
      "loss : 3.0977, time : 28.4671\n",
      "loss : 3.0789, time : 28.4564\n",
      "loss : 3.0598, time : 28.4312\n",
      "loss : 3.0801, time : 28.4784\n",
      "loss : 3.1045, time : 28.4323\n",
      "loss : 3.0275, time : 28.4567\n",
      "loss : 3.0763, time : 28.4552\n",
      "loss : 3.1051, time : 28.4402\n",
      "loss : 3.0538, time : 28.4673\n",
      "loss : 3.1172, time : 28.4684\n",
      "loss : 3.0719, time : 28.4606\n",
      "loss : 3.0743, time : 28.4714\n",
      "loss : 3.0442, time : 28.4670\n",
      "loss : 3.0742, time : 28.4532\n",
      "loss : 3.0873, time : 28.4391\n",
      "loss : 3.0798, time : 28.4758\n",
      "loss : 3.1511, time : 28.4723\n",
      "loss : 3.0908, time : 28.4549\n",
      "loss : 3.0630, time : 28.4557\n",
      "loss : 3.0402, time : 28.4759\n",
      "loss : 3.0154, time : 28.4533\n",
      "loss : 3.0973, time : 28.4510\n",
      "loss : 3.0821, time : 28.4540\n",
      "loss : 3.0903, time : 28.4533\n",
      "loss : 3.0412, time : 28.4460\n",
      "loss : 3.0653, time : 28.4651\n",
      "loss : 3.0604, time : 28.4431\n",
      "loss : 3.1184, time : 28.5035\n",
      "loss : 3.0731, time : 28.4715\n",
      "loss : 3.0617, time : 28.4606\n",
      "loss : 3.0706, time : 28.4534\n",
      "loss : 3.0888, time : 28.4671\n",
      "loss : 3.1021, time : 28.4705\n",
      "loss : 3.0840, time : 28.4888\n",
      "loss : 3.0689, time : 28.4519\n",
      "loss : 3.1083, time : 28.4472\n",
      "loss : 3.1628, time : 28.4564\n",
      "loss : 3.1588, time : 28.4560\n",
      "loss : 3.1381, time : 28.4796\n",
      "loss : 3.0739, time : 28.4942\n",
      "loss : 3.0866, time : 28.4654\n",
      "loss : 3.0494, time : 28.4729\n",
      "test loss : 3.042366, lr : 1.999470e-04\n",
      "loss : 3.1037, time : 28.6339\n",
      "loss : 3.0629, time : 28.4854\n",
      "loss : 3.0521, time : 28.4841\n",
      "loss : 3.0986, time : 28.5187\n",
      "loss : 3.0646, time : 28.4945\n",
      "loss : 3.0859, time : 28.4680\n",
      "loss : 3.1007, time : 28.4434\n",
      "loss : 3.0490, time : 28.4587\n",
      "loss : 3.0714, time : 28.4991\n",
      "loss : 3.0981, time : 28.4687\n",
      "loss : 3.0804, time : 28.4771\n",
      "loss : 3.0658, time : 28.4540\n",
      "loss : 3.0634, time : 28.4536\n",
      "loss : 3.0978, time : 28.4678\n",
      "loss : 3.1058, time : 28.4461\n",
      "loss : 3.1049, time : 28.4376\n",
      "loss : 3.0957, time : 28.4430\n",
      "loss : 3.0719, time : 28.4379\n",
      "loss : 3.1726, time : 28.4599\n",
      "loss : 3.0620, time : 28.4857\n",
      "loss : 3.0441, time : 28.4937\n",
      "loss : 3.0738, time : 28.4671\n",
      "loss : 3.1065, time : 28.4608\n",
      "loss : 3.0759, time : 28.4782\n",
      "loss : 3.0623, time : 28.4849\n",
      "loss : 3.0754, time : 28.5167\n",
      "loss : 3.0966, time : 28.4779\n",
      "loss : 3.0288, time : 28.4832\n",
      "loss : 3.0973, time : 28.4974\n",
      "loss : 3.0774, time : 28.4952\n",
      "loss : 3.0565, time : 28.4725\n",
      "loss : 3.0600, time : 28.4775\n",
      "loss : 3.1150, time : 28.5095\n",
      "loss : 3.1104, time : 28.4757\n",
      "loss : 3.0635, time : 28.4648\n",
      "loss : 3.0590, time : 28.4689\n",
      "loss : 3.0729, time : 28.4530\n",
      "loss : 3.0777, time : 28.4588\n",
      "loss : 3.1648, time : 28.4671\n",
      "loss : 3.0803, time : 28.4538\n",
      "loss : 3.0698, time : 28.4447\n",
      "loss : 3.0733, time : 28.4489\n",
      "loss : 3.0623, time : 28.4497\n",
      "loss : 3.0449, time : 28.4513\n",
      "loss : 3.0626, time : 28.4795\n",
      "loss : 3.1051, time : 28.4459\n",
      "loss : 3.0750, time : 28.4555\n",
      "loss : 3.0925, time : 28.4462\n",
      "loss : 3.1034, time : 28.4331\n",
      "loss : 3.0517, time : 28.4422\n",
      "loss : 3.1117, time : 28.4347\n",
      "loss : 3.1179, time : 28.4259\n",
      "loss : 3.0992, time : 28.4351\n",
      "loss : 3.0507, time : 28.4546\n",
      "loss : 3.0950, time : 28.4380\n",
      "loss : 3.0871, time : 28.4504\n",
      "loss : 3.0587, time : 28.4838\n",
      "loss : 3.0559, time : 28.4468\n",
      "loss : 3.0715, time : 28.4532\n",
      "loss : 3.0609, time : 28.4616\n",
      "loss : 3.0506, time : 28.4304\n",
      "loss : 3.0902, time : 28.4373\n",
      "test loss : 3.036586, lr : 1.999460e-04\n",
      "loss : 3.0659, time : 28.6196\n",
      "loss : 3.0852, time : 28.4712\n",
      "loss : 3.0603, time : 28.4832\n",
      "loss : 3.1054, time : 28.4723\n",
      "loss : 3.0861, time : 28.4837\n",
      "loss : 3.1200, time : 28.5091\n",
      "loss : 3.0961, time : 28.4715\n",
      "loss : 3.1633, time : 28.4509\n",
      "loss : 3.1143, time : 28.4607\n",
      "loss : 3.1129, time : 28.4414\n",
      "loss : 3.1128, time : 28.4371\n",
      "loss : 3.0922, time : 28.4617\n",
      "loss : 3.0630, time : 28.4527\n",
      "loss : 3.0977, time : 28.4621\n",
      "loss : 3.0959, time : 28.4736\n",
      "loss : 3.0920, time : 28.4505\n",
      "loss : 3.0919, time : 28.4523\n",
      "loss : 3.0523, time : 28.4899\n",
      "loss : 3.0877, time : 28.4455\n",
      "loss : 3.0810, time : 28.4454\n",
      "loss : 3.0786, time : 28.4545\n",
      "loss : 3.0746, time : 28.4657\n",
      "loss : 3.0482, time : 28.4520\n",
      "loss : 3.0667, time : 28.4633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 3.0695, time : 28.4736\n",
      "loss : 3.0755, time : 28.4810\n",
      "loss : 3.0455, time : 28.4700\n",
      "loss : 3.0915, time : 28.4845\n",
      "loss : 3.1149, time : 28.4436\n",
      "loss : 3.1332, time : 28.4608\n",
      "loss : 3.0566, time : 28.4380\n",
      "loss : 3.0387, time : 28.4376\n",
      "loss : 3.1125, time : 28.4677\n",
      "loss : 3.0939, time : 28.4634\n",
      "loss : 3.0283, time : 28.4509\n",
      "loss : 3.0970, time : 28.4662\n",
      "loss : 3.0796, time : 28.4497\n",
      "loss : 3.1181, time : 28.4738\n",
      "loss : 3.0792, time : 28.4583\n",
      "loss : 3.0585, time : 28.4550\n",
      "loss : 3.0974, time : 28.4406\n",
      "loss : 3.0786, time : 28.4784\n",
      "loss : 3.0964, time : 28.4677\n",
      "loss : 3.0538, time : 28.4589\n",
      "loss : 3.0646, time : 28.4865\n",
      "loss : 3.0945, time : 28.4842\n",
      "loss : 3.0470, time : 28.4695\n",
      "loss : 3.0807, time : 28.4701\n",
      "loss : 3.0618, time : 28.4543\n",
      "loss : 3.0230, time : 28.4189\n",
      "loss : 3.1016, time : 28.4386\n",
      "loss : 3.0765, time : 28.4472\n",
      "loss : 3.0440, time : 28.4571\n",
      "loss : 3.0589, time : 28.4876\n",
      "loss : 3.0749, time : 28.4529\n",
      "loss : 3.0566, time : 28.4603\n",
      "loss : 3.0324, time : 28.4511\n",
      "loss : 3.0634, time : 28.4534\n",
      "loss : 3.1221, time : 28.4612\n",
      "loss : 3.0797, time : 28.4837\n",
      "loss : 3.0662, time : 28.4451\n",
      "loss : 3.1547, time : 28.4529\n",
      "test loss : 3.049337, lr : 1.999450e-04\n",
      "loss : 3.1409, time : 28.6517\n",
      "loss : 3.0774, time : 28.4815\n",
      "loss : 3.0413, time : 28.5042\n",
      "loss : 3.0308, time : 28.4589\n",
      "loss : 3.0298, time : 28.4601\n",
      "loss : 3.1158, time : 28.4515\n",
      "loss : 3.1160, time : 28.4457\n",
      "loss : 3.0523, time : 28.4431\n",
      "loss : 3.0904, time : 28.4557\n",
      "loss : 3.0682, time : 28.4500\n",
      "loss : 3.0424, time : 28.4609\n",
      "loss : 3.0559, time : 28.4697\n",
      "loss : 3.0420, time : 28.4726\n",
      "loss : 3.0675, time : 28.4554\n",
      "loss : 3.0787, time : 28.5021\n",
      "loss : 3.1446, time : 28.4499\n",
      "loss : 3.1138, time : 28.4659\n",
      "loss : 3.0464, time : 28.4782\n",
      "loss : 3.0846, time : 28.4417\n",
      "loss : 3.0441, time : 28.4687\n",
      "loss : 3.0660, time : 28.4632\n",
      "loss : 3.1510, time : 28.4541\n",
      "loss : 3.0663, time : 28.4582\n",
      "loss : 3.0781, time : 28.4481\n",
      "loss : 3.1421, time : 28.4494\n",
      "loss : 3.1374, time : 28.4570\n",
      "loss : 3.1062, time : 28.4771\n",
      "loss : 3.0740, time : 28.4502\n",
      "loss : 3.0817, time : 28.4359\n",
      "loss : 3.1207, time : 28.4477\n",
      "loss : 3.0546, time : 28.4719\n",
      "loss : 3.0568, time : 28.4560\n",
      "loss : 3.0594, time : 28.4511\n",
      "loss : 3.0863, time : 28.4505\n",
      "loss : 3.0304, time : 28.4669\n",
      "loss : 3.0874, time : 28.4724\n",
      "loss : 2.9993, time : 28.4743\n",
      "loss : 3.0633, time : 28.4622\n",
      "loss : 3.0524, time : 28.4842\n",
      "loss : 3.0778, time : 28.4698\n",
      "loss : 3.0792, time : 28.4766\n",
      "loss : 3.0852, time : 28.5408\n",
      "loss : 3.1003, time : 28.4679\n",
      "loss : 3.0465, time : 28.4632\n",
      "loss : 3.0716, time : 28.4540\n",
      "loss : 3.1299, time : 28.4697\n",
      "loss : 3.0836, time : 28.4632\n",
      "loss : 3.0804, time : 28.4563\n",
      "loss : 3.0545, time : 28.4555\n",
      "loss : 3.0718, time : 28.4881\n",
      "loss : 3.0569, time : 28.5061\n",
      "loss : 3.1257, time : 28.4637\n",
      "loss : 3.0961, time : 28.4725\n",
      "loss : 3.0771, time : 28.4730\n",
      "loss : 3.0842, time : 28.4675\n",
      "loss : 3.0596, time : 28.4681\n",
      "loss : 3.0655, time : 28.4600\n",
      "loss : 3.1007, time : 28.4681\n",
      "loss : 3.1374, time : 28.4510\n",
      "loss : 3.0756, time : 28.4583\n",
      "loss : 3.0795, time : 28.4631\n",
      "loss : 3.1274, time : 28.4709\n",
      "test loss : 3.262874, lr : 1.999440e-04\n",
      "loss : 3.1306, time : 28.7986\n",
      "loss : 3.0846, time : 28.5255\n",
      "loss : 3.0804, time : 28.5008\n",
      "loss : 3.1119, time : 28.4790\n",
      "loss : 3.0607, time : 28.4939\n",
      "loss : 3.0803, time : 28.4737\n",
      "loss : 3.0596, time : 28.4869\n",
      "loss : 3.1039, time : 28.4459\n",
      "loss : 3.0763, time : 28.4579\n",
      "loss : 3.0809, time : 28.4527\n",
      "loss : 3.0713, time : 28.4421\n",
      "loss : 3.0454, time : 28.4588\n",
      "loss : 3.0878, time : 28.4988\n",
      "loss : 3.0676, time : 28.4429\n",
      "loss : 3.1015, time : 28.4370\n",
      "loss : 3.0510, time : 28.4434\n",
      "loss : 3.0479, time : 28.4650\n",
      "loss : 2.9811, time : 28.4569\n",
      "loss : 3.1020, time : 28.4542\n",
      "loss : 3.0487, time : 28.4582\n",
      "loss : 3.0352, time : 28.4691\n",
      "loss : 3.0768, time : 28.4455\n",
      "loss : 3.0396, time : 28.4740\n",
      "loss : 3.0680, time : 28.5311\n",
      "loss : 3.1399, time : 28.4999\n",
      "loss : 3.1180, time : 28.4584\n",
      "loss : 3.0803, time : 28.4736\n",
      "loss : 3.0794, time : 28.5086\n",
      "loss : 3.1166, time : 28.4660\n",
      "loss : 3.0405, time : 28.4956\n",
      "loss : 3.0549, time : 28.4697\n",
      "loss : 3.0821, time : 28.4672\n",
      "loss : 3.0618, time : 28.4738\n",
      "loss : 3.0887, time : 28.4641\n",
      "loss : 3.0750, time : 28.4731\n",
      "loss : 3.1105, time : 28.4670\n",
      "loss : 3.0691, time : 28.4950\n",
      "loss : 3.0713, time : 28.4648\n",
      "loss : 3.0922, time : 28.4763\n",
      "loss : 3.1245, time : 28.4664\n",
      "loss : 3.0535, time : 28.4471\n",
      "loss : 3.1197, time : 28.4584\n",
      "loss : 3.1027, time : 28.4602\n",
      "loss : 3.0778, time : 28.4732\n",
      "loss : 3.0794, time : 28.4738\n",
      "loss : 3.0918, time : 28.4653\n",
      "loss : 3.0653, time : 28.4552\n",
      "loss : 3.1020, time : 28.4790\n",
      "loss : 3.0403, time : 28.4942\n",
      "loss : 3.0890, time : 28.4749\n",
      "loss : 3.0761, time : 28.4465\n",
      "loss : 3.0795, time : 28.4690\n",
      "loss : 3.0634, time : 28.4579\n",
      "loss : 3.0671, time : 28.4639\n",
      "loss : 3.0813, time : 28.4567\n",
      "loss : 3.1150, time : 28.4528\n",
      "loss : 3.0845, time : 28.4368\n",
      "loss : 3.0382, time : 28.4428\n",
      "loss : 3.0624, time : 28.4374\n",
      "loss : 3.0788, time : 28.4569\n",
      "loss : 3.0803, time : 28.5226\n",
      "loss : 3.0596, time : 28.4629\n",
      "test loss : 3.050583, lr : 1.999430e-04\n",
      "loss : 3.0460, time : 28.7015\n",
      "loss : 3.0639, time : 28.4870\n",
      "loss : 3.1092, time : 28.4639\n",
      "loss : 3.1081, time : 28.4562\n",
      "loss : 3.0806, time : 28.4727\n",
      "loss : 3.0962, time : 28.4566\n",
      "loss : 3.1422, time : 28.4653\n",
      "loss : 3.0602, time : 28.4558\n",
      "loss : 3.0354, time : 28.4504\n",
      "loss : 3.0165, time : 28.5320\n",
      "loss : 3.1196, time : 28.4706\n",
      "loss : 3.1299, time : 28.4452\n",
      "loss : 3.0758, time : 28.4437\n",
      "loss : 3.1040, time : 28.4538\n",
      "loss : 3.1928, time : 28.4613\n",
      "loss : 3.0635, time : 28.4484\n",
      "loss : 3.0327, time : 28.4401\n",
      "loss : 3.0506, time : 28.4442\n",
      "loss : 3.0846, time : 28.4503\n",
      "loss : 3.0621, time : 28.4521\n",
      "loss : 3.0678, time : 28.4371\n",
      "loss : 3.0839, time : 28.4952\n",
      "loss : 3.0767, time : 28.4839\n",
      "loss : 3.0615, time : 28.4239\n",
      "loss : 3.0520, time : 28.4403\n",
      "loss : 3.0531, time : 28.4547\n",
      "loss : 3.0644, time : 28.4384\n",
      "loss : 3.0501, time : 28.4368\n",
      "loss : 3.0635, time : 28.4567\n",
      "loss : 3.0879, time : 28.4724\n",
      "loss : 3.0499, time : 28.4739\n",
      "loss : 3.0683, time : 28.4783\n",
      "loss : 3.0677, time : 28.4450\n",
      "loss : 3.0534, time : 28.4874\n",
      "loss : 3.0673, time : 28.4386\n",
      "loss : 3.0826, time : 28.4678\n",
      "loss : 3.1094, time : 28.4575\n",
      "loss : 3.0684, time : 28.4108\n",
      "loss : 3.0869, time : 28.4181\n",
      "loss : 3.0941, time : 28.4413\n",
      "loss : 3.0434, time : 28.5340\n",
      "loss : 3.0898, time : 28.4747\n",
      "loss : 3.0408, time : 28.4333\n",
      "loss : 3.0704, time : 28.4367\n",
      "loss : 3.0721, time : 28.4431\n",
      "loss : 3.0444, time : 28.4841\n",
      "loss : 3.1034, time : 28.4797\n",
      "loss : 3.0452, time : 28.4822\n",
      "loss : 3.0929, time : 28.4819\n",
      "loss : 3.0741, time : 28.4628\n",
      "loss : 3.0353, time : 28.4463\n",
      "loss : 3.1441, time : 28.4627\n",
      "loss : 3.1019, time : 28.4740\n",
      "loss : 3.0847, time : 28.4641\n",
      "loss : 3.0599, time : 28.4676\n",
      "loss : 3.0980, time : 28.4669\n",
      "loss : 3.0561, time : 28.4870\n",
      "loss : 3.0479, time : 28.5039\n",
      "loss : 3.1062, time : 28.4774\n",
      "loss : 3.0807, time : 28.4827\n",
      "loss : 3.0633, time : 28.4815\n",
      "loss : 3.0609, time : 28.4762\n",
      "test loss : 3.057501, lr : 1.999420e-04\n",
      "loss : 3.0810, time : 28.6508\n",
      "loss : 3.0926, time : 28.5134\n",
      "loss : 3.0955, time : 28.4965\n",
      "loss : 3.0958, time : 28.4737\n",
      "loss : 3.0673, time : 28.4669\n",
      "loss : 3.0633, time : 28.4607\n",
      "loss : 3.0540, time : 28.5006\n",
      "loss : 3.0932, time : 28.4466\n",
      "loss : 3.0657, time : 28.4500\n",
      "loss : 3.0823, time : 28.4771\n",
      "loss : 3.0344, time : 28.4600\n",
      "loss : 3.0533, time : 28.4595\n",
      "loss : 3.0545, time : 28.4688\n",
      "loss : 3.1256, time : 28.4655\n",
      "loss : 3.0960, time : 28.4696\n",
      "loss : 3.0399, time : 28.4585\n",
      "loss : 3.0887, time : 28.4648\n",
      "loss : 3.0693, time : 28.4498\n",
      "loss : 3.0762, time : 28.4876\n",
      "loss : 3.0739, time : 28.4665\n",
      "loss : 3.0592, time : 28.4536\n",
      "loss : 3.0347, time : 28.4592\n",
      "loss : 3.0681, time : 28.4620\n",
      "loss : 3.0822, time : 28.4522\n",
      "loss : 3.0861, time : 28.4661\n",
      "loss : 3.0581, time : 28.4429\n",
      "loss : 3.0470, time : 28.4500\n",
      "loss : 3.0638, time : 28.4325\n",
      "loss : 3.1011, time : 28.4262\n",
      "loss : 3.0818, time : 28.4432\n",
      "loss : 3.0472, time : 28.4594\n",
      "loss : 3.0904, time : 28.4553\n",
      "loss : 3.1126, time : 28.4598\n",
      "loss : 3.1134, time : 28.4514\n",
      "loss : 3.0528, time : 28.4557\n",
      "loss : 3.0728, time : 28.4600\n",
      "loss : 3.0304, time : 28.4909\n",
      "loss : 3.0455, time : 28.4751\n",
      "loss : 3.0850, time : 28.4713\n",
      "loss : 3.0505, time : 28.4630\n",
      "loss : 3.0663, time : 28.4752\n",
      "loss : 3.0647, time : 28.4478\n",
      "loss : 3.0975, time : 28.4866\n",
      "loss : 3.1110, time : 28.4443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 3.0655, time : 28.4490\n",
      "loss : 3.0493, time : 28.4448\n",
      "loss : 3.0723, time : 28.4341\n",
      "loss : 3.0636, time : 28.4606\n",
      "loss : 3.1048, time : 28.4580\n",
      "loss : 3.0993, time : 28.4562\n",
      "loss : 3.0683, time : 28.4564\n",
      "loss : 3.0781, time : 28.4356\n",
      "loss : 3.0757, time : 28.4408\n",
      "loss : 3.0745, time : 28.4481\n",
      "loss : 3.0999, time : 28.4898\n",
      "loss : 3.1161, time : 28.4586\n",
      "loss : 3.0619, time : 28.4618\n",
      "loss : 3.1318, time : 28.4785\n",
      "loss : 3.0613, time : 28.4643\n",
      "loss : 3.0242, time : 28.4744\n",
      "loss : 3.0766, time : 28.4736\n",
      "loss : 3.1054, time : 28.4459\n",
      "test loss : 3.068322, lr : 1.999410e-04\n",
      "loss : 3.0840, time : 28.6445\n",
      "loss : 3.0597, time : 28.4409\n",
      "loss : 3.0983, time : 28.3828\n",
      "loss : 3.0824, time : 28.4237\n",
      "loss : 3.1313, time : 28.3933\n",
      "loss : 3.1527, time : 28.3772\n",
      "loss : 3.0834, time : 28.3575\n",
      "loss : 3.0864, time : 28.3694\n",
      "loss : 3.0599, time : 28.3792\n",
      "loss : 3.1227, time : 28.4285\n",
      "loss : 3.0882, time : 28.3764\n",
      "loss : 3.0563, time : 28.3574\n",
      "loss : 3.0782, time : 28.4354\n",
      "loss : 3.0772, time : 28.4888\n",
      "loss : 3.0269, time : 28.5090\n",
      "loss : 3.0378, time : 28.7814\n",
      "loss : 3.1037, time : 28.5827\n",
      "loss : 3.0316, time : 28.5206\n",
      "loss : 3.1138, time : 28.5006\n",
      "loss : 3.0510, time : 28.5032\n",
      "loss : 3.0774, time : 28.5099\n",
      "loss : 3.0289, time : 28.5031\n",
      "loss : 3.1022, time : 28.4883\n",
      "loss : 3.0501, time : 28.5088\n",
      "loss : 3.0775, time : 28.4982\n",
      "loss : 3.0851, time : 28.4905\n",
      "loss : 3.1523, time : 28.5010\n",
      "loss : 3.1083, time : 28.5309\n",
      "loss : 3.0585, time : 28.5054\n",
      "loss : 3.0481, time : 28.4760\n",
      "loss : 3.0444, time : 28.4972\n",
      "loss : 3.0542, time : 28.4773\n",
      "loss : 3.0246, time : 28.4672\n",
      "loss : 3.0751, time : 28.4668\n",
      "loss : 3.0674, time : 28.5008\n",
      "loss : 3.0820, time : 28.4897\n",
      "loss : 3.0616, time : 28.4909\n",
      "loss : 3.0787, time : 28.4748\n",
      "loss : 3.0821, time : 28.4866\n",
      "loss : 3.0537, time : 28.5235\n",
      "loss : 3.0716, time : 28.6383\n",
      "loss : 3.0460, time : 28.5048\n",
      "loss : 3.0993, time : 28.4905\n",
      "loss : 3.0439, time : 28.5041\n",
      "loss : 3.0650, time : 28.4878\n",
      "loss : 3.0568, time : 28.4925\n",
      "loss : 3.0038, time : 28.4906\n",
      "loss : 3.0522, time : 28.5156\n",
      "loss : 3.0891, time : 28.4957\n",
      "loss : 3.0899, time : 28.4867\n",
      "loss : 3.0692, time : 28.4836\n",
      "loss : 3.0346, time : 28.5380\n",
      "loss : 3.0329, time : 28.4728\n",
      "loss : 3.0657, time : 28.4845\n",
      "loss : 3.0670, time : 28.4861\n",
      "loss : 3.1248, time : 28.4934\n",
      "loss : 3.1126, time : 28.4905\n",
      "loss : 3.0741, time : 28.4821\n",
      "loss : 3.0575, time : 28.4732\n",
      "loss : 3.1161, time : 28.4864\n",
      "loss : 3.1176, time : 28.4726\n",
      "loss : 3.0712, time : 28.4746\n",
      "test loss : 3.093625, lr : 1.999400e-04\n",
      "sampling...\n",
      "loss : 3.0804, time : 28.7082\n",
      "loss : 3.0650, time : 28.4925\n",
      "loss : 3.0180, time : 28.4915\n",
      "loss : 3.0876, time : 28.4636\n",
      "loss : 3.0832, time : 28.5037\n",
      "loss : 3.1380, time : 28.4978\n",
      "loss : 3.0613, time : 28.6749\n",
      "loss : 3.0336, time : 28.4764\n",
      "loss : 3.0622, time : 28.4921\n",
      "loss : 3.0854, time : 28.4752\n",
      "loss : 3.0871, time : 28.4707\n",
      "loss : 3.0790, time : 28.4966\n",
      "loss : 3.0673, time : 28.5144\n",
      "loss : 3.0764, time : 28.4513\n",
      "loss : 3.1004, time : 28.4541\n",
      "loss : 3.0789, time : 28.4780\n",
      "loss : 2.9999, time : 28.4895\n",
      "loss : 3.0483, time : 28.4979\n",
      "loss : 3.0463, time : 28.4891\n",
      "loss : 3.0750, time : 28.4888\n",
      "loss : 3.1011, time : 28.4782\n",
      "loss : 3.0528, time : 28.4569\n",
      "loss : 3.0289, time : 28.4730\n",
      "loss : 3.0615, time : 28.4565\n",
      "loss : 3.0162, time : 28.4940\n",
      "loss : 3.1145, time : 28.4675\n",
      "loss : 3.0836, time : 28.4774\n",
      "loss : 3.0497, time : 28.5343\n",
      "loss : 3.0497, time : 28.5023\n",
      "loss : 3.0194, time : 28.4987\n",
      "loss : 3.0580, time : 28.5086\n",
      "loss : 3.0629, time : 28.5073\n",
      "loss : 3.1012, time : 28.5181\n",
      "loss : 3.0836, time : 28.5304\n",
      "loss : 3.0486, time : 28.5307\n",
      "loss : 3.1406, time : 28.5128\n",
      "loss : 3.1194, time : 28.5420\n",
      "loss : 3.1027, time : 28.5278\n",
      "loss : 3.0606, time : 28.5177\n",
      "loss : 3.0784, time : 28.5092\n",
      "loss : 3.0830, time : 28.5313\n",
      "loss : 3.0369, time : 28.5060\n",
      "loss : 3.1063, time : 28.4985\n",
      "loss : 3.0858, time : 28.4942\n",
      "loss : 3.0395, time : 28.4930\n",
      "loss : 3.0703, time : 28.5225\n",
      "loss : 3.0267, time : 28.5562\n",
      "loss : 3.1075, time : 28.4926\n",
      "loss : 3.0586, time : 28.5118\n",
      "loss : 3.0438, time : 28.4966\n",
      "loss : 3.0733, time : 28.4901\n",
      "loss : 3.0779, time : 28.4873\n",
      "loss : 3.0817, time : 28.4824\n",
      "loss : 3.0897, time : 28.4687\n",
      "loss : 3.0916, time : 28.4833\n",
      "loss : 3.1359, time : 28.4597\n",
      "loss : 3.0779, time : 28.4701\n",
      "loss : 3.1139, time : 28.4741\n",
      "loss : 3.0714, time : 28.4617\n",
      "loss : 3.0998, time : 28.4732\n",
      "loss : 3.0522, time : 28.4903\n",
      "loss : 3.0616, time : 28.4547\n",
      "test loss : 3.028081, lr : 1.999390e-04\n",
      "loss : 3.0037, time : 28.6764\n",
      "loss : 3.0792, time : 28.5057\n",
      "loss : 3.0425, time : 28.4830\n",
      "loss : 3.0823, time : 28.4821\n",
      "loss : 3.0528, time : 28.4815\n",
      "loss : 3.0678, time : 28.4672\n",
      "loss : 3.0627, time : 28.4680\n",
      "loss : 3.0638, time : 28.4752\n",
      "loss : 3.0782, time : 28.4783\n",
      "loss : 3.0572, time : 28.5123\n",
      "loss : 3.0746, time : 28.4767\n",
      "loss : 3.0694, time : 28.4924\n",
      "loss : 3.0897, time : 28.4646\n",
      "loss : 3.0755, time : 28.4748\n",
      "loss : 3.0480, time : 28.5008\n",
      "loss : 3.0517, time : 28.5149\n",
      "loss : 3.0782, time : 28.4922\n",
      "loss : 3.0662, time : 28.4787\n",
      "loss : 3.0584, time : 28.4829\n",
      "loss : 3.1083, time : 28.4678\n",
      "loss : 3.0593, time : 28.4815\n",
      "loss : 3.0477, time : 28.5184\n",
      "loss : 3.0952, time : 28.4830\n",
      "loss : 3.0501, time : 28.4841\n",
      "loss : 3.0605, time : 28.4941\n",
      "loss : 3.0794, time : 28.4975\n",
      "loss : 3.0675, time : 28.4842\n",
      "loss : 3.0973, time : 28.4895\n",
      "loss : 3.0639, time : 28.4941\n",
      "loss : 3.0565, time : 28.4844\n",
      "loss : 3.0461, time : 28.4873\n",
      "loss : 3.0830, time : 28.4817\n",
      "loss : 3.0452, time : 28.4783\n",
      "loss : 3.0371, time : 28.5152\n",
      "loss : 3.0591, time : 28.4891\n"
     ]
    }
   ],
   "source": [
    "print('starting training')\n",
    "writes = 0\n",
    "for epoch in range(args.max_epochs):\n",
    "    model.train(True)\n",
    "    torch.cuda.synchronize()\n",
    "    train_loss = 0.\n",
    "    time_ = time.time()\n",
    "    model.train()\n",
    "    for batch_idx, (input,_) in enumerate(train_loader):\n",
    "        input = input.to(device)\n",
    "        input = Variable(input)\n",
    "        output = model(input)\n",
    "        output.retain_grad()\n",
    "        loss = loss_op(input, output)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        if output.grad.isnan().any():\n",
    "            print('output grad are nan')\n",
    "            break\n",
    "        optimizer.step()\n",
    "        train_loss += loss.detach().cpu().numpy()\n",
    "        if (batch_idx +1) % args.print_every == 0 :\n",
    "            deno = args.print_every * args.batch_size * np.prod(obs) * np.log(2.)\n",
    "            print('loss : {:.4f}, time : {:.4f}'.format(\n",
    "                (train_loss / deno),\n",
    "                (time.time() - time_)))\n",
    "            train_loss = 0.\n",
    "            writes += 1\n",
    "            time_ = time.time()\n",
    "\n",
    "    if loss.isnan() or loss.isinf() or output.grad.isnan().any():\n",
    "        break\n",
    "\n",
    "    # decrease learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    model.eval()\n",
    "    test_loss = 0.\n",
    "    for batch_idx, (input,_) in enumerate(test_loader):\n",
    "        input = input.to(device)\n",
    "        input_var = Variable(input)\n",
    "        output = model(input_var)\n",
    "        loss = loss_op(input_var, output)\n",
    "        test_loss += loss.detach().cpu().numpy()\n",
    "        del loss, output\n",
    "\n",
    "    deno = batch_idx * args.batch_size * np.prod(obs) * np.log(2.)\n",
    "    print('test loss : {:4f}, lr : {:4e}'.format(test_loss / deno, optimizer.param_groups[0]['lr']))\n",
    "    test_losses.append(test_loss / deno)\n",
    "\n",
    "    if (epoch + 1) % args.save_interval == 0:\n",
    "        torch.save(model.state_dict(), '{}/{}_{}.pth'.format(args.save_dir, model_name, epoch))\n",
    "        print('sampling...')\n",
    "        sample_t = sample(model)\n",
    "        sample_t = rescaling_inv(sample_t)\n",
    "        utils.save_image(sample_t,'/data/pixelcnn/images/{}_{}.png'.format(model_name, epoch),\n",
    "                nrow=5, padding=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.to('cpu').state_dict(), './cifar_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.9946067419982145,\n",
       " 3.7095731649833557,\n",
       " 3.5397387527198334,\n",
       " 3.4285993233309866,\n",
       " 3.506644070579394,\n",
       " 3.355817989354682,\n",
       " 3.332827659111572,\n",
       " 3.318594494726047,\n",
       " 3.3263378349837764,\n",
       " 3.337990469515122,\n",
       " 3.241161852473759,\n",
       " 3.2009310717845274,\n",
       " 3.215256899540375,\n",
       " 3.2043920480781005,\n",
       " 3.2180318627635573,\n",
       " 3.1785560517355735,\n",
       " 3.192287648196525,\n",
       " 3.2052902351894015,\n",
       " 3.181655520497033,\n",
       " 3.1468223925211043,\n",
       " 3.1636116053244465,\n",
       " 3.127488230428158,\n",
       " 3.184809518048376,\n",
       " 3.1293779934350847,\n",
       " 3.1181206981426617,\n",
       " 3.1670931901373796,\n",
       " 3.1238606558636377,\n",
       " 3.116251112596015,\n",
       " 3.1218776676028988,\n",
       " 3.094179535355388,\n",
       " 3.103162447551413,\n",
       " 3.095869164944386,\n",
       " 3.1980283989486136,\n",
       " 3.169104161959829,\n",
       " 3.0808783839082237,\n",
       " 3.10101412626816,\n",
       " 3.097537354029577,\n",
       " 3.1172043411377937,\n",
       " 3.073054336381765,\n",
       " 3.0977771033571444,\n",
       " 3.064713736758534,\n",
       " 3.064957556709656,\n",
       " 3.0588753182349704,\n",
       " 3.065151132737794,\n",
       " 3.0964910075394005,\n",
       " 3.1430258023739324,\n",
       " 3.0580621464108084,\n",
       " 3.076979823850915,\n",
       " 3.0595846253999617,\n",
       " 3.0496567949520434]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input, _ = next(iter(train_loader))\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input.to(device))\n",
    "\n",
    "n_steps = 300\n",
    "unif_x = torch.linspace(-bounds, bounds, steps=n_steps, device=output.device).reshape(-1, 1).tile(1, 3)\n",
    "params = output[0,:,10,10].reshape(-1, nits_model.tot_params).tile((n_steps, 1))\n",
    "pdfs = nits_model.to(unif_x.device).pdf(unif_x, params)\n",
    "\n",
    "colors = ['red', 'green', 'blue']\n",
    "for i in range(nits_model.d):\n",
    "    plt.scatter(unif_x[:,i].detach().cpu(), pdfs[:,i].detach().cpu(), c=colors[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
