{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nits.model import *\n",
    "from nits.fc_model import *\n",
    "from maf.datasets import *\n",
    "from nits.resmade import ResidualMADE\n",
    "    \n",
    "def create_batcher(x, batch_size=1):\n",
    "    idx = 0\n",
    "    p = torch.randperm(len(x))\n",
    "    x = x[p]\n",
    "\n",
    "    while idx + batch_size < len(x):\n",
    "        yield torch.tensor(x[idx:idx+batch_size], device=device)\n",
    "        idx += batch_size\n",
    "    else:\n",
    "        yield torch.tensor(x[idx:], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:6\n",
      "batch_size: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/henry/projects/NITS/nits/model.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer('start_val', torch.tensor(start))\n",
      "/home/henry/projects/NITS/nits/model.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer('end_val', torch.tensor(end))\n",
      "/home/henry/projects/NITS/nits/model.py:359: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer('start', torch.tensor(start).reshape(1, 1).tile(1, d))\n",
      "/home/henry/projects/NITS/nits/model.py:360: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer('end', torch.tensor(end).reshape(1, 1).tile(1, d))\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('-d', '--dataset', type=str, default='gas')\n",
    "parser.add_argument('-g', '--gpu', type=str, default='')\n",
    "parser.add_argument('-r', '--rotate', type=bool, default=False)\n",
    "\n",
    "args = parser.parse_args(['-g', '6', '-d', 'gas'])\n",
    "\n",
    "device = 'cuda:' + args.gpu if args.gpu else 'cpu'\n",
    "print('device:', device)\n",
    "\n",
    "lr = 2e-3\n",
    "gamma = 1.\n",
    "polyak_decay = 0.99995\n",
    "n_residual_blocks = 4\n",
    "hidden_dim = 512\n",
    "use_batch_norm = False\n",
    "zero_initialization = True\n",
    "weight_norm = False\n",
    "batch_size = 2048\n",
    "print('batch_size:', batch_size)\n",
    "if args.dataset == 'gas':\n",
    "    # training set size: 852,174\n",
    "    data = gas.GAS()\n",
    "    dropout_probability = 0.1\n",
    "    nits_arch = [8, 8, 8, 1]\n",
    "    gamma = 1 - 5e-7\n",
    "elif args.dataset == 'power':\n",
    "    # training set size: 1,659,917\n",
    "    data = power.POWER()\n",
    "    dropout_probability = 0.1\n",
    "    nits_arch = [8, 8, 8, 1]\n",
    "    gamma = 1 - 5e-7\n",
    "elif args.dataset == 'miniboone':\n",
    "    # training set size: 29,556\n",
    "    data = miniboone.MINIBOONE()\n",
    "    dropout_probability = 0.5\n",
    "    nits_arch = [8, 8, 8, 1]\n",
    "    gamma = 1 - 5e-7\n",
    "elif args.dataset == 'hepmass':\n",
    "    # training set size: 315,123\n",
    "    data = hepmass.HEPMASS()\n",
    "    dropout_probability = 0.5\n",
    "    nits_arch = [8, 8, 8, 1]\n",
    "    gamma = 1 - 5e-7\n",
    "elif args.dataset == 'bsds300':\n",
    "    # training set size: 1,000,000\n",
    "    data = bsds300.BSDS300()\n",
    "    dropout_probability = 0.2\n",
    "    nits_arch = [8, 8, 8, 1]\n",
    "    gamma = 1 - 5e-7\n",
    "\n",
    "d = data.trn.x.shape[1]\n",
    "\n",
    "max_val = max(data.trn.x.max(), data.val.x.max(), data.tst.x.max())\n",
    "min_val = min(data.trn.x.min(), data.val.x.min(), data.tst.x.min())\n",
    "max_val, min_val = torch.tensor(max_val).to(device).float(), torch.tensor(min_val).to(device).float()\n",
    "\n",
    "nits_model = NITS(d=d, start=min_val, end=max_val, monotonic_const=1e-5,\n",
    "                 A_constraint='neg_exp', arch=[1, 1] + nits_arch,\n",
    "                 final_layer_constraint='softmax',\n",
    "                 softmax_temperature=False).to(device)\n",
    "\n",
    "model = ResMADEModel(\n",
    "    d=d, \n",
    "    rotate=args.rotate, \n",
    "    nits_model=nits_model,\n",
    "    n_residual_blocks=n_residual_blocks,\n",
    "    hidden_dim=hidden_dim,\n",
    "    dropout_probability=dropout_probability,\n",
    "    use_batch_norm=use_batch_norm,\n",
    "    zero_initialization=zero_initialization,\n",
    "    weight_norm=weight_norm,\n",
    "    use_normalizer=True\n",
    ")\n",
    "\n",
    "model.normalizer.set_weights(torch.tensor(data.trn.x, device='cpu'), device=device)\n",
    "\n",
    "shadow = ResMADEModel(\n",
    "    d=d, \n",
    "    rotate=args.rotate, \n",
    "    nits_model=nits_model,\n",
    "    n_residual_blocks=n_residual_blocks,\n",
    "    hidden_dim=hidden_dim,\n",
    "    dropout_probability=dropout_probability,\n",
    "    use_batch_norm=use_batch_norm,\n",
    "    zero_initialization=zero_initialization,\n",
    "    weight_norm=weight_norm,\n",
    "    use_normalizer=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize weight norm\n",
    "if weight_norm:\n",
    "    with torch.no_grad():\n",
    "        for i, x in enumerate(create_batcher(data.trn.x, batch_size=4096)):\n",
    "            params = model(x)\n",
    "            break\n",
    "    \n",
    "model = EMA(model, shadow, decay=polyak_decay).to(device)\n",
    "\n",
    "max_epochs = 20000\n",
    "print_every = 10\n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=1, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   10, time: 264.24, train_ll: 18.0814, val_ll: -26.9011, test_ll: -26.8979, lr: 1.9958e-03\n",
      "epoch:   20, time: 264.18, train_ll: 28.3048, val_ll: -31.7231, test_ll: -31.7259, lr: 1.9917e-03\n",
      "epoch:   30, time: 264.51, train_ll: 31.5114, val_ll: -47.0623, test_ll: -47.0712, lr: 1.9875e-03\n",
      "epoch:   40, time: 264.28, train_ll: 32.5305, val_ll: -56.0281, test_ll: -56.0407, lr: 1.9834e-03\n",
      "epoch:   50, time: 264.53, train_ll: 33.2242, val_ll: -61.2323, test_ll: -61.2463, lr: 1.9793e-03\n",
      "epoch:   60, time: 264.32, train_ll: 33.2999, val_ll: -65.6327, test_ll: -65.6465, lr: 1.9751e-03\n",
      "epoch:   70, time: 264.09, train_ll: 33.5629, val_ll: -69.4532, test_ll: -69.4659, lr: 1.9710e-03\n",
      "epoch:   80, time: 264.19, train_ll: 34.0633, val_ll: -72.6767, test_ll: -72.6853, lr: 1.9669e-03\n",
      "epoch:   90, time: 264.61, train_ll: 34.1814, val_ll: -77.8454, test_ll: -77.8460, lr: 1.9628e-03\n",
      "epoch:  100, time: 264.31, train_ll: 34.9003, val_ll: -81.5145, test_ll: -81.5078, lr: 1.9587e-03\n",
      "epoch:  110, time: 264.26, train_ll: 35.2342, val_ll: -83.2520, test_ll: -83.2439, lr: 1.9547e-03\n",
      "epoch:  120, time: 264.36, train_ll: 35.5015, val_ll: -84.5819, test_ll: -84.5754, lr: 1.9506e-03\n",
      "epoch:  130, time: 264.08, train_ll: 35.9104, val_ll: -85.2212, test_ll: -85.2180, lr: 1.9465e-03\n",
      "epoch:  140, time: 264.33, train_ll: 36.8475, val_ll: -85.4905, test_ll: -85.4888, lr: 1.9425e-03\n",
      "epoch:  150, time: 264.16, train_ll: 37.2431, val_ll: -85.4519, test_ll: -85.4508, lr: 1.9384e-03\n",
      "epoch:  160, time: 264.15, train_ll: 37.7504, val_ll: -85.4284, test_ll: -85.4272, lr: 1.9344e-03\n",
      "epoch:  170, time: 264.24, train_ll: 37.7488, val_ll: -85.4187, test_ll: -85.4178, lr: 1.9304e-03\n",
      "epoch:  180, time: 264.12, train_ll: 38.5593, val_ll: -85.4293, test_ll: -85.4282, lr: 1.9263e-03\n",
      "epoch:  190, time: 264.05, train_ll: 39.1055, val_ll: -85.4222, test_ll: -85.4204, lr: 1.9223e-03\n",
      "epoch:  200, time: 264.01, train_ll: 39.4364, val_ll: -85.3475, test_ll: -85.3414, lr: 1.9183e-03\n",
      "epoch:  210, time: 264.21, train_ll: 39.6131, val_ll: -85.4133, test_ll: -85.4022, lr: 1.9143e-03\n",
      "epoch:  220, time: 264.25, train_ll: 40.1055, val_ll: -85.3845, test_ll: -85.3732, lr: 1.9103e-03\n",
      "epoch:  230, time: 264.38, train_ll: 40.1534, val_ll: -84.6831, test_ll: -84.6728, lr: 1.9064e-03\n",
      "epoch:  240, time: 264.00, train_ll: 40.1997, val_ll: -82.9126, test_ll: -82.9048, lr: 1.9024e-03\n",
      "epoch:  250, time: 264.21, train_ll: 39.8678, val_ll: -80.5727, test_ll: -80.5688, lr: 1.8984e-03\n",
      "epoch:  260, time: 264.28, train_ll: 40.1518, val_ll: -77.9927, test_ll: -77.9920, lr: 1.8945e-03\n",
      "epoch:  270, time: 263.12, train_ll: 40.2781, val_ll: -74.9903, test_ll: -74.9930, lr: 1.8905e-03\n",
      "epoch:  280, time: 259.05, train_ll: 41.1128, val_ll: -73.5596, test_ll: -73.5635, lr: 1.8866e-03\n",
      "epoch:  290, time: 257.69, train_ll: 41.2529, val_ll: -71.2122, test_ll: -71.2155, lr: 1.8827e-03\n",
      "epoch:  300, time: 258.74, train_ll: 41.1733, val_ll: -67.9726, test_ll: -67.9745, lr: 1.8787e-03\n",
      "epoch:  310, time: 258.08, train_ll: 41.4565, val_ll: -64.3028, test_ll: -64.3035, lr: 1.8748e-03\n",
      "epoch:  320, time: 258.89, train_ll: 41.6237, val_ll: -60.7961, test_ll: -60.7961, lr: 1.8709e-03\n",
      "epoch:  330, time: 259.21, train_ll: 41.7267, val_ll: -57.3422, test_ll: -57.3420, lr: 1.8670e-03\n",
      "epoch:  340, time: 258.68, train_ll: 41.8837, val_ll: -53.6785, test_ll: -53.6781, lr: 1.8631e-03\n",
      "epoch:  350, time: 259.00, train_ll: 42.0471, val_ll: -51.1796, test_ll: -51.1792, lr: 1.8592e-03\n",
      "epoch:  360, time: 258.83, train_ll: 42.4978, val_ll: -51.3656, test_ll: -51.3653, lr: 1.8554e-03\n",
      "epoch:  370, time: 258.53, train_ll: 42.8193, val_ll: -55.6945, test_ll: -55.6944, lr: 1.8515e-03\n",
      "epoch:  380, time: 258.50, train_ll: 43.0957, val_ll: -61.0348, test_ll: -61.0350, lr: 1.8477e-03\n",
      "epoch:  390, time: 258.38, train_ll: 43.3048, val_ll: -62.4078, test_ll: -62.4081, lr: 1.8438e-03\n",
      "epoch:  400, time: 258.37, train_ll: 42.9692, val_ll: -63.4786, test_ll: -63.4790, lr: 1.8400e-03\n",
      "epoch:  410, time: 259.68, train_ll: 43.0916, val_ll: -61.8130, test_ll: -61.8133, lr: 1.8361e-03\n",
      "epoch:  420, time: 259.23, train_ll: 43.0321, val_ll: -57.8944, test_ll: -57.8946, lr: 1.8323e-03\n",
      "epoch:  430, time: 258.26, train_ll: 43.2853, val_ll: -53.3983, test_ll: -53.3984, lr: 1.8285e-03\n",
      "epoch:  440, time: 257.98, train_ll: 43.3433, val_ll: -48.4029, test_ll: -48.4029, lr: 1.8247e-03\n",
      "epoch:  450, time: 258.76, train_ll: 43.7439, val_ll: -43.4668, test_ll: -43.4668, lr: 1.8209e-03\n",
      "epoch:  460, time: 258.30, train_ll: 43.4574, val_ll: -37.9048, test_ll: -37.9047, lr: 1.8171e-03\n",
      "epoch:  470, time: 258.22, train_ll: 43.4582, val_ll: -31.5502, test_ll: -31.5500, lr: 1.8133e-03\n",
      "epoch:  480, time: 258.40, train_ll: 43.3376, val_ll: -25.0083, test_ll: -25.0080, lr: 1.8095e-03\n",
      "epoch:  490, time: 260.28, train_ll: 43.4679, val_ll: -19.1538, test_ll: -19.1535, lr: 1.8058e-03\n",
      "epoch:  500, time: 259.86, train_ll: 43.7796, val_ll: -15.2954, test_ll: -15.2952, lr: 1.8020e-03\n",
      "epoch:  510, time: 258.93, train_ll: 43.4626, val_ll: -12.8204, test_ll: -12.8202, lr: 1.7982e-03\n",
      "epoch:  520, time: 258.88, train_ll: 43.7974, val_ll: -12.7409, test_ll: -12.7407, lr: 1.7945e-03\n",
      "epoch:  530, time: 258.07, train_ll: 43.9687, val_ll: -15.3397, test_ll: -15.3395, lr: 1.7908e-03\n",
      "epoch:  540, time: 258.73, train_ll: 44.1880, val_ll: -20.2107, test_ll: -20.2106, lr: 1.7870e-03\n",
      "epoch:  550, time: 258.42, train_ll: 44.0990, val_ll: -28.4965, test_ll: -28.4964, lr: 1.7833e-03\n",
      "epoch:  560, time: 262.89, train_ll: 44.5703, val_ll: -41.1903, test_ll: -41.1903, lr: 1.7796e-03\n",
      "epoch:  570, time: 264.04, train_ll: 44.8109, val_ll: -56.4077, test_ll: -56.4077, lr: 1.7759e-03\n",
      "epoch:  580, time: 264.28, train_ll: 45.0000, val_ll: -62.5605, test_ll: -62.5605, lr: 1.7722e-03\n"
     ]
    }
   ],
   "source": [
    "time_ = time.time()\n",
    "train_ll = 0.\n",
    "for epoch in range(max_epochs):\n",
    "    model.train()\n",
    "    for i, x in enumerate(create_batcher(data.trn.x, batch_size=batch_size)):\n",
    "        orig_x = x.cpu().detach().clone()\n",
    "        ll = model(x)\n",
    "        optim.zero_grad()\n",
    "        (-ll).backward()\n",
    "        train_ll += ll.detach().cpu().numpy()\n",
    "\n",
    "        optim.step()\n",
    "        scheduler.step()\n",
    "        model.update()\n",
    "\n",
    "    if (epoch + 1) % print_every == 0:\n",
    "        # compute train loss\n",
    "        train_ll /= len(data.trn.x) * print_every\n",
    "        lr = optim.param_groups[0]['lr']\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            val_ll = 0.\n",
    "            for i, x in enumerate(create_batcher(data.val.x, batch_size=batch_size)):\n",
    "                x = torch.tensor(x, device=device)\n",
    "                ll = model(x)\n",
    "                val_ll += ll.detach().cpu().numpy()\n",
    "\n",
    "            val_ll /= len(data.val.x)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            test_ll = 0.\n",
    "            for i, x in enumerate(create_batcher(data.tst.x, batch_size=batch_size)):\n",
    "                x = torch.tensor(x, device=device)\n",
    "                ll = model(x)\n",
    "                test_ll += ll.detach().cpu().numpy()\n",
    "\n",
    "            test_ll /= len(data.tst.x)\n",
    "            \n",
    "        fmt_str1 = 'epoch: {:4d}, time: {:.2f}, train_ll: {:.4f},'\n",
    "        fmt_str2 = ' val_ll: {:.4f}, test_ll: {:.4f}, lr: {:.4e}'\n",
    "\n",
    "        print((fmt_str1 + fmt_str2).format(\n",
    "            epoch + 1,\n",
    "            time.time() - time_,\n",
    "            train_ll,\n",
    "            val_ll,\n",
    "            test_ll,\n",
    "            lr))\n",
    "\n",
    "        time_ = time.time()\n",
    "        train_ll = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_x = model.model.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sampled_x[:,0].cpu(), sampled_x[:,1].cpu(), s=1, alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data.trn.x[:,0].cpu(), data.trn.x[:,1].cpu(), alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.trn.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
