{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nits.model import *\n",
    "from nits.fc_model import *\n",
    "from maf.datasets import *\n",
    "from nits.resmade import ResidualMADE, CausalTransformer\n",
    "\n",
    "def list_str_to_list(s):\n",
    "    print(s)\n",
    "    assert s[0] == '[' and s[-1] == ']'\n",
    "    s = s[1:-1]\n",
    "    s = s.replace(' ', '')\n",
    "    s = s.split(',')\n",
    "\n",
    "    s = [int(x) for x in s]\n",
    "\n",
    "    return s\n",
    "    \n",
    "def create_batcher(x, batch_size=1):\n",
    "    idx = 0\n",
    "    p = torch.randperm(len(x))\n",
    "    x = x[p]\n",
    "\n",
    "    while idx + batch_size < len(x):\n",
    "        yield torch.tensor(x[idx:idx+batch_size], device=device).float()\n",
    "        idx += batch_size\n",
    "    else:\n",
    "        yield torch.tensor(x[idx:], device=device).float()\n",
    "        \n",
    "class Dataset:\n",
    "    def __init__(self, x, permute=False, train_idx=0, val_idx=0):\n",
    "        # splits x into train, val, and test\n",
    "        self.n = len(x)\n",
    "        if permute:\n",
    "            p = np.random.permutation(self.n)\n",
    "            x = x[p]\n",
    "            \n",
    "        train_idx = train_idx if train_idx else int(0.8 * self.n)\n",
    "        val_idx = val_idx if val_idx else int(0.9 * self.n)\n",
    "        \n",
    "        class DataHolder:\n",
    "            def __init__(self, x):\n",
    "                self.x = x\n",
    "                \n",
    "        self.trn = DataHolder(x[:train_idx])\n",
    "        self.val = DataHolder(x[train_idx:val_idx])\n",
    "        self.tst = DataHolder(x[val_idx:])\n",
    "        \n",
    "def build_tridiagonal(n=1000000, d=30, k=1, permute=False):\n",
    "    precov = np.random.normal(size=(d, d))\n",
    "    precov = np.matmul(precov, precov.T)\n",
    "    cov = np.tril(np.triu(precov, -k), k)\n",
    "    cov = cov / np.diag(cov).mean()\n",
    "    \n",
    "    pre_x = np.random.normal(size=(n, d))\n",
    "    x = np.matmul(pre_x, cov)\n",
    "    \n",
    "    # normalize\n",
    "    m = np.mean(x, axis=0, keepdims=True)\n",
    "    std = np.std(x, axis=0, keepdims=True)\n",
    "    assert m.shape == x[0:1].shape and std.shape == x[0:1].shape\n",
    "    x = (x - m) / std\n",
    "    \n",
    "    return Dataset(x.astype(np.float))\n",
    "\n",
    "def permute_data(dataset):\n",
    "    d = dataset.trn.x.shape[1]\n",
    "    train_idx = len(dataset.trn.x)\n",
    "    val_idx = train_idx + len(dataset.val.x)\n",
    "    x = np.concatenate([dataset.trn.x, dataset.val.x, dataset.tst.x], axis=0)\n",
    "    \n",
    "    P = np.eye(d)\n",
    "    P = P[np.random.permutation(d)]\n",
    "    permuted_x = np.matmul(x, P)\n",
    "    assert np.allclose(np.matmul(permuted_x, P.T), x)\n",
    "    \n",
    "    return Dataset(permuted_x.astype(np.float), train_idx=train_idx, val_idx=val_idx), P.astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16,16,1]\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('-d', '--dataset', type=str, default='gas')\n",
    "parser.add_argument('-g', '--gpu', type=str, default='')\n",
    "parser.add_argument('-s', '--seed', type=int, default=1)\n",
    "parser.add_argument('-b', '--batch_size', type=int, default=1024)\n",
    "parser.add_argument('-hi', '--hidden_dim', type=int, default=64)\n",
    "parser.add_argument('-nr', '--n_blocks', type=int, default=2)\n",
    "parser.add_argument('-n', '--patience', type=int, default=-1)\n",
    "parser.add_argument('-ga', '--gamma', type=float, default=1)\n",
    "parser.add_argument('-pd', '--polyak_decay', type=float, default=1 - 5e-5)\n",
    "parser.add_argument('-a', '--nits_arch', type=list_str_to_list, default='[16,16,1]')\n",
    "parser.add_argument('-r', '--rotate', action='store_true')\n",
    "parser.add_argument('-dn', '--dont_normalize_inverse', type=bool, default=False)\n",
    "parser.add_argument('-l', '--learning_rate', type=float, default=2e-4)\n",
    "parser.add_argument('-p', '--dropout', type=float, default=-1.0)\n",
    "parser.add_argument('-rc', '--add_residual_connections', type=bool, default=False)\n",
    "parser.add_argument('-bm', '--bound_multiplier', type=float, default=1.0)\n",
    "parser.add_argument('-pe', '--permute_data', action='store_true')\n",
    "\n",
    "# args = parser.parse_args(['-g', '6', '-d', 'tridiagonal', '-pe', '-r',])\n",
    "args = parser.parse_args([\n",
    "    '--gpu=1', \n",
    "    '--batch_size=128', \n",
    "    '--hidden_dim=64', \n",
    "    '--learning_rate=2e-4',\n",
    "    '--n_blocks=8'\n",
    "])\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = 'cuda:' + args.gpu if args.gpu else 'cpu'\n",
    "\n",
    "# param_model = ResidualMADE\n",
    "param_model = CausalTransformer\n",
    "default_patience = 10\n",
    "if args.dataset == 'gas':\n",
    "    # training set size: 852,174\n",
    "    data = gas.GAS()\n",
    "    default_dropout = 0.1\n",
    "elif args.dataset == 'power':\n",
    "    # training set size: 1,659,917\n",
    "    data = power.POWER()\n",
    "    default_dropout = 0.1\n",
    "elif args.dataset == 'miniboone':\n",
    "    # training set size: 29,556\n",
    "    data = miniboone.MINIBOONE()\n",
    "    default_dropout = 0.3\n",
    "elif args.dataset == 'hepmass':\n",
    "    # training set size: 315,123\n",
    "    data = hepmass.HEPMASS()\n",
    "    default_dropout = 0.5\n",
    "    default_pateince = 3\n",
    "elif args.dataset == 'bsds300':\n",
    "    # training set size: 1,000,000\n",
    "    data = bsds300.BSDS300()\n",
    "    default_dropout = 0.2\n",
    "elif args.dataset == 'tridiagonal':\n",
    "    data = build_tridiagonal()\n",
    "    default_dropout = 0.0\n",
    "    \n",
    "if args.permute_data:\n",
    "    print(\"PERMUTED DATA\")\n",
    "    data, P = permute_data(data)\n",
    "    print(\"P\\n\", np.arange(data.trn.x.shape[1]).reshape(1, -1)@(P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.patience = args.patience if args.patience >= 0 else default_patience\n",
    "args.dropout = args.dropout if args.dropout >= 0.0 else default_dropout\n",
    "print(args)\n",
    "\n",
    "d = data.trn.x.shape[1]\n",
    "\n",
    "max_val = max(data.trn.x.max(), data.val.x.max(), data.tst.x.max())\n",
    "min_val = min(data.trn.x.min(), data.val.x.min(), data.tst.x.min())\n",
    "max_val, min_val = torch.tensor(max_val).to(device).float(), torch.tensor(min_val).to(device).float()\n",
    "\n",
    "max_val *= args.bound_multiplier\n",
    "min_val *= args.bound_multiplier\n",
    "\n",
    "nits_model = NITS(start=min_val, end=max_val, monotonic_const=1e-5,\n",
    "                  A_constraint='neg_exp', arch=[d] + args.nits_arch,\n",
    "                  final_layer_constraint='softmax',\n",
    "                  add_residual_connections=args.add_residual_connections,\n",
    "                  normalize_inverse=(not args.dont_normalize_inverse),\n",
    "                  softmax_temperature=False).to(device)\n",
    "\n",
    "model = Model(\n",
    "    d=d,\n",
    "    rotate=args.rotate,\n",
    "    nits_model=nits_model,\n",
    "    param_model=param_model,\n",
    "    n_blocks=args.n_blocks,\n",
    "    hidden_dim=args.hidden_dim,\n",
    "    dropout_probability=args.dropout,\n",
    ").to(device)\n",
    "\n",
    "shadow = Model(\n",
    "    d=d,\n",
    "    rotate=args.rotate,\n",
    "    nits_model=nits_model,\n",
    "    param_model=param_model,\n",
    "    n_blocks=args.n_blocks,\n",
    "    hidden_dim=args.hidden_dim,\n",
    "    dropout_probability=args.dropout,\n",
    ").to(device)\n",
    "\n",
    "model = EMA(model, shadow, decay=args.polyak_decay).to(device)\n",
    "\n",
    "print_every = 1\n",
    "optim = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=1, gamma=args.gamma)\n",
    "\n",
    "time_ = time.time()\n",
    "epoch = 0\n",
    "train_ll = 0.\n",
    "max_val_ll = -np.inf\n",
    "patience = args.patience\n",
    "keep_training = True\n",
    "while keep_training:\n",
    "    model.train()\n",
    "    for i, x in enumerate(create_batcher(data.trn.x, batch_size=args.batch_size)):\n",
    "        ll = model(x)\n",
    "        optim.zero_grad()\n",
    "        (-ll).backward()\n",
    "        train_ll += ll.detach().cpu().numpy()\n",
    "\n",
    "        optim.step()\n",
    "        scheduler.step()\n",
    "        model.update()\n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        # compute train loss\n",
    "        train_ll /= len(data.trn.x) * print_every\n",
    "        lr = optim.param_groups[0]['lr']\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            val_ll = 0.\n",
    "            ema_val_ll = 0.\n",
    "            for i, x in enumerate(create_batcher(data.val.x, batch_size=args.batch_size)):\n",
    "                x = torch.tensor(x, device=device)\n",
    "                val_ll += model.model(x).detach().cpu().numpy()\n",
    "                ema_val_ll += model(x).detach().cpu().numpy()\n",
    "\n",
    "            val_ll /= len(data.val.x)\n",
    "            ema_val_ll /= len(data.val.x)\n",
    "\n",
    "        # early stopping\n",
    "        if ema_val_ll > max_val_ll + 1e-4:\n",
    "            patience = args.patience\n",
    "            max_val_ll = ema_val_ll\n",
    "        else:\n",
    "            patience -= 1\n",
    "\n",
    "        if patience == 0:\n",
    "            print(\"Patience reached zero. max_val_ll stayed at {:.3f} for {:d} iterations.\".format(max_val_ll, args.patience))\n",
    "            keep_training = False\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            test_ll = 0.\n",
    "            ema_test_ll = 0.\n",
    "            for i, x in enumerate(create_batcher(data.tst.x, batch_size=args.batch_size)):\n",
    "                x = torch.tensor(x, device=device)\n",
    "                test_ll += model.model(x).detach().cpu().numpy()\n",
    "                ema_test_ll += model(x).detach().cpu().numpy()\n",
    "\n",
    "            test_ll /= len(data.tst.x)\n",
    "            ema_test_ll /= len(data.tst.x)\n",
    "\n",
    "        fmt_str1 = 'epoch: {:3d}, time: {:3d}s, train_ll: {:.3f},'\n",
    "        fmt_str2 = ' ema_val_ll: {:.3f}, ema_test_ll: {:.3f},'\n",
    "        fmt_str3 = ' val_ll: {:.3f}, test_ll: {:.3f}, lr: {:.2e}'\n",
    "\n",
    "        print((fmt_str1 + fmt_str2 + fmt_str3).format(\n",
    "            epoch,\n",
    "            int(time.time() - time_),\n",
    "            train_ll,\n",
    "            ema_val_ll,\n",
    "            ema_test_ll,\n",
    "            val_ll,\n",
    "            test_ll,\n",
    "            lr))\n",
    "\n",
    "        time_ = time.time()\n",
    "        train_ll = 0.\n",
    "\n",
    "    if epoch % (print_every * 10) == 0:\n",
    "        print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_hat = model.model.get_P()\n",
    "v = torch.ones(d, device=device).reshape(1, -1) / np.sqrt(d)\n",
    "print(v.norm())\n",
    "print((v @ P_hat).norm())\n",
    "P_hat.argmax(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;32m[INFO ] \u001b[0m134298 leaf nodes in the tree using full dataset; minimum alpha: -42.8865.\n",
      "\u001b[0;32m[INFO ] \u001b[0mPerforming 10-fold cross validation.\n",
      "\u001b[0;32m[INFO ] \u001b[0m39733 trees in the sequence; maximum alpha: 8.99925.\n",
      "\u001b[0;32m[INFO ] \u001b[0mOptimal alpha: 8.83999.\n",
      "\u001b[0;32m[INFO ] \u001b[0m100 leaf nodes in the optimally pruned tree; optimal alpha: 8.83999.\n",
      "\u001b[0;33m[WARN ] \u001b[0mUnable to open file '' to save tag membership info.\n"
     ]
    }
   ],
   "source": [
    "from mlpack import det\n",
    "\n",
    "d = det(folds=10, test=data.tst.x,\n",
    "#         max_leaf_size=10,\n",
    "#         min_leaf_size=5,\n",
    "#         training=np.concatenate([data.trn.x, data.val.x]), verbose=False)\n",
    "        training=data.trn.x, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-13.7834544349559"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(d['test_set_estimates'] + 1e-7).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
